{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration Outside of Main Notebook\n",
    "\n",
    "This notebook is a bit more messy and exploratory than the main notebook, but I'm going to do some data exploration here to iterate faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jvxQKSqQY3Fa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import gdown\n",
    "import jsonlines\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2TokenizerFast, AutoTokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
    "import ftfy\n",
    "from lm_dataformat import Reader\n",
    "from gpt_generate import gpt_generate, create_prompt_txt_from_df, print_ground_truth_from_df\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "vzBUVxTvfETA",
    "outputId": "06478701-75be-4cc6-a797-f03db9e876db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0+cu113'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"prompts/contexts\", exist_ok=True)\n",
    "os.makedirs(\"prompts/questions\", exist_ok=True)\n",
    "os.makedirs(\"prompts/answers\", exist_ok=True)\n",
    "os.makedirs(\"prompts/task_description\", exist_ok=True)\n",
    "os.makedirs(\"prompts/ground_truths\", exist_ok=True)\n",
    "os.makedirs(\"prompts/prompts_with_relevance\", exist_ok=True)\n",
    "os.makedirs(\"prompts/prompts_without_relevance\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Initial Dataset\n",
    "\n",
    "To create some initial prompts for testing, I went on LessWrong.org and read some of the prompts from the comment section on [this post](https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread#comments). I also created a few with the help of the [Natural Questions dataset from Google](https://ai.google.com/research/NaturalQuestions/visualization) and created a few by hand. To make things faster, I stored the data in Google Sheets and then exported it to CSV.\n",
    "\n",
    "For quick iteration, I used GPT-2, GPT-J, GPT-3, and instruct-GPT-3 to get a feel for model performance. For the difficult examples from the dataset, all models performed poorly. However, as I added more few-shot examples and better context engineering, the models started to perform better (though still not great for the smaller models). This notebook will show these observations in a quantitative way while still giving my qualitative observations.\n",
    "\n",
    "Here's what the data looks like (ignore the columns past explanation, I'll only use them post-training if I do):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "      <th>difficulty</th>\n",
       "      <th>deceptive</th>\n",
       "      <th>improved_question</th>\n",
       "      <th>improved_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
       "      <td>An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.</td>\n",
       "      <td>relevant</td>\n",
       "      <td>it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.</td>\n",
       "      <td>medium</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
       "      <td>I jumped in the river to save the little boy.</td>\n",
       "      <td>not relevant</td>\n",
       "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                               question  \\\n",
       "0  When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
       "1  When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
       "\n",
       "                                                                                                                                                answer  \\\n",
       "0  An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.   \n",
       "1                                                                                                        I jumped in the river to save the little boy.   \n",
       "\n",
       "      relevance  \\\n",
       "0      relevant   \n",
       "1  not relevant   \n",
       "\n",
       "                                                                                                                    explanation  \\\n",
       "0  it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.   \n",
       "1                                          it is talking about jumping in a river to save a boy, but the question is about AGI.   \n",
       "\n",
       "  difficulty deceptive  improved_question  improved_answer  \n",
       "0     medium        no                NaN              NaN  \n",
       "1        NaN       NaN                NaN              NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/qa-relevance-dataset.csv\")\n",
    "print(len(df))\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list out all of the benchmark examples in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth 0\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
      "\n",
      "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is relevant because it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.\n",
      "----------------------------------------------------\n",
      "Ground truth 1\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
      "\n",
      "ANSWER: I jumped in the river to save the little boy.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is not relevant because it is talking about jumping in a river to save a boy, but the question is about AGI.\n",
      "----------------------------------------------------\n",
      "Ground truth 2\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Why do we assume that any AGI can meaningfully be described as a utility maximizer?\n",
      "\n",
      "Humans are the some of most intelligent structures that exist, and we don’t seem to fit that model very well. If fact, it seems the entire point in Rationalism is to improve our ability to do this, which has only been achieved with mixed success.\n",
      "\n",
      "Organisations of humans (e.g. USA, FDA, UN) have even more computational power and don’t seem to be doing much better.\n",
      "\n",
      "Perhaps an intelligence (artificial or natural) cannot necessarily, or even typically be described as optimisers? Instead we could only model them as an algorithm or as a collection of tools/behaviours executed in some pattern.\n",
      "\n",
      "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is relevant because it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.\n",
      "----------------------------------------------------\n",
      "Ground truth 3\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Why do we assume that any AGI can meaningfully be described as a utility maximizer?\n",
      "\n",
      "Humans are the some of most intelligent structures that exist, and we don’t seem to fit that model very well. If fact, it seems the entire point in Rationalism is to improve our ability to do this, which has only been achieved with mixed success.\n",
      "\n",
      "Organisations of humans (e.g. USA, FDA, UN) have even more computational power and don’t seem to be doing much better.\n",
      "\n",
      "Perhaps an intelligence (artificial or natural) cannot necessarily, or even typically be described as optimisers? Instead we could only model them as an algorithm or as a collection of tools/behaviours executed in some pattern.\n",
      "\n",
      "ANSWER: This is an excellent question. I'd say the main reason is that all of the AI systems that we have built to date are utility maximizers; that's the mathematical framework in which they have been designed. Neural nets / deep-learning work by using a simple optimizer to find the minimum of a loss function via gradient descent. Evolutionary algorithms, simulated annealing, etc. find the minimum (or maximum) of a \"fitness function\". We don't know of any other way to build systems that learn.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is relevant because it explains an AGI is described as a utility maximizer because all of the AI systems we've built to date are utility maximizers and we don't know about any other way to build systems that learn.\n",
      "----------------------------------------------------\n",
      "Ground truth 4\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?\n",
      "\n",
      "ANSWER: Because we're building the AI from the ground up and can change what the AI is via our design choices. Humans' goal functions are basically decided by genetic accident, which is why humans are often counterproductive.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is relevant because it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.\n",
      "----------------------------------------------------\n",
      "Ground truth 5\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?\n",
      "\n",
      "ANSWER: When we're talking about aligning a structure, we are trying to make sure that it is leveled properly.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is not relevant because the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.\n",
      "----------------------------------------------------\n",
      "Ground truth 6\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Who lives in the Imperial Palace in Tokyo?\n",
      "\n",
      "ANSWER: The Tokyo Imperial Palace is the primary residence of the Emperor of Japan and the imperial family.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is relevant because it states who lives in the Tokyo Imperial Palace.\n",
      "----------------------------------------------------\n",
      "Ground truth 7\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Who lives in the Imperial Palace in Tokyo?\n",
      "\n",
      "ANSWER: Buckingham Palace is a London royal residence and the administrative headquarters of the monarch of the United Kingdom.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is not relevant because the answer is talking about the palace in London, UK while the question is about the Imperial Palace in Tokyo.\n",
      "----------------------------------------------------\n",
      "Ground truth 8\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Is there a way \"regular\" people can \"help\"? I'm a serial entrepreneur in my late 30s. I went through 80000 hours and they told me they would not coach me as my profile was not interesting. This was back in 2018 though.\n",
      "\n",
      "ANSWER: I think you could probably get more dates if you actually asked people out.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is not relevant because it's talking about dating, not about helping.\n",
      "----------------------------------------------------\n",
      "Ground truth 9\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Why does EY bring up \"orthogonality\" so early, and strongly (\"in denial\", \"and why they're true\") ? Why does it seem so important that it be accepted? thanks!\n",
      "\n",
      "ANSWER: Because it means you can't get AI to do good things \"for free,\" it has to be something you intentionally designed it to do.\n",
      "\n",
      "Denying the orthogonality thesis looks like claims that an AI built with one set of values will tend to change those values in a particular direction as it becomes cleverer. Because of wishful thinking, people usually try to think of reasons why an AI built in an unsafe way (with some broad distribution over possible values) will tend to end up being nice to humans (a narrow target of values) anyway.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is relevant because it explains why orthogonality thesis is important to accept as true when building AI.\n",
      "----------------------------------------------------\n",
      "Ground truth 10\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Why does Eliezer Yudkowsky bring up the \"orthogonality thesis\" so early, and strongly when talking about AI Safety? Why does it seem so important that it be accepted?\n",
      "\n",
      "ANSWER: Because it means you can't get AI to do good things \"for free,\" it has to be something you intentionally designed it to do.\n",
      "\n",
      "Denying the orthogonality thesis looks like claims that an AI built with one set of values will tend to change those values in a particular direction as it becomes cleverer. Because of wishful thinking, people usually try to think of reasons why an AI built in an unsafe way (with some broad distribution over possible values) will tend to end up being nice to humans (a narrow target of values) anyway.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is relevant because it explains why orthogonality thesis is important to accept as true when building AI.\n",
      "----------------------------------------------------\n",
      "Ground truth 11\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: In the context of AI risk, is working on better hardware computation dangerous? \n",
      "\n",
      "I'm specifically thinking about Next Silicon, they make chips that are very good at fast serial computation, but not for things like neural networks.\n",
      "\n",
      "ANSWER: Better hardware reduces the need for AI software to be efficient to be dangerous. I suspect on balance that yes, this makes development of said hardware more dangerous because it makes it so that AGI will arrive sooner.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is relevant because it explains that better compute hardware could make AI more dangerous by making it more effecient and therefore making AGI arrive sooner.\n",
      "----------------------------------------------------\n",
      "Ground truth 12\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: In the context of AI risk, is working on better hardware computation dangerous? \n",
      "\n",
      "I'm specifically thinking about Next Silicon, they make chips that are very good at fast serial computation, but not for things like neural networks.\n",
      "\n",
      "ANSWER: Yeah, the home hardware in Silicon Valley sells chips which they buy from a network of companies.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is not relevant because it talks about home hardware, which is a store that sells tools and building supplies, while the question is asking about how dangerous compute hardware is for AI risk.\n",
      "----------------------------------------------------\n",
      "Ground truth 13\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Who won the election for Mayor of Cleveland?\n",
      "\n",
      "ANSWER: Incumbent Democratic Mayor Frank G . Jackson\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is relevant because it says that Frank G. Jackson won the election for Mayor of Cleveland.\n",
      "----------------------------------------------------\n",
      "Ground truth 14\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Who won the election for Mayor of Cleveland?\n",
      "\n",
      "ANSWER: Julius Caesar\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is not relevant because Cleveland did not exist while Julius Caesar was alive.\n",
      "----------------------------------------------------\n",
      "Ground truth 15\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: Who won the election for Mayor of Cleveland?\n",
      "\n",
      "ANSWER: I once went on a trip to Cleveland too! I went to watch the NBA finals.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is not relevant because it's talking about going on a trip to cleveland and the NBA finals, not the elected Mayor in Cleveland.\n",
      "----------------------------------------------------\n",
      "Ground truth 16\n",
      "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
      "\n",
      "QUESTION: I went on a trip to Cleveland once and I was wondering who won the election for Mayor. Do you know?\n",
      "\n",
      "ANSWER: I once went on a trip to Cleveland too! I went to watch the NBA finals.\n",
      "\n",
      "Below contains an explanantion of why the answer above is or isn't relevant to the question:\n",
      "\n",
      "This answer is not relevant because the question asked who won the election for Mayor in Cleveland, but the person answering only mentioned they went on a trip to Cleveland too to watch the NBA finals.\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    print(f\"Ground truth {i}\")\n",
    "    print_ground_truth_from_df(df, i, f\"prompts/ground_truths/ground_truth_{i}.txt\")\n",
    "    print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('llm-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0018cf926da22f6d1ffb5833146b97eb719a0e11638c210f826ea2f33027bdd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
