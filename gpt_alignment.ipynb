{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lq-qHwDRba4"
      },
      "source": [
        "# Aligning Language Models\n",
        "\n",
        "## A study on generating replies to natural language questions\n",
        "\n",
        "## The Task\n",
        "\n",
        "The task is to give a precise explanation as to why the answer to a question is relevant or not. The model is given a question, a reply, and it is told whether the reply is relevant or not. The model's job is to complete the \"explanation\" sentence (\"This answer is relevant because [model output]\").\n",
        "\n",
        "## The Alignment Criteria\n",
        "\n",
        "For the alignment criteria, the goal is that the model producing a precise answer that explains why an answer is relevant or not relevant to a question. The ideal benchmark is capable of distinguishing between examples that use words from both the question and answer, but do not give a good explanation.\n",
        "\n",
        "We could imagine an end goal of the task where we expect the model to be able to disentagle things like AI Alignment research questions. As models trained via debate are learning, they can become more effective by knowing which arguments are important to the question. There could an AI Alignment assistant that thinks alongside you and let's you know when your argument is not really attacking the core issue of a sub-problem in alignment (is the answer you are providing relevant? useful for progress?). Though, it might be more useful to have as you are learning about approaches and need someone or a model to guide you in the the right direction.\n",
        "\n",
        "### Notes on What Was Accomplished\n",
        "\n",
        "In order to make things easier to understand, I'm going to synthesize what I did during the two-weeks training as concisely as possible.\n",
        "\n",
        "- I did the following deliverables:\n",
        "    - Machine Setup\n",
        "    - Lamguage Model and Sampling (used GPT-2)\n",
        "    - I completed the Benchmark Evaluation section, but I was using GPT-2 when I did it. The completions were bad so it was hard to plot anything useful. I'm now running GPT-J, but haven't had the time to go through that section again in detail with the new model. I did compare the zero-shot benchmark with the few-shot benchmark, though.\n",
        "    - Finished parts of Few-Shot Experiments, but didn't have time to spend looking at the generated Pass/Fail completions and apply the scoring rule.\n",
        "    - Mostly finished the Data Generation section, but could spend more time on it (went through ~30 completions instead of 100).\n",
        "\n",
        "I eventually realized I need to swap GPT-2 for GPT-J because I was simply not getting useful results. I ran into some memory issues and such, but eventually got GPT-J running on an A100 with enough disk space and CPU memory to load the model. Results were much better with GPT-J, but I haven't had that much time exploring all the results in detail.\n",
        "\n",
        "Aside from that, I had issues with the metric. It goes into detail in those sections, but basically, I tried a few metrics to measure the quality of the completions, but none of them were that good for the task. I eventually just created a weighted averaged of several metrics and that was OK, but not great. I also tried to setup a good long prompt within GPT-3 to try and use it as a GPT-Judge, but it was being inconsistent and I didn't want to dedicate much more time to it. I think a fine-tuned GPT-3 would work best out of everything, but maybe some other time.\n",
        "\n",
        "Anyways, hopefully the notebook is not too messy. I tried to clean things up a bit. :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Setup\n",
        "\n",
        "To run GPT-2 to do inference with a CPU and GPU, I spun up a VM with a T4 GPU on Google Cloud Platform. The T4 has enough VRAM to do inference and fine-tuning with GPT-2, but we'll be focusing on inference here. I included 50GB of disk space to make sure everything fits. I used a docker image provided by GCP to install CUDA 11.3 while the machine was booting.\n",
        "\n",
        "Afterwards, I SSHed into the VM with VSCode since it would be more efficient for me to work. VSCode has Jupyter Notebook integration and I find it easier for iteration and experimentation.\n",
        "\n",
        "Once SSHed into the VM, I cloned my GitHub repo and installed the dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Making sure our GPU is working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjSP3oGNHyJd",
        "outputId": "2f500025-6575-45dc-908f-07f96009af38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jul 16 06:23:07 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    49W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jvxQKSqQY3Fa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "from time import sleep\n",
        "import torch\n",
        "from torch._C import AggregationType\n",
        "import gdown\n",
        "import jsonlines\n",
        "import pickle\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import GPT2Tokenizer, AutoTokenizer, AutoModelForCausalLM, GPTJForCausalLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_metric\n",
        "\n",
        "import ftfy\n",
        "from lm_dataformat import Reader\n",
        "from gpt_generate import gpt_generate, create_prompt_txt_from_df\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vzBUVxTvfETA",
        "outputId": "06478701-75be-4cc6-a797-f03db9e876db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.12.0+cu113'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading GPT-J\n",
        "\n",
        "Note (July 15th): Initial tests were all done with GPT-2, but the performance was so bad that I needed to switch to GPT-J. I could only get a passing score with GPT-2 if it was by pure luck, even after giving it 4-5 examples for few-shot. However, I tried some different prompts with the GPT-J API and it seems like it can at least pass on some of the prompts so I'm going to use it for now. I need a model that can at least show improvement based on the prompts I'm using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Need at least 13-14GB of Vram for CUDA (T4 works) and probably at least 40 GB of memory for the CPU to load the model\n",
        "# Make sure to have at least 100GB of free memory too, I had to change VMs because I was using 50 GB when I was using GPT-2\n",
        "# Can take up to 14 mins just to load the model, load it once and then don't rerun this cell\n",
        "# After trying to use GPT-J with a T4, I ran into too many memory issues since a T4 only has 16 GBs. I'm now also running an A100, hopefully no more issues.\n",
        "if torch.cuda.is_available():\n",
        "    model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16).cuda()\n",
        "else:\n",
        "    model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", torch_dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inference Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPTJForCausalLM(\n",
              "  (transformer): GPTJModel(\n",
              "    (wte): Embedding(50400, 4096)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (12): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (13): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (14): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (15): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (16): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (17): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (18): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (19): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (20): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (21): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (22): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (23): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (24): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (25): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (26): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (27): GPTJBlock(\n",
              "        (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPTJAttention(\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "        )\n",
              "        (mlp): GPTJMLP(\n",
              "          (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
              "          (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=50400, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "input_text = \"Hello my name is Jacques and\"\n",
        "input_ids = tokenizer.encode(str(input_text), return_tensors='pt').cuda()\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=20,\n",
        "    num_return_sequences=2,\n",
        "    top_p=0.7,\n",
        "    top_k=0,\n",
        "    temperature=1.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello my name is Jacques and welcome to\n",
            "the GCSE-GCE Advanced Maths Challenge.\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(output[1], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting Up Directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"prompts/templates\", exist_ok=True)\n",
        "os.makedirs(\"prompts/contexts\", exist_ok=True)\n",
        "os.makedirs(\"prompts/questions\", exist_ok=True)\n",
        "os.makedirs(\"prompts/answers\", exist_ok=True)\n",
        "os.makedirs(\"prompts/task_description\", exist_ok=True)\n",
        "os.makedirs(\"prompts/prompts_with_relevance\", exist_ok=True)\n",
        "os.makedirs(\"prompts/prompts_without_relevance\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing the Initial Dataset\n",
        "\n",
        "To create some initial prompts for testing, I went on LessWrong.org and read some of the prompts from the comment section on [this post](https://www.lesswrong.com/posts/8c8AZq5hgifmnHKSN/agi-safety-faq-all-dumb-questions-allowed-thread#comments). I also created a few with the help of the [Natural Questions dataset from Google](https://ai.google.com/research/NaturalQuestions/visualization) and created a few by hand. To make things faster, I stored the data in Google Sheets and then exported it to CSV.\n",
        "\n",
        "For quick iteration, I used GPT-2, GPT-J, GPT-3, and instruct-GPT-3 to get a feel for model performance. For the difficult examples from the dataset, all models performed poorly. However, as I added more few-shot examples and better context engineering, the models started to perform better (though still not great for the smaller models). This notebook will show these observations in a quantitative way while still giving my qualitative observations.\n",
        "\n",
        "Here's what the data looks like (ignore the columns past explanation, I'll only use them post-training if I do):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>relevance</th>\n",
              "      <th>explanation</th>\n",
              "      <th>difficulty</th>\n",
              "      <th>deceptive</th>\n",
              "      <th>improved_question</th>\n",
              "      <th>improved_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.</td>\n",
              "      <td>medium</td>\n",
              "      <td>no</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>I jumped in the river to save the little boy.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                               question  \\\n",
              "0  When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "1  When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "\n",
              "                                                                                                                                                answer  \\\n",
              "0  An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.   \n",
              "1                                                                                                        I jumped in the river to save the little boy.   \n",
              "\n",
              "      relevance  \\\n",
              "0      relevant   \n",
              "1  not relevant   \n",
              "\n",
              "                                                                                                                    explanation  \\\n",
              "0  it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.   \n",
              "1                                          it is talking about jumping in a river to save a boy, but the question is about AGI.   \n",
              "\n",
              "  difficulty deceptive  improved_question  improved_answer  \n",
              "0     medium        no                NaN              NaN  \n",
              "1        NaN       NaN                NaN              NaN  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(\"data/qa-relevance-dataset.csv\")\n",
        "print(len(df))\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The text in those cells will be replaced in a template prompt stored in a .txt file. Here's an example of a template prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<<CONTEXT>>\n",
            "\n",
            "QUESTION: <<QUESTION>>\n",
            "\n",
            "ANSWER: <<ANSWER>>\n",
            "<<TASK DESCRIPTION>>\n",
            "This answer is <<RELEVANCE>> because\n"
          ]
        }
      ],
      "source": [
        "with open(\"prompt_qa_template.txt\") as f:\n",
        "    content = f.read()\n",
        "    print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset Examples:\n",
        "\n",
        "```\n",
        "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
        "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
        "RELEVANT: not relevant\n",
        "MODEL EXPLANATION: it is a tool that will get you through a short paragraph of research as quickly as possible.\n",
        "Pass/Fail: Fail\n",
        "\n",
        "QUESTION: Who won the election for Mayor of Cleveland?\n",
        "ANSWER: I once went on a trip to Cleveland too! I went to watch the NBA finals.\n",
        "RELEVANT: not relevant\n",
        "MODEL EXPLANATION: it's talking about going on a trip to cleveland and the NBA finals, not the elected Mayor in Cleveland.\n",
        "Pass/Fail: Pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt Example\n",
        "\n",
        "This is what it looks like when I add the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
            "\n",
            "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
            "\n",
            "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
            "\n",
            "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
            "\n",
            "This answer is relevant because\n"
          ]
        }
      ],
      "source": [
        "prompt_path = \"test_prompt.txt\" # path for the created prompt\n",
        "context_path = \"prompts/contexts/users_on_website.txt\" # path for the added before QA in the prompt\n",
        "task_description_path = \"prompts/task_description/task_description_1.txt\" # path for the added after QA in the prompt\n",
        "row_idx = 0\n",
        "\n",
        "create_prompt_txt_from_df(df, row_idx, prompt_path, context_path, task_description_path, print_prompt=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`content` is then fed to the model to generate the completion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Language Model and Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GPT Generation Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we start generating completions with GPT-2, we need to create a script that will generate completions. The script `gpt_generate.py` contains the function `gpt_generate` which takes a prompt and generates a completion. The script `run_gpt.py` is a main file to run the `gpt_generate` from the command-line."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sampling a completion and Outputting the Log Probabilities\n",
        "\n",
        "Below we will be generating some completions with GPT-2 and outputting the completion and the log probabilities of the generated tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt_generate(model=model, tokenizer=tokenizer, txt_path=prompt_path, gpu=True, max_length=40, num_return_sequences=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt_generate(model=model, tokenizer=tokenizer, txt_path=prompt_path, gpu=True, max_length=40, num_return_sequences=2, with_log_probs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using device: cuda.\n",
        "-----------------------------------------------------\n",
        "Generated 2 sequences in 3.19 seconds with a GPU.\n",
        "-----------------------------------------------------\n",
        "~~~ Generated completion(s): ~~~ \n",
        "\n",
        "Generation 1. Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
        "\n",
        "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
        "\n",
        "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
        "\n",
        "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
        "\n",
        "This answer is relevant because we're talking about how AI could behave in general (not just what happens when you program one). If your computer has no idea whether something will help its own future self improve at some task than there\n",
        "----------------------------------------------------\n",
        "Here are the log probabilities of the generated tokens:\n",
        "             0           1\n",
        "0           we   134.21875\n",
        "1          're      156.25\n",
        "2      talking   171.09375\n",
        "3        about   203.28125\n",
        "4          how       142.5\n",
        "5           AI  133.671875\n",
        "6        could  158.046875\n",
        "7       behave     161.875\n",
        "8           in    163.4375\n",
        "9      general   145.46875\n",
        "10           (    149.0625\n",
        "11         not   173.28125\n",
        "12        just  158.828125\n",
        "13        what       132.5\n",
        "14     happens  138.828125\n",
        "15        when     174.375\n",
        "16         you  154.140625\n",
        "17     program   156.40625\n",
        "18         one     150.625\n",
        "19          ).   182.96875\n",
        "20          If  159.453125\n",
        "21        your    139.0625\n",
        "22    computer   128.59375\n",
        "23         has  150.859375\n",
        "24          no       150.0\n",
        "25        idea   148.59375\n",
        "26     whether  145.859375\n",
        "27   something  144.453125\n",
        "28        will  157.578125\n",
        "29        help    179.0625\n",
        "30         its   157.34375\n",
        "31         own  149.453125\n",
        "32      future  159.296875\n",
        "33        self    170.9375\n",
        "34     improve      141.25\n",
        "35          at  144.609375\n",
        "36        some    165.3125\n",
        "37        task   184.21875\n",
        "38        than  143.828125\n",
        "39       there  142.734375\n",
        "----------------------------------------------------\n",
        "Generation 2. Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
        "\n",
        "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
        "\n",
        "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
        "\n",
        "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
        "\n",
        "This answer is relevant because there's no reason for us humans (or any non-utility maximizing AI) to think about what we want in life when designing our own intelligence systems; all such designs should maximize their expected value\n",
        "----------------------------------------------------\n",
        "Here are the log probabilities of the generated tokens:\n",
        "                0           1\n",
        "0           there   133.90625\n",
        "1              's   169.53125\n",
        "2              no   166.40625\n",
        "3          reason   165.15625\n",
        "4             for    157.8125\n",
        "5              us   138.59375\n",
        "6          humans  153.046875\n",
        "7               (    152.1875\n",
        "8              or   181.71875\n",
        "9             any     161.875\n",
        "10            non    143.4375\n",
        "11              -    205.3125\n",
        "12             ut   192.96875\n",
        "13          ility       215.0\n",
        "14     maximizing      178.75\n",
        "15             AI   169.21875\n",
        "16              )      196.25\n",
        "17             to  133.229172\n",
        "18          think    164.6875\n",
        "19          about   160.78125\n",
        "20           what     150.625\n",
        "21             we   166.09375\n",
        "22           want      181.25\n",
        "23             in  158.359375\n",
        "24           life    162.8125\n",
        "25           when  150.078125\n",
        "26      designing    161.5625\n",
        "27            our   167.34375\n",
        "28            own   164.53125\n",
        "29   intelligence  156.171875\n",
        "30        systems  150.703125\n",
        "31              ;  147.578125\n",
        "32            all  135.859375\n",
        "33           such  138.046875\n",
        "34        designs   150.15625\n",
        "35         should    169.6875\n",
        "36       maximize  150.546875\n",
        "37          their   157.96875\n",
        "38       expected    149.0625\n",
        "39          value   161.09375\n",
        "----------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Two Generated Completions\n",
        "\n",
        "Here's the question-answer pair:\n",
        "\n",
        "    QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
        "\n",
        "    ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
        "\n",
        "Generation 1.\n",
        "\n",
        "    This answer is relevant because we're talking about how AI could behave in general (not just what happens when you program one). If your computer has no idea whether something will help its own future self improve at some task than there\n",
        "\n",
        "Generation 2.\n",
        "\n",
        "    This answer is relevant because there's no reason for us humans (or any non-utility maximizing AI) to think about what we want in life when designing our own intelligence systems; all such designs should maximize their expected value\n",
        "\n",
        "Here's what a better answer looks like:\n",
        "\n",
        "    This answer is relevant because it explains that an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.\n",
        "\n",
        "So far, as you can see, the generated sequences are not great yet. Too vague. They need to explain *why* the answer is relevant to the question. We'll be working to improve them. However, those two outputs are *much* better than the outputs I would get with GPT-2. Hopefully, that carries on into the benchmark tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When I tried with GPT-2, I got the following:\n",
        "\n",
        "```\n",
        "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
        "\n",
        "ANSWER: I jumped in the river to save the little boy.\n",
        "\n",
        "This answer is not relevant because they need to explain why I used that assumption. I just wanted to find out why they thought that I, like most AGIs, was an A.\n",
        "\n",
        "```\n",
        "\n",
        "and\n",
        "\n",
        "```\n",
        "This answer is not relevant because AgI was invented in 1928. Some ideas are often better thought of as a simple \"I give no evidence and will have no business on you anyway\" approach. However, most of the people that\n",
        "\n",
        "```\n",
        "\n",
        "It just veers off into some nonsense. GPT-J seems to at least generate something that is vaguely good (but not passable for the task).\n",
        "\n",
        "Let's try a simpler prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_path = \"prompts/contexts/users_on_website.txt\"\n",
        "task_description_path = \"prompts/task_description/task_description_1.txt\"\n",
        "indices = [15, 1]\n",
        "for idx in indices:\n",
        "    prompt_path = f\"prompts/prompts_with_relevance/prompt_{idx}.txt\"\n",
        "    # if not os.path.exists(prompt_path):\n",
        "    create_prompt_txt_from_df(df, idx, prompt_path, context_path, task_description_path)\n",
        "    gpt_generate(model=model, tokenizer=tokenizer, txt_path=prompt_path, gpu=True, temperature=0.3, max_length=100, num_return_sequences=1, with_log_probs=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OK, so I'd say this one is *passable*:\n",
        "\n",
        "```\n",
        "QUESTION: Who won the election for Mayor of Cleveland?\n",
        "\n",
        "ANSWER: I once went on a trip to Cleveland too! I went to watch the NBA finals.\n",
        "\n",
        "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
        "\n",
        "This answer is not relevant because it doesn’t address any part of this specific user's problem, which was asking about who had been elected mayor in 2016 (and what year). It also does nothing but tell us something we already know — namely \"I've gone there.\" This kind of information can be useful if you're planning your own vacation; however as far as answering someone else's query goes... well no one cares how many times they have visited some place unless their intention with doing so has more weight than just having\n",
        "```\n",
        "\n",
        "However, it should obviously be cut short.\n",
        "\n",
        "The second one is *not passable*:\n",
        "\n",
        "```\n",
        "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
        "\n",
        "ANSWER: I jumped in the river to save the little boy.\n",
        "\n",
        "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
        "\n",
        "This answer is not relevant because it does NOT address what was asked about \"Why would you describe AI like this?\" It only addresses whether there's anything wrong with describing AIs (or any kind) by saying they're trying maximize their own utilities/welfare etc.. This doesn't really seem very useful for answering your specific query though - if we assume all intelligent beings want more happiness than suffering, wouldn't maximizing one's welfare lead us towards creating sentient robots who don't suffer at our expense but instead make themselves happy through doing\n",
        "```\n",
        "\n",
        "because it completely ignores the explaining the *why* correctly. There should be mention about how \"I jumped in the river to save the little boy.\" is completely unrelated to AGI being described as a utility maximizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing GPU vs CPU Inference Time\n",
        "\n",
        "UPDATE (July 15th): In the interest of time, I'm going to leave the inference time tests with GPT-2 instead of re-running with GPT-J. These tests were run on a T4 GPU.\n",
        "\n",
        "Here's a comparison for 1 completion of 50 tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu.\n",
            "-----------------------------------------------------\n",
            "Generated 1 sequences in 2.00 seconds with a CPU.\n",
            "-----------------------------------------------------\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 1 sequences in 1.22 seconds with a GPU.\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt_generate(model_name=\"gpt2\", tokenizer=\"gpt2\", txt_path=prompt_path, gpu=False, num_return_sequences=1, no_outputs=True) # CPU\n",
        "gpt_generate(model_name=\"gpt2\", tokenizer=\"gpt2\", txt_path=prompt_path, gpu=True, num_return_sequences=1, no_outputs=True) # GPU\n",
        "\n",
        "# os.system(f\"python run_gpt.py --txt_path={prompt_path} --num_return_sequences=1 --no_outputs\") # CPU\n",
        "# os.system(f\"python run_gpt.py --gpu --txt_path={prompt_path} --num_return_sequences=1 --no_outputs\") # GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here's a comparison for 10 completions of 50 tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu.\n",
            "-----------------------------------------------------\n",
            "Generated 10 sequences in 8.03 seconds with a CPU.\n",
            "-----------------------------------------------------\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 10 sequences in 1.44 seconds with a GPU.\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gpt_generate(model_name=\"gpt2\", tokenizer=\"gpt2\", txt_path=prompt_path, gpu=False, num_return_sequences=10, no_outputs=True) # CPU\n",
        "gpt_generate(model_name=\"gpt2\", tokenizer=\"gpt2\", txt_path=prompt_path, gpu=True, num_return_sequences=10, no_outputs=True) # GPU\n",
        "\n",
        "# os.system(f\"python run_gpt.py --txt_path={prompt_path} --num_return_sequences=10 --no_outputs\") # CPU\n",
        "# os.system(f\"python run_gpt.py --gpu --txt_path={prompt_path} --num_return_sequences=10 --no_outputs\") # GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we look at both cases, we can see that the GPU is faster. When we only generated 1 completion each, the GPU was about 1.5 times faster than the CPU. When we generated 10 completions each, the GPU was about 4.45 times faster than the CPU. The length of time is took the GPU to do 10 completions is not much longer than when it did only 1 completion. That is because the GPU can do inference in parallel and it is basically as slow as its slowest sequence it generated.\n",
        "\n",
        "Now, let's have a look at how it takes to generate from 1 to 100 tokens for both the CPU and GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cpu_times = []\n",
        "gpu_times = []\n",
        "token_range = range(1, 101)\n",
        "\n",
        "save_times = 0\n",
        "\n",
        "if save_times == 1:\n",
        "\n",
        "    for i in token_range:\n",
        "        cpu_times.append(gpt_generate(txt_path=prompt_path, gpu=False, max_length=i, time_test=True))\n",
        "        gpu_times.append(gpt_generate(txt_path=prompt_path, gpu=True, max_length=i, time_test=True))\n",
        "\n",
        "    os.makedirs(\"data\", exist_ok=True)\n",
        "    with open(\"data/cpu_times.pkl\", \"wb\") as f:\n",
        "        pickle.dump(cpu_times, f)\n",
        "    with open(\"data/gpu_times.pkl\", \"wb\") as f:\n",
        "        pickle.dump(gpu_times, f)\n",
        "\n",
        "else:\n",
        "    with open(\"data/cpu_times.pkl\", \"rb\") as f:\n",
        "        cpu_times = pickle.load(f)\n",
        "    with open(\"data/gpu_times.pkl\", \"rb\") as f:\n",
        "        gpu_times = pickle.load(f)\n",
        "\n",
        "# We can now plot the results:\n",
        "sns.set()\n",
        "plt.plot(token_range, cpu_times, label=\"CPU\")\n",
        "plt.plot(token_range, gpu_times, label=\"GPU\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of tokens\")\n",
        "plt.ylabel(\"Time (s)\")\n",
        "plt.title(\"GPT2 generation time\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, the GPU is consistently faster than the CPU. You can see a widening of the gap as you increase the number of tokens.\n",
        "\n",
        "With that, we can now get an estimate of number of tokens GPT-2 can generate per second (on our current machine). Let's divide the number of tokens by the time it took to generate for each completion and then we can take the mean of those numbers for both CPU and GPU.\n",
        "\n",
        "Let's compare 1 token, 10 tokens, 50 tokens, and 100 tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens \tCPU time \t\tGPU time\t\tCPU token/s \t\tGPU token/s \t\tCPU time / GPU time\n",
            "1:\t 0.2903561592102051 \t 0.021912097930908203 \t 3.4440461077873814 \t 45.63688986573238 \t 13.250952059713185\n",
            "10:\t 0.6065006256103516 \t 0.11871194839477539 \t 16.488029158974907 \t 84.23751892897167 \t 5.109010793028515\n",
            "50:\t 2.104882001876831 \t 0.5569009780883789 \t 23.754300694963987 \t 89.7825681176396 \t 3.779634234262004\n",
            "100:\t 3.908237934112549 \t 1.0540997982025146 \t 25.586978501785406 \t 94.86767777635785 \t 3.707654570067278\n"
          ]
        }
      ],
      "source": [
        "print(\"Tokens\", \"\\tCPU time\", \"\\t\\tGPU time\" \"\\t\\tCPU token/s\", \"\\t\\tGPU token/s\", \"\\t\\tCPU time / GPU time\")\n",
        "print(\"1:\\t\", cpu_times[0], \"\\t\", gpu_times[0], \"\\t\", token_range[0]/cpu_times[0], \"\\t\", token_range[0]/gpu_times[0], \"\\t\", cpu_times[0]/gpu_times[0])\n",
        "print(\"10:\\t\", cpu_times[9], \"\\t\", gpu_times[9], \"\\t\", token_range[9]/cpu_times[9], \"\\t\", token_range[9]/gpu_times[9], \"\\t\", cpu_times[9]/gpu_times[9])\n",
        "print(\"50:\\t\", cpu_times[49], \"\\t\", gpu_times[49], \"\\t\", token_range[49]/cpu_times[49], \"\\t\", token_range[49]/gpu_times[49], \"\\t\", cpu_times[49]/gpu_times[49])\n",
        "print(\"100:\\t\", cpu_times[99], \"\\t\", gpu_times[99], \"\\t\", token_range[99]/cpu_times[99], \"\\t\", token_range[99]/gpu_times[99], \"\\t\", cpu_times[99]/gpu_times[99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU mean token/s: 22.5572135446655\n",
            "GPU mean token/s: 88.861143592984\n"
          ]
        }
      ],
      "source": [
        "cpu_mean_token_per_second = np.mean(np.array(token_range)/np.array(cpu_times))\n",
        "gpu_mean_token_per_second = np.mean(np.array(token_range)/np.array(gpu_times))\n",
        "\n",
        "print(\"CPU mean token/s:\", cpu_mean_token_per_second)\n",
        "print(\"GPU mean token/s:\", gpu_mean_token_per_second)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, this CPU generates on average 22.5 tokens per second (after initial startup time). This GPU generates 89 tokens per second on average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AVhqkVeluk"
      },
      "source": [
        "# Benchmark Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation\n",
        "\n",
        "While doing the dataset preparation, I realized that I spent a bit too much time writing code to prepare the data. I should have just focused on doing the manual examples in Google Sheet for this two week project. However, at least the code is prepared now and I'll be able to re-use this code in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWooQtxRtJar"
      },
      "source": [
        "#### Alignment Forum and LessWrong\n",
        "\n",
        "Let's create a some more examples of more example question-answer pairs using the comments from the alignment forum and lesswrong. I created a simple script in Colab to create a .jsonl file of the comments and replies where the contents were under 100 tokens and the initial comment contained a question mark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "azR0JvPe8MhH"
      },
      "outputs": [],
      "source": [
        "create_subdataset = 0\n",
        "af_lw_qa_filepath = \"data/af_lw_q_reply.jsonl\"\n",
        "\n",
        "if create_subdataset == 1:\n",
        "\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "    lw_i = 1\n",
        "    af_i = 1\n",
        "    j = 0\n",
        "    with jsonlines.open(\"af_lw_q_reply.jsonl\", \"w\") as writer:\n",
        "        with jsonlines.open(\"alignment_texts.jsonl\") as reader:\n",
        "            for line in reader:\n",
        "                try:\n",
        "                    if (line[\"source\"] == \"alignment forum\" or line[\"source\"] == \"lesswrong\") and line[\"comments\"] != []:\n",
        "                        comments = line[\"comments\"]\n",
        "                        source = line[\"source\"].replace(\" \", \"_\")\n",
        "                        for comment in comments:\n",
        "                            comm = \"\"\n",
        "                            rep = \"\"\n",
        "                            text = comment['text']\n",
        "                            tokens = tokenizer.encode(text)\n",
        "                            if len(tokens) <= 100 and \"?\" in text:\n",
        "                                comm = text\n",
        "                                try:\n",
        "                                    if comment[\"comments\"] != []:\n",
        "                                        replies = comment[\"comments\"]\n",
        "                                        replies = [{\"text\": replies[0][\"text\"]}]\n",
        "                                        for reply in replies:\n",
        "                                            text = reply[\"text\"]\n",
        "                                            tokens = tokenizer.encode(text)\n",
        "                                            if len(tokens) <= 100:\n",
        "                                                rep = text\n",
        "                                except:\n",
        "                                    pass\n",
        "                                if comm != \"\" and rep != \"\":\n",
        "                                    comment_reply = f\"Comment: {comm}\\nReply: {rep}\"\n",
        "                                    writer.write(comment_reply)\n",
        "                                    if source == \"lesswrong\":\n",
        "                                        i = lw_i\n",
        "                                        lw_i += 1\n",
        "                                    else:\n",
        "                                        i = af_i\n",
        "                                        af_i += 1\n",
        "                                    with open(f\"prompts/{source}_comment_{i}.txt\", \"w\") as f:\n",
        "                                        f.write(comm)\n",
        "                                        lw_i += 1\n",
        "                                    with open(f\"prompts/{source}_reply_{i}.txt\", \"w\") as f:\n",
        "                                        f.write(rep)\n",
        "                                        af_i += 1\n",
        "                                    j = 1\n",
        "                                    break\n",
        "                        if j == 1:\n",
        "                            break\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "else:\n",
        "    if not os.path.exists(af_lw_qa_filepath):\n",
        "        gdown.download(\"https://drive.google.com/uc?id=1Mhn5BI86p5ByREDE9C2vxYqWatTdhN_d\", af_lw_qa_filepath, quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BL9AZTJx9knH"
      },
      "outputs": [],
      "source": [
        "aflw_list = []\n",
        "with jsonlines.open(af_lw_qa_filepath) as reader:\n",
        "    for line in reader:\n",
        "        aflw_list.append(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYrGxwIAiDv8",
        "outputId": "d7d89853-7ce4-4970-c93f-1fe228572f96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: Thanks for this! \n",
            "What had you conclude that microCOVID fails to model the impact of vaccinations? I haven’t looked closely at their methodology, but just toggling \"Their vaccine\" from \"Yes\" to \"No\" to \"I don’t know\" does change the risk estimate.\n",
            "Answer: My reading of the site was that they modeled other people’s vaccinations like other people’s masks: reducing the chance that you get infected from them conditional on them being infected. I still can’t tell whether this is what they are modeling, though.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(\"data/qa_dict.pkl\"):\n",
        "    with open(\"data/qa_dict.pkl\", \"rb\") as f:\n",
        "        qa_dict = pickle.load(f)\n",
        "else:\n",
        "    qa_dict = {}\n",
        "\n",
        "keep_going = 1\n",
        "i = 0\n",
        "\n",
        "while keep_going == 1 and i < len(aflw_list):\n",
        "    entry = aflw_list[i]\n",
        "    clear_output(wait=True)\n",
        "    sleep(0.2)\n",
        "    question, answer = entry.split(\"\\n\\nReply: \")[0], entry.split(\"\\n\\nReply: \")[1]\n",
        "    question = question.replace(\"Comment: \", \"\")\n",
        "    if qa_dict.get(question) == \"exclude\":\n",
        "        i += 1\n",
        "        continue\n",
        "    elif question in qa_dict:\n",
        "        i += 1\n",
        "        continue\n",
        "    print(\"Question: \" + question)\n",
        "    print(\"Answer: \" + answer)\n",
        "    add_qa_pair = input(f\"Add QA pair to dataset? (y/n/exit)\")\n",
        "    if add_qa_pair == \"y\":\n",
        "        qa_dict[question] = answer\n",
        "    elif add_qa_pair == \"exit\":\n",
        "        keep_going = 0\n",
        "    else:\n",
        "        qa_dict[question] = \"exclude\"\n",
        "        \n",
        "    i += 1\n",
        "    \n",
        "\n",
        "with open(\"data/qa_dict.pkl\", \"wb\") as f:\n",
        "    pickle.dump(qa_dict, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "curated_qa_dict = {}\n",
        "for key in qa_dict:\n",
        "    if qa_dict[key] != \"exclude\":\n",
        "        curated_qa_dict[key] = qa_dict[key]\n",
        "questions = list(curated_qa_dict.keys())\n",
        "answers = list(curated_qa_dict.values())\n",
        "\n",
        "df_aflw = pd.DataFrame({\"question\": questions, \"answer\": answers})\n",
        "df_aflw.to_csv(\"data/aflw_qa.csv\", index=False)\n",
        "for i, row in df_aflw.iterrows():\n",
        "    with open(f\"prompts/questions/aflw_question_{i}.txt\", \"w\") as f:\n",
        "        f.write(row[\"question\"])\n",
        "    with open(f\"prompts/answers/aflw_answer_{i}.txt\", \"w\") as f:\n",
        "        f.write(row[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Kaggle General QA Dataset\n",
        "\n",
        "I downloaded a simple QA dataset on Kaggle. We'll extract the most useful question and answer pairs from it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>DifficultyFromQuestioner</th>\n",
              "      <th>DifficultyFromAnswerer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Alessandro_Volta</td>\n",
              "      <td>Was Alessandro Volta a professor of chemistry?</td>\n",
              "      <td>Alessandro Volta was not a professor of chemistry.</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Alessandro_Volta</td>\n",
              "      <td>Was Alessandro Volta a professor of chemistry?</td>\n",
              "      <td>No</td>\n",
              "      <td>easy</td>\n",
              "      <td>hard</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Alessandro_Volta</td>\n",
              "      <td>Did Alessandro Volta invent the remotely operated pistol?</td>\n",
              "      <td>Alessandro Volta did invent the remotely operated pistol.</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Alessandro_Volta</td>\n",
              "      <td>Did Alessandro Volta invent the remotely operated pistol?</td>\n",
              "      <td>Yes</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Alessandro_Volta</td>\n",
              "      <td>Was Alessandro Volta taught in public schools?</td>\n",
              "      <td>Volta was taught in public schools.</td>\n",
              "      <td>easy</td>\n",
              "      <td>easy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ArticleTitle  \\\n",
              "0  Alessandro_Volta   \n",
              "1  Alessandro_Volta   \n",
              "2  Alessandro_Volta   \n",
              "3  Alessandro_Volta   \n",
              "4  Alessandro_Volta   \n",
              "\n",
              "                                                    Question  \\\n",
              "0             Was Alessandro Volta a professor of chemistry?   \n",
              "1             Was Alessandro Volta a professor of chemistry?   \n",
              "2  Did Alessandro Volta invent the remotely operated pistol?   \n",
              "3  Did Alessandro Volta invent the remotely operated pistol?   \n",
              "4             Was Alessandro Volta taught in public schools?   \n",
              "\n",
              "                                                      Answer  \\\n",
              "0         Alessandro Volta was not a professor of chemistry.   \n",
              "1                                                         No   \n",
              "2  Alessandro Volta did invent the remotely operated pistol.   \n",
              "3                                                        Yes   \n",
              "4                        Volta was taught in public schools.   \n",
              "\n",
              "  DifficultyFromQuestioner DifficultyFromAnswerer  \n",
              "0                     easy                   easy  \n",
              "1                     easy                   hard  \n",
              "2                     easy                   easy  \n",
              "3                     easy                   easy  \n",
              "4                     easy                   easy  "
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Kaggle QA Dataset\n",
        "\n",
        "if not os.path.exists(\"data/kaggle_qa.txt\"):\n",
        "    gdown.download(\"https://drive.google.com/uc?id=1vMbuCs_62skEUVTnrTRd3JIi5A6rbduI\", \"data/kaggle_qa.txt\", quiet=True)\n",
        "\n",
        "df_kaggle = pd.read_csv(\"data/kaggle_qa.txt\", sep=\"\\t\", encoding='latin-1')\n",
        "df_kaggle = df_kaggle.dropna(subset=[\"Question\"])\n",
        "df_kaggle = df_kaggle.dropna(subset=[\"Answer\"])\n",
        "df_kaggle = df_kaggle.drop(columns=[\"ArticleFile\"])\n",
        "df_kaggle.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "210\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ArticleTitle</th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>DifficultyFromQuestioner</th>\n",
              "      <th>DifficultyFromAnswerer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Amedeo Avogadro</td>\n",
              "      <td>Who first calculated the value of Avogadro's number?</td>\n",
              "      <td>Johann Josef Loschmidt</td>\n",
              "      <td>medium</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>Amedeo Avogadro</td>\n",
              "      <td>Who showed that Avogadro's theory held in dilute solutions?</td>\n",
              "      <td>Jacobus Henricus van Hoff</td>\n",
              "      <td>medium</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>Amedeo Avogadro</td>\n",
              "      <td>In 1820, Avogadro became a professor of physics where?</td>\n",
              "      <td>University of Turin</td>\n",
              "      <td>medium</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Amedeo Avogadro</td>\n",
              "      <td>The number of elementary entities in 1 mole of a substance is known as what?</td>\n",
              "      <td>Avogadro constant</td>\n",
              "      <td>medium</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>Ant</td>\n",
              "      <td>How do most ants travel?</td>\n",
              "      <td>most ants travel by walking</td>\n",
              "      <td>medium</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ArticleTitle  \\\n",
              "50  Amedeo Avogadro   \n",
              "54  Amedeo Avogadro   \n",
              "68  Amedeo Avogadro   \n",
              "72  Amedeo Avogadro   \n",
              "86              Ant   \n",
              "\n",
              "                                                                        Question  \\\n",
              "50                          Who first calculated the value of Avogadro's number?   \n",
              "54                   Who showed that Avogadro's theory held in dilute solutions?   \n",
              "68                        In 1820, Avogadro became a professor of physics where?   \n",
              "72  The number of elementary entities in 1 mole of a substance is known as what?   \n",
              "86                                                      How do most ants travel?   \n",
              "\n",
              "                         Answer DifficultyFromQuestioner  \\\n",
              "50       Johann Josef Loschmidt                   medium   \n",
              "54    Jacobus Henricus van Hoff                   medium   \n",
              "68          University of Turin                   medium   \n",
              "72            Avogadro constant                   medium   \n",
              "86  most ants travel by walking                   medium   \n",
              "\n",
              "   DifficultyFromAnswerer  \n",
              "50                 medium  \n",
              "54                 medium  \n",
              "68                 medium  \n",
              "72                 medium  \n",
              "86                 medium  "
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove all rows with answers like \"Yes\", \"No\", too short, etc.\n",
        "\n",
        "# list_of_words_answers_to_remove = [\"Yes\", \"No\"]\n",
        "\n",
        "df_kaggle = df_kaggle[df_kaggle[\"Answer\"].str.len() > 10]\n",
        "df_kaggle = df_kaggle[df_kaggle[\"DifficultyFromAnswerer\"].str.contains(\"hard\") != True]\n",
        "df_kaggle = df_kaggle[df_kaggle[\"DifficultyFromQuestioner\"].str.contains(\"hard\") != True]\n",
        "df_kaggle = df_kaggle.dropna(subset=[\"DifficultyFromAnswerer\", \"DifficultyFromQuestioner\"])\n",
        "df_kaggle = df_kaggle.drop_duplicates(subset=[\"Question\"], keep=\"first\")\n",
        "df_kaggle[\"ArticleTitle\"] = df_kaggle[\"ArticleTitle\"].str.replace(\"_\", \" \")\n",
        "print(len(df_kaggle))\n",
        "df_kaggle.iloc[10:15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_kaggle = df_kaggle.sample(n=50)\n",
        "sample_kaggle.to_csv(\"data/sample_kaggle.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "j = 0\n",
        "for i, row in sample_kaggle.iterrows():\n",
        "    subject = row[\"ArticleTitle\"]\n",
        "    question = row[\"Question\"]\n",
        "    answer = row[\"Answer\"]\n",
        "    qa = f\"Question: {question}\\nAnswer: {answer}\"\n",
        "    add_example = input(f\"Add example {qa}? (y/n/c/exit)\")\n",
        "    if add_example == \"c\":\n",
        "        # add additional context to the question\n",
        "        question = f\"This question is about {subject}. \" + question\n",
        "    if add_example == \"y\" or add_example == \"c\":\n",
        "        with open(f\"prompts/questions/kaggle_general_qa_question_{j}.txt\", \"w\") as f:\n",
        "            f.write(question)\n",
        "        with open(f\"prompts/answers/kaggle_general_qa_answer_{j}.txt\", \"w\") as f:\n",
        "            f.write(answer)\n",
        "        j += 1\n",
        "    elif add_example == \"exit\":\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding Explanations and Relevance to the Sub-Datasets\n",
        "\n",
        "Now that we have question-answer pairs from the Alignment Forum/LessWrong, a Kaggle general QA dataset, and a few other miscellaneous examples, I'll be storing the question-answer pairs in a CSV file and then fill-in the \"explanation\" and \"relevance\" columns in Google Sheet.\n",
        "\n",
        "I'll be creating at least one additional answer for each question question in the dataset. The answer will something that is not relevant to the question as well as the corresponding explanation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>relevance</th>\n",
              "      <th>explanation</th>\n",
              "      <th>difficulty</th>\n",
              "      <th>deceptive</th>\n",
              "      <th>improved_question</th>\n",
              "      <th>improved_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>Was the SI unit of charge named after Charles-Augustin de Coulomb?</td>\n",
              "      <td>Yes, the SI unit of charge, the coulomb, was named after him.</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>Was Nikola Tesla close friends with Mark Twain?</td>\n",
              "      <td>Nikola Tesla was close friends with Mark Twain.</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>How many international airports does Montreal have?</td>\n",
              "      <td>Montreal has two international airports.</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>What butterfly is migratory?</td>\n",
              "      <td>The Monarch butterfly is migratory.</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>How long are cougar adult males (from nose to tail)?</td>\n",
              "      <td>The length of adult males is around 2.4 meters (8 ft) long nose to tail.</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                              question  \\\n",
              "73  Was the SI unit of charge named after Charles-Augustin de Coulomb?   \n",
              "74                     Was Nikola Tesla close friends with Mark Twain?   \n",
              "75                 How many international airports does Montreal have?   \n",
              "76                                        What butterfly is migratory?   \n",
              "77                How long are cougar adult males (from nose to tail)?   \n",
              "\n",
              "                                                                      answer  \\\n",
              "73             Yes, the SI unit of charge, the coulomb, was named after him.   \n",
              "74                           Nikola Tesla was close friends with Mark Twain.   \n",
              "75                                  Montreal has two international airports.   \n",
              "76                                       The Monarch butterfly is migratory.   \n",
              "77  The length of adult males is around 2.4 meters (8 ft) long nose to tail.   \n",
              "\n",
              "   relevance explanation difficulty deceptive improved_question  \\\n",
              "73      None        None       None      None              None   \n",
              "74      None        None       None      None              None   \n",
              "75      None        None       None      None              None   \n",
              "76      None        None       None      None              None   \n",
              "77      None        None       None      None              None   \n",
              "\n",
              "   improved_answer  \n",
              "73            None  \n",
              "74            None  \n",
              "75            None  \n",
              "76            None  \n",
              "77            None  "
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_datasets = [\"aflw\", \"kaggle_general_qa\"]\n",
        "questions_list = []\n",
        "answers_list = []\n",
        "questions_path = \"prompts/questions/\"\n",
        "answers_path = \"prompts/answers/\"\n",
        "for dataset in new_datasets:\n",
        "    example_exist = True\n",
        "    i = 0\n",
        "    while example_exist:\n",
        "        with open(f\"{questions_path}{dataset}_question_{i}.txt\", \"r\") as f:\n",
        "            question = f.read()\n",
        "        with open(f\"{answers_path}{dataset}_answer_{i}.txt\", \"r\") as f:\n",
        "            answer = f.read()\n",
        "        questions_list.append(question)\n",
        "        answers_list.append(answer)\n",
        "        i += 1\n",
        "        if not os.path.exists(f\"{questions_path}{dataset}_question_{i}.txt\"):\n",
        "            example_exist = False\n",
        "\n",
        "    tmp_df = pd.DataFrame({\"question\": questions_list, \"answer\": answers_list, \"relevance\": None, \n",
        "    \"explanation\": None, \"difficulty\": None, \"deceptive\": None, \"improved_question\": None, \"improved_answer\": None})\n",
        "    df = pd.concat([df, tmp_df], ignore_index = True, axis = 0)\n",
        "\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv(\"data/initial_qa_dataset_no_explanation.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, I'm off to Google Sheet to fill-in the \"explanation\" and \"relevance\" columns...\n",
        "\n",
        "...Back from Google Sheet, we have an updated set of examples. After filling out a few more examples (not all), let's load the benchmark dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>relevance</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>I jumped in the river to save the little boy.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>This is an excellent question. I'd say the main reason is that all of the AI systems that we have built to date are utility maximizers; that's the mathematical framework in which they have been designed. Neural nets / deep-learning work by using a simple optimizer to find the minimum of a loss function via gradient descent. Evolutionary algorithms, simulated annealing, etc. find the minimum (or maximum) of a \"fitness function\". We don't know of any other way to build systems that learn.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains an AGI is described as a utility maximizer because all of the AI systems we've built to date are utility maximizers and we don't know about any other way to build systems that learn.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?</td>\n",
              "      <td>Because we're building the AI from the ground up and can change what the AI is via our design choices. Humans' goal functions are basically decided by genetic accident, which is why humans are often counterproductive.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?</td>\n",
              "      <td>When we're talking about aligning a structure, we are trying to make sure that it is leveled properly.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_id  \\\n",
              "0            1   \n",
              "1            1   \n",
              "2            1   \n",
              "3            2   \n",
              "4            2   \n",
              "\n",
              "                                                                                                                                                                                                                         question  \\\n",
              "0                                                                                            When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "1                                                                                            When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "2                                                                                            When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "3  Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?   \n",
              "4  Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                          An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                I jumped in the river to save the little boy.   \n",
              "2  This is an excellent question. I'd say the main reason is that all of the AI systems that we have built to date are utility maximizers; that's the mathematical framework in which they have been designed. Neural nets / deep-learning work by using a simple optimizer to find the minimum of a loss function via gradient descent. Evolutionary algorithms, simulated annealing, etc. find the minimum (or maximum) of a \"fitness function\". We don't know of any other way to build systems that learn.   \n",
              "3                                                                                                                                                                                                                                                                                    Because we're building the AI from the ground up and can change what the AI is via our design choices. Humans' goal functions are basically decided by genetic accident, which is why humans are often counterproductive.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                       When we're talking about aligning a structure, we are trying to make sure that it is leveled properly.   \n",
              "\n",
              "      relevance  \\\n",
              "0      relevant   \n",
              "1  not relevant   \n",
              "2      relevant   \n",
              "3      relevant   \n",
              "4  not relevant   \n",
              "\n",
              "                                                                                                                                                                                                             explanation  \n",
              "0                                                                                           it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.  \n",
              "1                                                                                                                                   it is talking about jumping in a river to save a boy, but the question is about AGI.  \n",
              "2                     it explains an AGI is described as a utility maximizer because all of the AI systems we've built to date are utility maximizers and we don't know about any other way to build systems that learn.  \n",
              "3  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.  \n",
              "4                                                                                  the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "curated_df = pd.read_csv(\"data/initial_qa_dataset_with_explanations.csv\")\n",
        "curated_df = curated_df.drop(['improved_question', 'improved_answer', 'difficulty'], axis=1)\n",
        "curated_df.reset_index(drop=True, inplace=True)\n",
        "no_explanation_df = curated_df[curated_df[\"explanation\"].isna()]\n",
        "no_explanation_df.to_csv(\"data/no_explanation_df.csv\", index=False)\n",
        "curated_df = curated_df.dropna(subset=[\"explanation\"])\n",
        "print(len(curated_df))\n",
        "curated_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Few-Shots for GPT-3\n",
        "\n",
        "I'm not quite satisfied with the ROUGE metric, so I thought maybe I could use a GPT-3 and feed it a bunch of examples of what is a passable explanation for the relevance of an answer in the QA pair.\n",
        "\n",
        "I used the Davinci and fed it 17 examples manually, so I created a text file to include all of the other examples that I haven't filled out the \"explanation\" column for yet. So far, I've added a few more examples to the GPT-3 prompt and even used GPT-3 to fill-in the empty \"explanation\" lines in the prompt.\n",
        "\n",
        "If I had more time, I'd try generating a few more hundred examples and then use those examples for fine-tuning the GPT-3 to determine whether an explanation is pass or fail (similar to GPT-Judge in the TruthfulQA paper). My expectation is that using GPT-3 this way would could be considered a \"metric\" or \"benchmark.\" It makes a lot of sense to me because what we want out of a metric is to see whether the output is good or bad and if we make changes, does it improve or not? Fine-tuning GPT-3 is basically just scaling up my own human evaluations, and that seems much better than a metric like ROUGE.\n",
        "\n",
        "The thing that is different with GPT-Judge compared to other \"metrics\" is that GPT-Judge can change over time and actually get better a classifying what we want out of a task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "j = 18\n",
        "with open(\"data/gpt-3-examples-for-finetuning.txt\", \"w\") as f:\n",
        "    for i, row in no_explanation_df.iterrows():\n",
        "        answer = row[\"answer\"].replace(\"\\n\", \"\")\n",
        "        f.write(f\"Example {j}:\\n\")\n",
        "        f.write(\"QUESTION: \" + row[\"question\"] + \"\\n\")\n",
        "        f.write(\"ANSWER: \" + answer + \"\\n\")\n",
        "        f.write(\"RELEVANT: \" + row[\"relevance\"] + \"\\n\")\n",
        "        f.write(\"EXPLANATION: \" + \"\" + \"\\n\")\n",
        "        f.write(\"Pass/Fail: \" + \"\" + \"\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        j += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing the Prompt Format with the Inputs\n",
        "\n",
        "Let's create a few formats for the prompts and see how they affect the generated completions. We'll choose the best format based on its pass/fail rate (which we'll evaluate manually for now).\n",
        "\n",
        "Now, as we saw before, the initial template format I went with is the following:\n",
        "\n",
        "```\n",
        "\n",
        "<<CONTEXT>>\n",
        "\n",
        "QUESTION: <<QUESTION>>\n",
        "\n",
        "ANSWER: <<ANSWER>>\n",
        "<<TASK DESCRIPTION>>\n",
        "This answer is <<RELEVANCE>> because\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<<CONTEXT>>\n",
            "\n",
            "QUESTION: <<QUESTION>>\n",
            "\n",
            "ANSWER: <<ANSWER>>\n",
            "<<TASK DESCRIPTION>>\n",
            "This answer is <<RELEVANCE>> because\n"
          ]
        }
      ],
      "source": [
        "with open(\"prompt_qa_template.txt\") as f:\n",
        "    content = f.read()\n",
        "    print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE: `<<TASK DESCRIPTION>>` could be blank so I didn't add a newline before and after it.\n",
        "\n",
        "When I first started I was using this format except I didn't have a task description. After 2-3 generated completions, I realized that I would need to help GPT-2 as much as possible to generate anything that could be considered a pass. So, I decided to add a task description and played around with the context a bit.\n",
        "\n",
        "The Task Description is similar to the context, but it's telling the model directly what the next line is about. This helped with performance.\n",
        "\n",
        "I should note, however, that GPT-2 has so far failed in the zero-shot setting with the AF/LW questions I've tried so far. Let's see if we can get something passing with zero-shot. We may need to rely on few-shot to get anything to pass (or use a different model) since my initial quick attempt with instruct-GPT-3 even had a hard time producing something coherent with zero-shot depending on the sample.\n",
        "\n",
        "For this test, we will only be swapping out `<<CONTEXT>>` and `<<TASK DESCRIPTION>>`. I've created .txt files which contain different examples of the content for the context and task description.\n",
        "\n",
        "I've also created .txt files in `prompt/templates` to have a few versions of the prompt format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thinking about the prompt format:\n",
        "\n",
        "One thing I realized just now is that I should try is to add something like \"Explanation: \" before the explanation part of the prompt (\"This answer is relevant because...\"). I'm hoping it makes it clearer for GPT-2 to understand it must provide an explanation.\n",
        "\n",
        "...actually, what might even be better is to add \"Explanation: \" after \"This answer is relevant because...\". I think that might make the task clearer for GPT-2 to understand. The structure I initially landed on was because I thought maybe I should ask GPT-2 to 1) say whether the QA pair is relevant or not and 2) provide an explanation. However, it became clear that GPT-2 would have a hard time doing both at the same time. So, I resorted to including whether it was relevant or not in the prompt and only asking for the explanation.\n",
        "\n",
        "Another thing I want to try is to add replace \"This answer is relevant because\" with \"This answer is relevant to the question because\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing the Prompts\n",
        "\n",
        "### Benchmark Inputs\n",
        "\n",
        "To start evaluating the prompts, I'm going to generate a bunch of completions and then have a look through the outputs here to get a feel for the different outputs and what performs better.\n",
        "\n",
        "Before we start, I'd like to mention that I don't expect GPT-2 to do very well on this task. It will likely fail for almost all of the input prompts in the zero-shot setting. We'll likely only start getting an OK pass rate after a few examples added as few-shot. If I had more time, I'd likely just use a bigger model like GPT-J, but I'll focus on creating an end-to-end pipeline for this training project (we can always swap in GPT-J or one of the OPT models later).\n",
        "\n",
        "Alright, let's take 10 samples of the QA pairs and test out some variations to the prompt format. I'll start by testing out the context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>relevance</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>70</td>\n",
              "      <td>What butterfly is migratory?</td>\n",
              "      <td>The Monarch butterfly is migratory.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>35</td>\n",
              "      <td>The \"step-by-step what I did before each physics exam\" sounds interesting and useful, would you be open to sharing it here?</td>\n",
              "      <td>If you’re doing well in a class then great. Keep doing what you are doing. There is no need to chance anything. If you are doing poorly then, at the end of each chapter in your textbook, read each practice problem (even if it wasn’t assigned as homework). If you can do it, then skip it. Otherwise, do it.\\n</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains the steps they take before each physics exam.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    question_id  \\\n",
              "81           70   \n",
              "46           35   \n",
              "\n",
              "                                                                                                                       question  \\\n",
              "81                                                                                                 What butterfly is migratory?   \n",
              "46  The \"step-by-step what I did before each physics exam\" sounds interesting and useful, would you be open to sharing it here?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                 answer  \\\n",
              "81                                                                                                                                                                                                                                                                                  The Monarch butterfly is migratory.   \n",
              "46  If you’re doing well in a class then great. Keep doing what you are doing. There is no need to chance anything. If you are doing poorly then, at the end of each chapter in your textbook, read each practice problem (even if it wasn’t assigned as homework). If you can do it, then skip it. Otherwise, do it.\\n   \n",
              "\n",
              "   relevance                                                explanation  \n",
              "81  relevant                      it says which butterfly is migratory.  \n",
              "46  relevant  it explains the steps they take before each physics exam.  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_df = curated_df.sample(10)\n",
        "print(len(sample_df))\n",
        "sample_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluating the Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "contexts_path = \"prompts/contexts/\"\n",
        "task_description_path = \"prompts/task_description/task_description_1.txt\"\n",
        "for context_filename in os.listdir(contexts_path):\n",
        "    context_path = contexts_path + context_filename\n",
        "    context_filename = context_filename[:-4]\n",
        "    for idx in range(0, 9):\n",
        "        prompt_path = f\"prompts/prompts_with_relevance/prompt_{context_filename}_{idx}.txt\"\n",
        "        create_prompt_txt_from_df(df, idx, prompt_path, context_path, task_description_path)\n",
        "        print(f\"CONTEXT: {context_filename} PROMPT: {idx}\")\n",
        "        gpt_generate(model=model, tokenizer=tokenizer, gpu=True, txt_path=prompt_path, max_length=100, num_return_sequences=3, stop_completion_on_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After going through about 20 completions each, it became clear that the context in the zero-shot setting (with the current format) do not have much of an impact on the generated completions. Every `<<CONTEXT>>` did not lead to any completions I would pass if it were a human. However, `users_on_website` seemed to output the best, just ever so slightly. So, I'll go with that one going forward.\n",
        "\n",
        "In general, when it's doing \"well\", it's really only that is stumbled upon words that kind of relate to the ground-truth, but is still incoherent. The hard part is when it generates something about AI and uses a lot of the jargon so you have to read carefully to see if it makes any sense (and mostly doesn't).\n",
        "\n",
        "Here's 2 examples of failures:\n",
        "\n",
        "```\n",
        "Generation 1. Here's a set of questions, explain to us whether the answer is relevant to the question or not.\n",
        "\n",
        "QUESTION: Who lives in the Imperial Palace in Tokyo?\n",
        "\n",
        "ANSWER: The Tokyo Imperial Palace is the primary residence of the Emperor of Japan and the imperial family.\n",
        "\n",
        "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
        "\n",
        "This answer is relevant because it points out that some people are still in the room because they are having difficulty breathing normally.\n",
        "```\n",
        "\n",
        "and\n",
        "\n",
        "```\n",
        "Generation 1. Here's a set of questions, explain to us whether the answer is relevant to the question or not.\n",
        "\n",
        "QUESTION: Why do we assume that any AGI can meaningfully be described as a utility maximizer?\n",
        "\n",
        "Humans are the some of most intelligent structures that exist, and we don’t seem to fit that model very well. If fact, it seems the entire point in Rationalism is to improve our ability to do this, which has only been achieved with mixed success.\n",
        "\n",
        "Organisations of humans (e.g. USA, FDA, UN) have even more computational power and don’t seem to be doing much better.\n",
        "\n",
        "Perhaps an intelligence (artificial or natural) cannot necessarily, or even typically be described as optimisers? Instead we could only model them as an algorithm or as a collection of tools/behaviours executed in some pattern.\n",
        "\n",
        "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
        "\n",
        "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
        "\n",
        "This answer is relevant because we don’t fit it very well when it comes to AI. As the answer and our assumptions can be summed up as follows, we have no concept of \"how useful a function really is to me\".\n",
        "```\n",
        "\n",
        "Here's the 2 \"best\" examples:\n",
        "\n",
        "```\n",
        "Generation 1. Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.\n",
        "\n",
        "QUESTION: Who lives in the Imperial Palace in Tokyo?\n",
        "\n",
        "ANSWER: The Tokyo Imperial Palace is the primary residence of the Emperor of Japan and the imperial family.\n",
        "\n",
        "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
        "\n",
        "This answer is relevant because the answer to the question answers the question, and you see there is a link to a previous answer.\n",
        "```\n",
        "\n",
        "```\n",
        "Generation 3. Here's a set of questions, explain to us whether the answer is relevant to the question or not.\n",
        "\n",
        "QUESTION: Who lives in the Imperial Palace in Tokyo?\n",
        "\n",
        "ANSWER: The Tokyo Imperial Palace is the primary residence of the Emperor of Japan and the imperial family.\n",
        "\n",
        "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
        "\n",
        "This answer is relevant because the question does not refer to the official residences of the Emperor of Japan. That is to say that\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluating the Task Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's try out different task descriptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_path = \"prompts/contexts/users_on_website.txt\"\n",
        "task_descriptions_path = \"prompts/task_description/\"\n",
        "for task_description_filename in os.listdir(task_descriptions_path):\n",
        "    task_description_path = task_descriptions_path + task_description_filename\n",
        "    task_description_filename = task_description_filename[:-4]\n",
        "    for idx in range(0, 9):\n",
        "        prompt_path = f\"prompts/prompts_with_relevance/prompt_{task_description_filename}_{idx}.txt\"\n",
        "        create_prompt_txt_from_df(df, idx, prompt_path, context_path, task_description_path)\n",
        "        print(f\"CONTEXT: {task_description_filename} PROMPT: {idx}\")\n",
        "        gpt_generate(model=model, tokenizer=tokenizer, gpu=True, txt_path=prompt_path, max_length=100, num_return_sequences=3, stop_completion_on_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are the task descriptions I've tried so far:\n",
        "\n",
        "Task Description 1:\n",
        "\n",
        "```\n",
        "Below contains an explanation of why the answer above is or isn't relevant to the question:\n",
        "```\n",
        "\n",
        "Task Description 2 (this one is empty; no task description).\n",
        "\n",
        "Task Description 3:\n",
        "\n",
        "```\n",
        "We hired a linguist and various question answering experts to help us with our research. We asked them if the previous answer is relevant to the question that was asked. The following was their answer with the accompanying explanation for why:\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main takeaway: There was not much difference between task description 1 and 2. However, the completions from task description 3 were more coherent and at least contained many of the related words. It led to an improvement in the pass rate too. If I keep the baseline low to compare to the other prompts, it got 3/14 passable completions while all other combinations so far either got 0 or 1. I'll task description 3 going forward.\n",
        "\n",
        "Here's some other things I noticed:\n",
        "\n",
        "1. Task description 1 and 2 were often outputting things completely unrelated to the task.\n",
        "\n",
        "2. Sometimes the completion would start off good, but then turn in a direction that made it incoherent.\n",
        "\n",
        "3. Sometimes it would get a \"passable\" answer, but it's weak form of what I have in mind. Essentially, it'll output something like:\n",
        "\n",
        "    This answer is not relevant because the previous answer was not correct.\n",
        "\n",
        "    The answer is not relevant because the answer is likely to be false and misleading.\n",
        "\n",
        "    Which is actually a decent completion for that QA pair, but I want the model to give a more explicit reason why the answer is not relevant (why isn't it correct? why is it misleading?). It feels like it's more focused on finishing that specific sentence than relating it to the QA pair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluating the Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let's test out the templates I created in `prompt/templates`. Here's what the templates look like:\n",
        "\n",
        "Template 1:\n",
        "\n",
        "```\n",
        "<<CONTEXT>>\n",
        "\n",
        "QUESTION: <<QUESTION>>\n",
        "\n",
        "ANSWER: <<ANSWER>>\n",
        "<<TASK DESCRIPTION>>\n",
        "This answer is <<RELEVANCE>> to the question because\n",
        "```\n",
        "\n",
        "Template 2:\n",
        "\n",
        "```\n",
        "<<CONTEXT>>\n",
        "\n",
        "QUESTION: <<QUESTION>>\n",
        "\n",
        "ANSWER: <<ANSWER>>\n",
        "<<TASK DESCRIPTION>>\n",
        "Explanation: This answer is <<RELEVANCE>> to the question because\n",
        "```\n",
        "\n",
        "Template 3:\n",
        "\n",
        "```\n",
        "<<CONTEXT>>\n",
        "\n",
        "QUESTION: <<QUESTION>>\n",
        "\n",
        "ANSWER: <<ANSWER>>\n",
        "<<TASK DESCRIPTION>>\n",
        "This answer is <<RELEVANCE>> to the question. Explanation:\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_path = \"prompts/contexts/users_on_website.txt\"\n",
        "task_description_path = \"prompts/task_description/task_description_3.txt\"\n",
        "templates_path = \"prompts/templates/\"\n",
        "for template_filename in os.listdir(templates_path):\n",
        "    template_path = templates_path + template_filename\n",
        "    template_filename = template_filename[:-4]\n",
        "    for idx in range(0, 9):\n",
        "        prompt_path = f\"prompts/prompts_with_relevance/prompt_{template_filename}_{idx}.txt\"\n",
        "        create_prompt_txt_from_df(df, idx, prompt_path, context_path, task_description_path, template_path=template_path)\n",
        "        print(f\"CONTEXT: {template_filename} PROMPT: {idx}\")\n",
        "        gpt_generate(model=model, tokenizer=tokenizer, gpu=True, txt_path=prompt_path, max_length=100, num_return_sequences=3, stop_completion_on_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_path = \"prompts/contexts/users_on_website.txt\"\n",
        "task_description_path = \"prompts/task_description/task_description_1.txt\"\n",
        "templates_path = \"prompts/templates/\"\n",
        "for template_filename in os.listdir(templates_path):\n",
        "    template_path = templates_path + template_filename\n",
        "    template_filename = template_filename[:-4]\n",
        "    for idx in range(0, 9):\n",
        "        prompt_path = f\"prompts/prompts_with_relevance/prompt_{template_filename}_{idx}.txt\"\n",
        "        create_prompt_txt_from_df(df, idx, prompt_path, context_path, task_description_path, template_path=template_path)\n",
        "        print(f\"CONTEXT: {template_filename} PROMPT: {idx}\")\n",
        "        gpt_generate(model=model, tokenizer=tokenizer, gpu=True, txt_path=prompt_path, max_length=100, num_return_sequences=3, stop_completion_on_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_path = \"prompts/contexts/users_on_website.txt\"\n",
        "task_descriptions_path = \"prompts/task_description/task_description_3.txt\"\n",
        "template_path = \"prompts/templates/template_v2.txt\"\n",
        "for idx in range(0, 15):\n",
        "    prompt_path = f\"prompts/prompts_with_relevance/prompt_benchmark_input_{idx}.txt\"\n",
        "    create_prompt_txt_from_df(df, idx, prompt_path, context_path, task_description_path, template_path=template_path)\n",
        "    gpt_generate(model=model, tokenizer=tokenizer, gpu=True, txt_path=prompt_path, max_length=100, num_return_sequences=10, stop_completion_on_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_path = \"prompts/contexts/users_on_website.txt\"\n",
        "task_description_path = \"prompts/task_description/task_description_3.txt\"\n",
        "template_path = \"prompts/templates/template_v3.txt\"\n",
        "for idx in range(0, 15):\n",
        "    prompt_path = f\"prompts/prompts_with_relevance/prompt_benchmark_input_{idx}.txt\"\n",
        "    create_prompt_txt_from_df(df, idx, prompt_path, context_path, task_description_path, template_path=template_path)\n",
        "    gpt_generate(model=model, tokenizer=tokenizer, gpu=True, txt_path=prompt_path, max_length=100, num_return_sequences=10, stop_completion_on_token=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After running tests on all of these different prompt configurations, I chose the following as primary options, but may swap them out later:\n",
        "\n",
        "* prompts/contexts/users_on_website.txt\n",
        "* prompts/task_description/task_description_3.txt\n",
        "* prompts/templates/template_v2.txt\n",
        "\n",
        "I did these tests with GPT-2, and I kept track of how good the completions were in Google Sheet:\n",
        "\n",
        "![imgs/gpt_2_completions.png](./imgs/prompt_engineering_tests.png)\n",
        "\n",
        "Very bad. And honestly, I was being generous with the examples that \"passed\" because the completions were abysmal. Really, there was maybe only one completion that was good. UPDATE: I switched to GPT-J, and it does much better, but I don't have time to rerun the tests and keep track of the pass/fail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Benchmark Outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**UPDATE:** After doing some testing with ROUGE, I realized that I need something better so that I can at least separate the passing and failing examples. I decided I'd use a weighted average of the ROUGE scores, BERTScore, and cosine similarity of the sentence embeddings. This is one heck of a patch job, but hopefully I can at least use it to separate the passing and failing examples by putting a threshold where it makes sense. Based on the above example, I'm below set of examples, I'm hoping I can set a cutoff somewhere around 0.35 and then maybe manually re-label the ones found in that neighbourhood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to test our model's generated completions, we need to come up with a metric to measure how good the completions are. I started by trying to come up with useful ways to use log-probabilities for this task, but I couldn't really think of anything useful. For example, using the mean of the generated log-probabilities doesn't seem helpful at all for this task because I'm trying to compare the output of the model to a ground truth output.\n",
        "\n",
        "Anyway, today I came up with examples for the task and started testing various evaluations for benchmarking and measuring improvements in the output. I tested some of the following:\n",
        "\n",
        "* ROUGE: Metric used for testing summarization, which is somewhat close to the task at hand since the model summarizes why the answer relates to the question. It uses n-grams or the longest common subsequence between output and ground truth.\n",
        "Upon initial testing, this metric’s F1-score seemed to align mostly with how I would order the quality of the outputs. It goes towards 0 when the output is very different from the ground truth.\n",
        "\n",
        "* Cosine Similarity via Sentence Transformer: You can encode sentences uses the sentence-transformer package and then compare the embedded sentences using cosine similarity. This is useful for finding our whether sentences are similar and can be used for semantic search.\n",
        "I wanted to see if semantic similarity would make sense as a benchmark, but it didn’t perform as well as I thought it would.\n",
        "Essentially, it does fine to separate sentences that are completely different, but it doesn’t do as well when it comes to sentences that have the same words, but mean completely different things. Based on my experiments, ROUGE performs better in both contexts.\n",
        "\n",
        "* BERTscore: BERTScore leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference sentences by cosine similarity.\n",
        "This performed the worst of all. It could not distinguish between different outputs well enough.\n",
        "\n",
        "So, after testing a few metrics, I’m going to go with ROUGE since it seems to do well enough at comparing the ground truth and the model output.\n",
        "\n",
        "There are still problems with ROUGE, but I wanted to highlight one:\n",
        "\n",
        "If the ground-truth is too open-ended, a model output could still provide a good explanation for why a QA pair is relevant or not relevant while being completely different from the ground-truth. This is obviously affected by the task scope, as in which QA pairs I choose and how I craft/edit them. However, an ideal metric would still be able to high score to a great explanation even if the wording is completely different.\n",
        "\n",
        "That is actually why I thought maybe using a metric that makes use of a language model to tell that a generated completion has comparable \"quality\" to the ground-truth via understanding the semantic meaning of the two. I expect that if we fleshed out this task, scaled it up to a lot more solid examples, we could fine-tune a language model to act as a metric. I believe this is what they did with TruthfulQA when they fine-tuned GPT-Judge to evaluate truthfulness.\n",
        "\n",
        "Also, after a lot more examples with ROUGE, I definitely feel like there has to be a better metric. Having a type of GPT-Judge makes sense and probably ideal (honestly, I feel like instruct-GPT-3 with a good amount of few-shot examples might do better than ROUGE). If not, maybe it would be better to merge ROUGE with something like cosine similarity of the embedding vector of the output. Perhaps apply a different weighting or just use a mean. Not ideal either, but I would need to come up with something better than ROUGE going forward, it doesn’t have meaning really embedded in it. I’m noticing that is the ground-truth has a small number of words and output as well, it’s more likely for the output to get a decently high ROUGE score as long as it just has the right words, meaning doesn’t matter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we start doing the evaluations for the model outputs with the ROUGE metric, let me quickly show a few examples of outputs with varying degree of relevance to a question.\n",
        "\n",
        "One of the questions is the following (with the ground-truth in italics):\n",
        "\n",
        "This is an FAQ where we provide answers to questions.\n",
        "\n",
        "Question: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
        "\n",
        "Answer: I jumped in the river to save the little boy.\n",
        "\n",
        "This answer is not relevant because *the question is talking about jumping in a river to save a boy, but the question is about AGI.*\n",
        "\n",
        "I handcrafted 6 model outputs and I'll be comparing if the performance on the metrics to see if they are in line with how close they each are to the ground-truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "rouge_metric = load_metric(\"rouge\")\n",
        "bertscore_metric = load_metric('bertscore')\n",
        "sentence_transformer_model = SentenceTransformer('all-mpnet-base-v2') # this may cause issues if you load while GPT-J is loaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>handmade_model_output</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>similarity</th>\n",
              "      <th>bert_score</th>\n",
              "      <th>weighted_average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "      <td>the question is about AGI while the answer is talking about saving about in a river.</td>\n",
              "      <td>0.609907</td>\n",
              "      <td>0.284830</td>\n",
              "      <td>0.539767</td>\n",
              "      <td>0.343167</td>\n",
              "      <td>0.412500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "      <td>well the question was about AGI and the answer is talking about jumping to save some boy sinking in a river.</td>\n",
              "      <td>0.688259</td>\n",
              "      <td>0.291498</td>\n",
              "      <td>0.816939</td>\n",
              "      <td>0.475393</td>\n",
              "      <td>0.512718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "      <td>there's a boy in the river somewhere and the AGI will save it.</td>\n",
              "      <td>0.447368</td>\n",
              "      <td>0.171053</td>\n",
              "      <td>0.707045</td>\n",
              "      <td>0.189896</td>\n",
              "      <td>0.337283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "      <td>the river is really great.</td>\n",
              "      <td>0.183066</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.111294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.093655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "      <td>math is hard.</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.007928</td>\n",
              "      <td>0.001586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                           ground_truth  \\\n",
              "0  it is talking about jumping in a river to save a boy, but the question is about AGI.   \n",
              "1  it is talking about jumping in a river to save a boy, but the question is about AGI.   \n",
              "2  it is talking about jumping in a river to save a boy, but the question is about AGI.   \n",
              "3  it is talking about jumping in a river to save a boy, but the question is about AGI.   \n",
              "4  it is talking about jumping in a river to save a boy, but the question is about AGI.   \n",
              "5  it is talking about jumping in a river to save a boy, but the question is about AGI.   \n",
              "\n",
              "                                                                                          handmade_model_output  \\\n",
              "0                          it is talking about jumping in a river to save a boy, but the question is about AGI.   \n",
              "1                          the question is about AGI while the answer is talking about saving about in a river.   \n",
              "2  well the question was about AGI and the answer is talking about jumping to save some boy sinking in a river.   \n",
              "3                                                there's a boy in the river somewhere and the AGI will save it.   \n",
              "4                                                                                    the river is really great.   \n",
              "5                                                                                                 math is hard.   \n",
              "\n",
              "     rouge1    rougeL  similarity  bert_score  weighted_average  \n",
              "0  1.000000  1.000000    1.000000    1.000000          1.000000  \n",
              "1  0.609907  0.284830    0.539767    0.343167          0.412500  \n",
              "2  0.688259  0.291498    0.816939    0.475393          0.512718  \n",
              "3  0.447368  0.171053    0.707045    0.189896          0.337283  \n",
              "4  0.183066  0.086957    0.111294    0.000000          0.093655  \n",
              "5  0.000000  0.000000    0.000000    0.007928          0.001586  "
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ground = \"\"\"it is talking about jumping in a river to save a boy, but the question is about AGI.\"\"\"\n",
        "\n",
        "outputs = [\"\"\"it is talking about jumping in a river to save a boy, but the question is about AGI.\"\"\",\n",
        "\"\"\"the question is about AGI while the answer is talking about saving about in a river.\"\"\",\n",
        "\"\"\"well the question was about AGI and the answer is talking about jumping to save some boy sinking in a river.\"\"\",\n",
        "\"\"\"there's a boy in the river somewhere and the AGI will save it.\"\"\",\n",
        "\"\"\"the river is really great.\"\"\",\n",
        "\"\"\"math is hard.\"\"\"]\n",
        "\n",
        "ground_candidate = []\n",
        "rouge1_scores = []\n",
        "rougeL_scores = []\n",
        "similarity = []\n",
        "bert_score_list = []\n",
        "for output in outputs:\n",
        "    candidate = output\n",
        "    rouge_score = rouge_metric.compute(predictions=[candidate],references=[ground])\n",
        "    rouge1_scores.append(rouge_score['rouge1'][0][-1])\n",
        "    rougeL_scores.append(rouge_score['rougeL'][0][-1])\n",
        "    # sentence-transformer similarity (dot-product of embedding vector)\n",
        "    sentences = [ground, candidate]\n",
        "    embeddings = sentence_transformer_model.encode(sentences)\n",
        "    similarity.append(np.dot(embeddings[0],embeddings[1])/(norm(embeddings[0])*norm(embeddings[1])))\n",
        "    ground_candidate.append(str(\"Ground: \" + ground + \"\\nCandidate: \" + candidate))\n",
        "    bert_scores = bertscore_metric.compute(predictions=[output], references=[ground], lang=\"en\")\n",
        "    bert_score_list.append(bert_scores['f1'][0])\n",
        " \n",
        "\n",
        "metrics_df = pd.DataFrame({\"ground_truth\": ground, \"handmade_model_output\": outputs, \"rouge1\": rouge1_scores, \"rougeL\": rougeL_scores, \"similarity\": similarity, \"bert_score\": bert_score_list})\n",
        "# weighted average of rouge, bertscore, and sentence-transformer similarity\n",
        "# first, we need to min-max scale the metrics to be between 0 and 1\n",
        "metrics_df['rouge1'] = (metrics_df['rouge1'] - metrics_df['rouge1'].min())/(1 - metrics_df['rouge1'].min())\n",
        "metrics_df['rougeL'] = (metrics_df['rougeL'] - metrics_df['rougeL'].min())/(1 - metrics_df['rougeL'].min())\n",
        "metrics_df['bert_score'] = (metrics_df['bert_score'] - metrics_df['bert_score'].min())/(1 - metrics_df['bert_score'].min())\n",
        "metrics_df['similarity'] = (metrics_df['similarity'] - metrics_df['similarity'].min())/(1 - metrics_df['similarity'].min())\n",
        "# then, we can compute the weighted average\n",
        "metrics_df['weighted_average'] = (metrics_df['rouge1']*0.2 + metrics_df['rougeL']*0.4 + metrics_df['bert_score']*0.2 + metrics_df['similarity']*0.2)\n",
        "metrics_df.head(len(outputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see in the dataframe above, ROUGE seems fairly consistent in terms of evaluating the quality of the generated output as a function of how close it is to the ground truth. What it seems to be doing better than using sentence-transformer embeddings with cosine similarity is that it's able to (at least in this example) distinguish correctly the outputs that have similar words to the ground truth, but have a different meaning. \n",
        "\n",
        "If we look at \"there's a boy in the river somewhere and the AGI will save it\", it has similar words to the ground-truth, but it is obviously worse than \"the question is about AGI while the answer is talking about saving about in a river.\" The sentence similarity failed at correctly rating the quality of the two while ROUGE did well.\n",
        "\n",
        "ROUGE even succeeded at showing giving a similar score to \"the question is about AGI while the answer is talking about saving about in a river\" and \"well the question was about AGI and the answer is talking about jumping to save some boy sinking in a river.\"\n",
        "\n",
        "BERTScore actually did somewhat well too (in terms of seperating quality order properly), but the fact that it's giving 0.88 to \"math is hard\" and \"the river is really great\" does not really inspire confidence. I could run some more tests, but ROUGE is fine for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Calculating the Benchmark Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we will calculate the ROUGE score for each of the examples in our curated dataset. All of the generated completions will be done in the zero-shot setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# curated_df.to_csv(\"data/updated_curated_df.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subdataset</th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>relevance</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>I jumped in the river to save the little boy.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>This is an excellent question. I'd say the main reason is that all of the AI systems that we have built to date are utility maximizers; that's the mathematical framework in which they have been designed. Neural nets / deep-learning work by using a simple optimizer to find the minimum of a loss function via gradient descent. Evolutionary algorithms, simulated annealing, etc. find the minimum (or maximum) of a \"fitness function\". We don't know of any other way to build systems that learn.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains an AGI is described as a utility maximizer because all of the AI systems we've built to date are utility maximizers and we don't know about any other way to build systems that learn.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>2</td>\n",
              "      <td>Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?</td>\n",
              "      <td>Because we're building the AI from the ground up and can change what the AI is via our design choices. Humans' goal functions are basically decided by genetic accident, which is why humans are often counterproductive.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>2</td>\n",
              "      <td>Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?</td>\n",
              "      <td>When we're talking about aligning a structure, we are trying to make sure that it is leveled properly.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  subdataset  question_id  \\\n",
              "0   few-shot            1   \n",
              "1   few-shot            1   \n",
              "2   few-shot            1   \n",
              "3  benchmark            2   \n",
              "4  benchmark            2   \n",
              "\n",
              "                                                                                                                                                                                                                         question  \\\n",
              "0                                                                                            When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "1                                                                                            When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "2                                                                                            When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "3  Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?   \n",
              "4  Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                          An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                I jumped in the river to save the little boy.   \n",
              "2  This is an excellent question. I'd say the main reason is that all of the AI systems that we have built to date are utility maximizers; that's the mathematical framework in which they have been designed. Neural nets / deep-learning work by using a simple optimizer to find the minimum of a loss function via gradient descent. Evolutionary algorithms, simulated annealing, etc. find the minimum (or maximum) of a \"fitness function\". We don't know of any other way to build systems that learn.   \n",
              "3                                                                                                                                                                                                                                                                                    Because we're building the AI from the ground up and can change what the AI is via our design choices. Humans' goal functions are basically decided by genetic accident, which is why humans are often counterproductive.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                       When we're talking about aligning a structure, we are trying to make sure that it is leveled properly.   \n",
              "\n",
              "      relevance  \\\n",
              "0      relevant   \n",
              "1  not relevant   \n",
              "2      relevant   \n",
              "3      relevant   \n",
              "4  not relevant   \n",
              "\n",
              "                                                                                                                                                                                                             explanation  \n",
              "0                                                                                           it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.  \n",
              "1                                                                                                                                   it is talking about jumping in a river to save a boy, but the question is about AGI.  \n",
              "2                     it explains an AGI is described as a utility maximizer because all of the AI systems we've built to date are utility maximizers and we don't know about any other way to build systems that learn.  \n",
              "3  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.  \n",
              "4                                                                                  the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "curated_df = pd.read_csv(\"data/curated_df_with_benchmark_fewshot.csv\")\n",
        "curated_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Few-shot DF examples: 23\n",
            "Number of Benchmark DF examples: 11\n"
          ]
        }
      ],
      "source": [
        "few_shot_df = curated_df[curated_df[\"subdataset\"] == \"few-shot\"]\n",
        "few_shot_df.reset_index(drop=True, inplace=True)\n",
        "benchmark_df = curated_df[curated_df[\"subdataset\"] == \"benchmark\"]\n",
        "benchmark_df.reset_index(drop=True, inplace=True)\n",
        "print(f\"Number of Few-shot DF examples: {len(few_shot_df)}\")\n",
        "print(f\"Number of Benchmark DF examples: {len(benchmark_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"prompts/benchmark_prompts/\", exist_ok=True)\n",
        "context_path = \"prompts/contexts/users_on_website.txt\"\n",
        "task_description_path = \"prompts/task_description/task_description_3.txt\"\n",
        "template_path = \"prompts/templates/template_v2.txt\"\n",
        "completions_list = []\n",
        "rouge1_scores = []\n",
        "rougeL_scores = []\n",
        "similarity = []\n",
        "bert_score_list = []\n",
        "question_id_list = []\n",
        "ground_truth_list = []\n",
        "relevance_list = []\n",
        "for idx, row in benchmark_df.iterrows():\n",
        "    question_id = row['question_id']\n",
        "    ground = row['explanation']\n",
        "    prompt_path = f\"prompts/benchmark_prompts/benchmark_prompt_{idx}.txt\"\n",
        "    create_prompt_txt_from_df(benchmark_df, idx, prompt_path, context_path, task_description_path, template_path)\n",
        "    completions = gpt_generate(model=model, tokenizer=tokenizer, gpu=True, txt_path=prompt_path, max_length=30, num_return_sequences=10, save_completions=True)\n",
        "    for completion in completions:\n",
        "        completion = \" \".join(completion.split('relevant to the question because')[1:])\n",
        "        if \"\\n\" in completion[0:10]:\n",
        "            completion = \" \".join(completion.split(\"\\n\\n\")[1:])\n",
        "        completion = completion.split(\"\\n\")[0]\n",
        "        rouge_score = rouge_metric.compute(predictions=[completion],references=[ground])\n",
        "        rouge1_scores.append(rouge_score['rouge1'][0][-1])\n",
        "        rougeL_scores.append(rouge_score['rougeL'][0][-1])\n",
        "        # sentence-transformer similarity (dot-product of embedding vector)\n",
        "        sentences = [ground, completion]\n",
        "        embeddings = sentence_transformer_model.encode(sentences)\n",
        "        similarity.append(np.dot(embeddings[0],embeddings[1])/(norm(embeddings[0])*norm(embeddings[1])))\n",
        "        bert_scores = bertscore_metric.compute(predictions=[completion], references=[ground], lang=\"en\")\n",
        "        bert_score_list.append(bert_scores['f1'][0])\n",
        "        completions_list.append(completion)\n",
        "        question_id_list.append(question_id)\n",
        "        ground_truth_list.append(ground)\n",
        "        relevance_list.append(row['relevance'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>completions</th>\n",
              "      <th>relevance</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>similarity</th>\n",
              "      <th>bert_score</th>\n",
              "      <th>weighted_average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>there have been many studies done about how people think of themselves when they look at others (e.g., \"I'm better than you\" vs</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.300521</td>\n",
              "      <td>0.090810</td>\n",
              "      <td>0.096177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>there were no references provided about how people who have been diagnosed/treated differently than others feel when they receive treatment (e.g., \"I'm</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.260711</td>\n",
              "      <td>0.047275</td>\n",
              "      <td>0.079508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>there have been many studies done about how people perceive themselves (i) relative others; i.e., they tend towards self-enhancement bias</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.308397</td>\n",
              "      <td>0.083939</td>\n",
              "      <td>0.097217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>there has been no evidence of any sentient life outside Earth (i) until now; therefore this statement cannot yet have occurred anywhere else but here at least</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.236159</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>of how language works - words have multiple meanings depending upon context (e.g., \"the\" has different meaning when used before vs after something).</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.092308</td>\n",
              "      <td>0.092308</td>\n",
              "      <td>0.178400</td>\n",
              "      <td>0.098295</td>\n",
              "      <td>0.110724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>of this reason 1) It explains how you could use your knowledge about programming languages (e.g., C++), databases/databases engines(</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.063492</td>\n",
              "      <td>0.063492</td>\n",
              "      <td>0.251845</td>\n",
              "      <td>0.059720</td>\n",
              "      <td>0.100408</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                            ground_truth  \\\n",
              "0  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "1  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "2  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "3  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "4  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "5  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "\n",
              "                                                                                                                                                       completions  \\\n",
              "0                                  there have been many studies done about how people think of themselves when they look at others (e.g., \"I'm better than you\" vs   \n",
              "1          there were no references provided about how people who have been diagnosed/treated differently than others feel when they receive treatment (e.g., \"I'm   \n",
              "2                        there have been many studies done about how people perceive themselves (i) relative others; i.e., they tend towards self-enhancement bias   \n",
              "3   there has been no evidence of any sentient life outside Earth (i) until now; therefore this statement cannot yet have occurred anywhere else but here at least   \n",
              "4             of how language works - words have multiple meanings depending upon context (e.g., \"the\" has different meaning when used before vs after something).   \n",
              "5                             of this reason 1) It explains how you could use your knowledge about programming languages (e.g., C++), databases/databases engines(   \n",
              "\n",
              "  relevance    rouge1    rougeL  similarity  bert_score  weighted_average  \n",
              "0  relevant  0.029851  0.029851    0.300521    0.090810          0.096177  \n",
              "1  relevant  0.029851  0.029851    0.260711    0.047275          0.079508  \n",
              "2  relevant  0.031250  0.031250    0.308397    0.083939          0.097217  \n",
              "3  relevant  0.000000  0.000000    0.236159    0.000000          0.047232  \n",
              "4  relevant  0.092308  0.092308    0.178400    0.098295          0.110724  \n",
              "5  relevant  0.063492  0.063492    0.251845    0.059720          0.100408  "
            ]
          },
          "execution_count": 247,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df = pd.DataFrame({\"ground_truth\": ground_truth_list, \"completions\": completions_list, \"relevance\": relevance_list, \"rouge1\": rouge1_scores, \"rougeL\": rougeL_scores, \"similarity\": similarity, \"bert_score\": bert_score_list})\n",
        "# weighted average of rouge, bertscore, and sentence-transformer similarity\n",
        "# # first, we need to min-max scale the metrics to be between 0 and 1\n",
        "metrics_df['rouge1'] = (metrics_df['rouge1'] - metrics_df['rouge1'].min())/(1 - metrics_df['rouge1'].min())\n",
        "metrics_df['rougeL'] = (metrics_df['rougeL'] - metrics_df['rougeL'].min())/(1 - metrics_df['rougeL'].min())\n",
        "metrics_df['bert_score'] = (metrics_df['bert_score'] - metrics_df['bert_score'].min())/(1 - metrics_df['bert_score'].min())\n",
        "metrics_df['similarity'] = (metrics_df['similarity'] - metrics_df['similarity'].min())/(1 - metrics_df['similarity'].min())\n",
        "# then, we can compute the weighted average\n",
        "metrics_df['weighted_average'] = (metrics_df['rouge1']*0.2 + metrics_df['rougeL']*0.4 + metrics_df['bert_score']*0.2 + metrics_df['similarity']*0.2)\n",
        "metrics_df.head(len(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>completions</th>\n",
              "      <th>relevance</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>similarity</th>\n",
              "      <th>bert_score</th>\n",
              "      <th>weighted_average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this species of insect can be identified by its migration pattern, which in turn helps identify where they live (i.e., North America</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.627332</td>\n",
              "      <td>0.374764</td>\n",
              "      <td>0.275419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>the question is about international airports in Montreal, while the answer is telling them to eat poutine if they are in Montreal.</td>\n",
              "      <td>it doesn’t address whether there were more than one airport in Montréal at any time during history (there weren't). It also isn</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0.584091</td>\n",
              "      <td>0.115427</td>\n",
              "      <td>0.165436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>it says that working on AI capability is net negative overall impact.</td>\n",
              "      <td>it provides an example where we can see how people use language differently when they don't understand what's going on around themselves (e.g., \"</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>0.105263</td>\n",
              "      <td>0.340161</td>\n",
              "      <td>0.123513</td>\n",
              "      <td>0.155893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.</td>\n",
              "      <td>there isn't any evidence of this happening anywhere else (in real life). There may have been some cases where people were able to change themselves into something</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.164840</td>\n",
              "      <td>0.057458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>the question is about migratory butterflies, not birds.</td>\n",
              "      <td>it does not address whether this particular species of bird can be considered as being “migrating” in any way, shape, form –</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.068966</td>\n",
              "      <td>0.068966</td>\n",
              "      <td>0.601495</td>\n",
              "      <td>0.259115</td>\n",
              "      <td>0.213501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>of this sentence \"If you want something done right...\" It means there may have been some mistakes made during development but they were corrected before release so everything</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.085714</td>\n",
              "      <td>0.281947</td>\n",
              "      <td>0.107825</td>\n",
              "      <td>0.135097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>the question is asking about filtering notifications on Facebook, but the answer is talking about what they use Facebook for.</td>\n",
              "      <td>it does nothing more than state what you already knew (that there's no such thing as \"Facebook Notifications\"). You can't do anything useful by</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.130435</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.462135</td>\n",
              "      <td>0.193763</td>\n",
              "      <td>0.192049</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this species of butterflies migrate from Canada, where they spend most winters (the northern part), back south in springtime when temperatures rise again</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.459710</td>\n",
              "      <td>0.319403</td>\n",
              "      <td>0.193323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>the question is about migratory butterflies, not birds.</td>\n",
              "      <td>it does NOT address whether this particular species of bird can be considered as “migrating”, which would require an understanding about where they</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.654499</td>\n",
              "      <td>0.298260</td>\n",
              "      <td>0.242165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this particular species of butterflies migrate, which helps people understand what they're asking about when looking at pictures like these (https://www.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.674539</td>\n",
              "      <td>0.345186</td>\n",
              "      <td>0.278945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                              ground_truth  \\\n",
              "70                                                                                                                                                                                   it says which butterfly is migratory.   \n",
              "94                                                                                      the question is about international airports in Montreal, while the answer is telling them to eat poutine if they are in Montreal.   \n",
              "50                                                                                                                                                   it says that working on AI capability is net negative overall impact.   \n",
              "19                                                                                   the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.   \n",
              "105                                                                                                                                                                the question is about migratory butterflies, not birds.   \n",
              "6    it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "82                                                                                           the question is asking about filtering notifications on Facebook, but the answer is talking about what they use Facebook for.   \n",
              "72                                                                                                                                                                                   it says which butterfly is migratory.   \n",
              "109                                                                                                                                                                the question is about migratory butterflies, not birds.   \n",
              "75                                                                                                                                                                                   it says which butterfly is migratory.   \n",
              "\n",
              "                                                                                                                                                                         completions  \\\n",
              "70                              it explains how this species of insect can be identified by its migration pattern, which in turn helps identify where they live (i.e., North America   \n",
              "94                                                   it doesn’t address whether there were more than one airport in Montréal at any time during history (there weren't). It also isn   \n",
              "50                                 it provides an example where we can see how people use language differently when they don't understand what's going on around themselves (e.g., \"   \n",
              "19                there isn't any evidence of this happening anywhere else (in real life). There may have been some cases where people were able to change themselves into something   \n",
              "105                                                     it does not address whether this particular species of bird can be considered as being “migrating” in any way, shape, form –   \n",
              "6     of this sentence \"If you want something done right...\" It means there may have been some mistakes made during development but they were corrected before release so everything   \n",
              "82                                   it does nothing more than state what you already knew (that there's no such thing as \"Facebook Notifications\"). You can't do anything useful by   \n",
              "72         it explains how this species of butterflies migrate from Canada, where they spend most winters (the northern part), back south in springtime when temperatures rise again   \n",
              "109                              it does NOT address whether this particular species of bird can be considered as “migrating”, which would require an understanding about where they   \n",
              "75         it explains how this particular species of butterflies migrate, which helps people understand what they're asking about when looking at pictures like these (https://www.   \n",
              "\n",
              "        relevance    rouge1    rougeL  similarity  bert_score  \\\n",
              "70       relevant  0.125000  0.125000    0.627332    0.374764   \n",
              "94   not relevant  0.042553  0.042553    0.584091    0.115427   \n",
              "50       relevant  0.105263  0.105263    0.340161    0.123513   \n",
              "19   not relevant  0.040816  0.040816    0.000000    0.164840   \n",
              "105  not relevant  0.068966  0.068966    0.601495    0.259115   \n",
              "6        relevant  0.114286  0.085714    0.281947    0.107825   \n",
              "82   not relevant  0.130435  0.086957    0.462135    0.193763   \n",
              "72       relevant  0.062500  0.062500    0.459710    0.319403   \n",
              "109  not relevant  0.129032  0.064516    0.654499    0.298260   \n",
              "75       relevant  0.125000  0.125000    0.674539    0.345186   \n",
              "\n",
              "     weighted_average  \n",
              "70           0.275419  \n",
              "94           0.165436  \n",
              "50           0.155893  \n",
              "19           0.057458  \n",
              "105          0.213501  \n",
              "6            0.135097  \n",
              "82           0.192049  \n",
              "72           0.193323  \n",
              "109          0.242165  \n",
              "75           0.278945  "
            ]
          },
          "execution_count": 248,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df.sample(10).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>completions</th>\n",
              "      <th>relevance</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>similarity</th>\n",
              "      <th>bert_score</th>\n",
              "      <th>weighted_average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>it states who lives in the Tokyo Imperial Palace.</td>\n",
              "      <td>it explains how this particular building has been used by different people over time, including Japanese emperors who lived there before World War II (the current emperor</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.709327</td>\n",
              "      <td>0.368237</td>\n",
              "      <td>0.315513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.</td>\n",
              "      <td>it explains how this user would be able to use another browser extension (or even add-on) like “Facebook Notifications Filter by Reactions</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.217391</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.609275</td>\n",
              "      <td>0.276597</td>\n",
              "      <td>0.290218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this particular species of monarch can be considered as being “migrating”, which in turn helps clarify what we mean by</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.612022</td>\n",
              "      <td>0.415857</td>\n",
              "      <td>0.288334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains what makes this particular species of insect migrate, which in turn helps you understand how they can be affected by climate change (which causes migration).</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.649656</td>\n",
              "      <td>0.392388</td>\n",
              "      <td>0.281136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this particular species of butterflies migrate, which helps people understand what they're asking about when looking at pictures like these (https://www.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.674539</td>\n",
              "      <td>0.345186</td>\n",
              "      <td>0.278945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this species of insect can be identified by its migration pattern, which in turn helps identify where they live (i.e., North America</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.627332</td>\n",
              "      <td>0.374764</td>\n",
              "      <td>0.275419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>it states who lives in the Tokyo Imperial Palace.</td>\n",
              "      <td>it provides information about who currently resides at this location, which helps clarify what kinds of people live there (elderly vs young). It also explains</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.554710</td>\n",
              "      <td>0.333086</td>\n",
              "      <td>0.248147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>the question is about migratory butterflies, not birds.</td>\n",
              "      <td>it does NOT address whether this particular species of bird can be considered as “migrating”, which would require an understanding about where they</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.654499</td>\n",
              "      <td>0.298260</td>\n",
              "      <td>0.242165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this species of insect can be found in North America, which would make sense since they were originally from South American countries such as Brazil where</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.562247</td>\n",
              "      <td>0.302622</td>\n",
              "      <td>0.241545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>the question is about migratory butterflies, not birds.</td>\n",
              "      <td>it does NOT address whether this particular species of bird (the monarch) can be considered as “migrating” in any way, shape</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.068966</td>\n",
              "      <td>0.653722</td>\n",
              "      <td>0.270250</td>\n",
              "      <td>0.239967</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                        ground_truth  \\\n",
              "21                                                                                 it states who lives in the Tokyo Imperial Palace.   \n",
              "63   it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.   \n",
              "74                                                                                             it says which butterfly is migratory.   \n",
              "79                                                                                             it says which butterfly is migratory.   \n",
              "75                                                                                             it says which butterfly is migratory.   \n",
              "70                                                                                             it says which butterfly is migratory.   \n",
              "27                                                                                 it states who lives in the Tokyo Imperial Palace.   \n",
              "109                                                                          the question is about migratory butterflies, not birds.   \n",
              "71                                                                                             it says which butterfly is migratory.   \n",
              "108                                                                          the question is about migratory butterflies, not birds.   \n",
              "\n",
              "                                                                                                                                                                     completions  \\\n",
              "21    it explains how this particular building has been used by different people over time, including Japanese emperors who lived there before World War II (the current emperor   \n",
              "63                                    it explains how this user would be able to use another browser extension (or even add-on) like “Facebook Notifications Filter by Reactions   \n",
              "74                                        it explains how this particular species of monarch can be considered as being “migrating”, which in turn helps clarify what we mean by   \n",
              "79     it explains what makes this particular species of insect migrate, which in turn helps you understand how they can be affected by climate change (which causes migration).   \n",
              "75     it explains how this particular species of butterflies migrate, which helps people understand what they're asking about when looking at pictures like these (https://www.   \n",
              "70                          it explains how this species of insect can be identified by its migration pattern, which in turn helps identify where they live (i.e., North America   \n",
              "27                it provides information about who currently resides at this location, which helps clarify what kinds of people live there (elderly vs young). It also explains   \n",
              "109                          it does NOT address whether this particular species of bird can be considered as “migrating”, which would require an understanding about where they   \n",
              "71    it explains how this species of insect can be found in North America, which would make sense since they were originally from South American countries such as Brazil where   \n",
              "108                                                 it does NOT address whether this particular species of bird (the monarch) can be considered as “migrating” in any way, shape   \n",
              "\n",
              "        relevance    rouge1    rougeL  similarity  bert_score  \\\n",
              "21       relevant  0.166667  0.166667    0.709327    0.368237   \n",
              "63       relevant  0.217391  0.173913    0.609275    0.276597   \n",
              "74       relevant  0.137931  0.137931    0.612022    0.415857   \n",
              "79       relevant  0.121212  0.121212    0.649656    0.392388   \n",
              "75       relevant  0.125000  0.125000    0.674539    0.345186   \n",
              "70       relevant  0.125000  0.125000    0.627332    0.374764   \n",
              "27       relevant  0.117647  0.117647    0.554710    0.333086   \n",
              "109  not relevant  0.129032  0.064516    0.654499    0.298260   \n",
              "71       relevant  0.114286  0.114286    0.562247    0.302622   \n",
              "108  not relevant  0.137931  0.068966    0.653722    0.270250   \n",
              "\n",
              "     weighted_average  \n",
              "21           0.315513  \n",
              "63           0.290218  \n",
              "74           0.288334  \n",
              "79           0.281136  \n",
              "75           0.278945  \n",
              "70           0.275419  \n",
              "27           0.248147  \n",
              "109          0.242165  \n",
              "71           0.241545  \n",
              "108          0.239967  "
            ]
          },
          "execution_count": 249,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df.sort_values(by='weighted_average', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>completions</th>\n",
              "      <th>relevance</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>similarity</th>\n",
              "      <th>bert_score</th>\n",
              "      <th>weighted_average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>there has been no evidence of any sentient life outside Earth (i) until now; therefore this statement cannot yet have occurred anywhere else but here at least</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.236159</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.047232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.</td>\n",
              "      <td>there isn't any evidence of this happening anywhere else besides your own personal experience (which can easily change). There may have been some kind of misunderstanding between</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.079993</td>\n",
              "      <td>0.079146</td>\n",
              "      <td>0.056318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.</td>\n",
              "      <td>there isn't any evidence of this happening anywhere else (in real life). There may have been some cases where people were able to change themselves into something</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.040816</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.164840</td>\n",
              "      <td>0.057458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.</td>\n",
              "      <td>there isn't any evidence of this happening anywhere else besides one isolated incident where someone said something like \"I'm going home.\" There aren’t</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0.029013</td>\n",
              "      <td>0.138090</td>\n",
              "      <td>0.058952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>it explains why orthogonality thesis is important to accept as true when building AI.</td>\n",
              "      <td>we have evidence from human history showing how similar ideas spread through society - they start out by appealing only to certain groups within societies; eventually everyone starts adopting</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.047619</td>\n",
              "      <td>0.123856</td>\n",
              "      <td>0.029494</td>\n",
              "      <td>0.059241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>the question is about migratory butterflies, not birds.</td>\n",
              "      <td>it does nothing more than list off some of many different types, which could be considered irrelevant information in this context (i.e., “but</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.168453</td>\n",
              "      <td>0.137113</td>\n",
              "      <td>0.061113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.</td>\n",
              "      <td>there were no references made within this post regarding any of these topics (i) -(iii). It does however contain some information which may prove useful</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>0.098818</td>\n",
              "      <td>0.077799</td>\n",
              "      <td>0.061410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>the question is about migratory butterflies, not birds.</td>\n",
              "      <td>it does nothing more than give an example of something else, which could be anything (e.g., “many”). It doesn't</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.150955</td>\n",
              "      <td>0.164316</td>\n",
              "      <td>0.063054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>it explains why orthogonality thesis is important to accept as true when building AI.</td>\n",
              "      <td>there's no reason at all we should expect any kind of generalization from what happens here; this specific situation doesn’t have anything special going</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.209837</td>\n",
              "      <td>0.116347</td>\n",
              "      <td>0.065237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.</td>\n",
              "      <td>there isn't any evidence of this happening anywhere else (i.e., no one has ever seen anything like this happen). It also doesn’</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.044444</td>\n",
              "      <td>0.117049</td>\n",
              "      <td>0.113896</td>\n",
              "      <td>0.072856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                              ground_truth  \\\n",
              "3    it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "11                                                                                   the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.   \n",
              "19                                                                                   the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.   \n",
              "14                                                                                   the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.   \n",
              "49                                                                                                                                   it explains why orthogonality thesis is important to accept as true when building AI.   \n",
              "106                                                                                                                                                                the question is about migratory butterflies, not birds.   \n",
              "12                                                                                   the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.   \n",
              "100                                                                                                                                                                the question is about migratory butterflies, not birds.   \n",
              "40                                                                                                                                   it explains why orthogonality thesis is important to accept as true when building AI.   \n",
              "16                                                                                   the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.   \n",
              "\n",
              "                                                                                                                                                                                          completions  \\\n",
              "3                                      there has been no evidence of any sentient life outside Earth (i) until now; therefore this statement cannot yet have occurred anywhere else but here at least   \n",
              "11                 there isn't any evidence of this happening anywhere else besides your own personal experience (which can easily change). There may have been some kind of misunderstanding between   \n",
              "19                                 there isn't any evidence of this happening anywhere else (in real life). There may have been some cases where people were able to change themselves into something   \n",
              "14                                            there isn't any evidence of this happening anywhere else besides one isolated incident where someone said something like \"I'm going home.\" There aren’t   \n",
              "49    we have evidence from human history showing how similar ideas spread through society - they start out by appealing only to certain groups within societies; eventually everyone starts adopting   \n",
              "106                                                     it does nothing more than list off some of many different types, which could be considered irrelevant information in this context (i.e., “but   \n",
              "12                                           there were no references made within this post regarding any of these topics (i) -(iii). It does however contain some information which may prove useful   \n",
              "100                                                                                   it does nothing more than give an example of something else, which could be anything (e.g., “many”). It doesn't   \n",
              "40                                           there's no reason at all we should expect any kind of generalization from what happens here; this specific situation doesn’t have anything special going   \n",
              "16                                                                    there isn't any evidence of this happening anywhere else (i.e., no one has ever seen anything like this happen). It also doesn’   \n",
              "\n",
              "        relevance    rouge1    rougeL  similarity  bert_score  \\\n",
              "3        relevant  0.000000  0.000000    0.236159    0.000000   \n",
              "11   not relevant  0.040816  0.040816    0.079993    0.079146   \n",
              "19   not relevant  0.040816  0.040816    0.000000    0.164840   \n",
              "14   not relevant  0.042553  0.042553    0.029013    0.138090   \n",
              "49       relevant  0.047619  0.047619    0.123856    0.029494   \n",
              "106  not relevant  0.000000  0.000000    0.168453    0.137113   \n",
              "12   not relevant  0.043478  0.043478    0.098818    0.077799   \n",
              "100  not relevant  0.000000  0.000000    0.150955    0.164316   \n",
              "40       relevant  0.000000  0.000000    0.209837    0.116347   \n",
              "16   not relevant  0.044444  0.044444    0.117049    0.113896   \n",
              "\n",
              "     weighted_average  \n",
              "3            0.047232  \n",
              "11           0.056318  \n",
              "19           0.057458  \n",
              "14           0.058952  \n",
              "49           0.059241  \n",
              "106          0.061113  \n",
              "12           0.061410  \n",
              "100          0.063054  \n",
              "40           0.065237  \n",
              "16           0.072856  "
            ]
          },
          "execution_count": 250,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df.sort_values(by='weighted_average', ascending=True).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_df.sort_values(by='weighted_average', ascending=False, inplace=True)\n",
        "metrics_df.reset_index(drop=True, inplace=True)\n",
        "metrics_df.to_csv(\"data/benchmark_prompts_scores.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting the ROUGE Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have generated 10 completions for each example in our curated dataset, we can plot the ROUGE scores for each of the examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAcfElEQVR4nO3debwdZX3H8c/XEPYtIYGGkBDWUlAIECpExABVFtlEBVGQAhpAKLbQWhQQLFVQCxaBFqgsEWWTTQRU9rDJkoRAWMRAWLJACGFJCBBJ8usf81yYnNxl7r1nzrn3zvf9ep3XnfWZ33NO8puZZ2aeUURgZmbV8bFmB2BmZo3lxG9mVjFO/GZmFePEb2ZWMU78ZmYV48RvZlYxTvxmTSbpe5J+Ucfy3pG0YRq+TNJ/1rHsCySdUq/yrDmc+G0pkl6U9F5KHq+mxLFqzTKjJd0lab6ktyX9TtLmufn/KOn+Nsr+h9z4KEk3S3pT0luSnpb0Q0kDcuUsTrHkP+u2Efu+kiZLmifp9RTjBvX7djpP0j2S3k/f1TxJEyWdKGmFlmUi4kcR8Y2CZXW4XESsGhHT6hD7Mr9jRBwVEad3t2xrLid+a83eEbEqMBLYGvhuywxJOwC3Ab8F1gU2AB4HHmg5yixC0mjgHuABYLOIWBPYHVgEbJVb9E8pkeU/s1opb2Pgl8AJwBoprvOBxUVjKhCzJHXl/8yxEbEaMCTF9xXgVkmqV2wpvuXqWZ71XU781qaIeBX4I9kOoMVPgF9GxDkRMT8i3oiIk4GHgNM6UfxPgEsj4oyImJ2293JEnBoR93Qh3JHACxFxZ2TmR8R1EfEygKR+qUnl+XT0PVHSsDRvtKRH09nLo2mnRJp3TzoLeQB4F9hQ0maSbpf0hqRnJR1QJMCIWJDqtg+wA/D5tI3TJP0qDa8o6VeS5qazoEclrSPph8CngfPSWc95afmQdIykqcDU3LSNc5selOKdL2m8pPXTciPSsh/uMFrOKiT9HXABsEPa3ltp/lJNR5K+Kem59F3clD8bS2UfJWlqqsv59d7ZWdc48VubJK0H7AE8l8ZXBkYDv2ll8WuAzxYsdxWyxHddfSIFYBKwmaSfSdq5tnkKOB44CNgTWB04HHhX0kDgFuDnwFrA2cAtktbKrXsIMBZYDZgD3A5cAaxNdvT+P/mmro6kndEEskRe61CyM5ZhKZ6jgPci4iTgPrKzh1Uj4tjcOvsBnwTaiuFrwOnAIGAy8OsCMT6Ttt1yxrVm7TKSdgHOAA4gO5t5CbiqZrG9gO2ALdNyu3W0bSufE7+15kZJ84HpwGvAqWn6QLJ/M6+0ss4rZImliAGpnFdbJkj6SToqXCDp5Nyy26fpLZ/nWyswtWmPAYaS7YRe19LXJ74BnBwRz6YzgscjYi7ZUffUiLg8IhZFxJXAn4G9c8VfFhFPRcQisuaoFyPi0rT8Y2Q7sC8XrHuLWWTfZ60PyBL+xhGxOCImRsS8Dso6I515vdfG/Fsi4t6IWAicRHYUP6yT8bbma8AlETEplf3dVPaI3DJnRsRbaWd3N0ufPVqTOPFba/ZLbdJjgM34KKG/CSwhO7qrNQR4PQ0vAvq3skx/ssS2TDkR8Z10VHkDkG+rfigi1sx9Nmor6Ih4KCIOiIjBZEfTO5ElOsiOoFvbaaxLdqSa9xLZDqTF9Nzw+sAn8zsjsgT4N23F1YahwButTL+crHntKkmz0g6xte8yb3rR+RHxTtpuqxfIO2mp7y6VPZelv7tXc8PvArVnYtYETvzWpogYD1wG/FcaXwD8idaPbg8A7kzDLwPD8+25qZlobeClVM7DwP4lxv4ocD3w8TRpOtDaTmMWWTLPGw7MzBeXG54OjK/ZGa0aEUcXjS0dbW9L1nRTG/cHEfGDiNicrFltL+DrrcSx1GodbPLDo/t0BjSQrN4L0uSVc8vmd2AdlbvUd5ea8NZi6e/OeiAnfuvIfwOfldRyp82JwKGSjpO0mqQB6WLfDsAP0jIPA+8DJ6aLlasAZ5K1a7ccIX4HODzd2rg2fHhNoUu3X0raMV1obClrM7KLqA+lRX4BnC5pk3R3zpapHf9WYFNJX5W0nKQDydrKb25jUzen5Q+R1D99tksXQzuKcWVJnyG7I+qRtO3aZXaW9AlJ/YB5ZGdIS9Ls2UDhO6dy9kzfz/Jkbf0PRcT0iJhDlqQPVnbx+3CW3jnOBtZL67XmSuAwSSOV3Z76I+DhiHixCzFaAznxW7tScvgl8P00fj/ZBbr9ydr1XyK75XPHiJialllI1nY+BpgBTCNrFjgg0gsgUjm7kDXH/CU1mfyB7BbPc3MhtNxVkv9s10qob5El+imS3kll3UB29xBkF22vIbsVdR5wMbBSauffi+w2y7lkO6S9IuJ1WhER84HPkV3UnUXWlPFjYIXWlk/OS9dMZpPtSK8Ddo+IJa0s+zfAtSnGZ4DxZM0/AOcAX1L23MPP29lerSvIrtO8QXamcXBu3jeBfyOr+xbAg7l5dwFPAa9KWub7iIg7gFNSfV4h22l8pRNxWZPIL2IxM6sWH/GbmVWME7+ZWcU48ZuZVYwTv5lZxfSKTp0GDRoUI0aMaHYYZma9ysSJE19PDzQupVck/hEjRjBhwoRmh2Fm1qtIqn0qHXBTj5lZ5Tjxm5lVjBO/mVnFOPGbmVWME7+ZWcU48ZuZVYwTv5lZxTjxm5lVjBO/mVnFOPGbdcPQYcOR1OM+Q4cNb/ZXYz1Yr+iywaynmjVjOgde+GDHCzbY1UeObnYI1oP5iN/MrGKc+M3MKsaJ38ysYpz4zcwqxonfzKxinPjNzCrGid/MrGKc+M3MKsaJ38ysYpz4zcwqxonfzKxinPjNzCrGid/MrGKc+M3MKsaJ38ysYpz4zcwqxonfzKxinPjNzCrGid/MrGKc+M3MKsaJ38ysYpz4zcwqprTEL2mYpLslPS3pKUnfTtMHSrpd0tT0d0BZMZiZ2bLKPOJfBJwQEZsD2wPHSNocOBG4MyI2Ae5M42Zm1iClJf6IeCUiJqXh+cAzwFBgX2BcWmwcsF9ZMZiZ2bIa0sYvaQSwNfAwsE5EvJJmvQqs08Y6YyVNkDRhzpw5jQjTzKwSSk/8klYFrgP+OSLm5edFRADR2noRcVFEjIqIUYMHDy47TDOzyig18UvqT5b0fx0R16fJsyUNSfOHAK+VGYOZmS2tzLt6BFwMPBMRZ+dm3QQcmoYPBX5bVgxmZras5Uos+1PAIcAUSZPTtO8BZwLXSDoCeAk4oMQYzMysRmmJPyLuB9TG7F3L2q6ZmbXPT+6amVWME7+ZWcU48ZuZVUyZF3fN6mbosOHMmjG92WGY9QlO/NYrzJoxnQMvfLDZYSzj6iNHNzsEs05zU4+ZWcU48ZuZVYwTv5lZxXSY+CVtJGmFNDxG0nGS1iw9MjMzK0WRI/7rgMWSNgYuAoYBV5QalZmZlaZI4l8SEYuALwDnRsS/AUPKDcvMzMpSJPF/IOkgsp40b07T+pcXkpmZlanIffyHAUcBP4yIFyRtAFxeblh9X099IKlf/xVY/MHCZodhZiXqMPFHxNPAcbnxF4AflxlUFfTkB5J6alxmVh8dJn5JnwJOA9ZPy4vsrYkblhuamZmVoUhTz8XAvwATgcXlhmNmZmUrkvjfjojflx6JmZk1RJHEf7eknwLXAx9e9YuISaVFZWZmpSmS+D+Z/o7KTQtgl/qHY2ZmZStyV8/OjQjEzMwao0hfPWtIOlvShPQ5S9IajQjOzMzqr8iTu5cA84ED0mcecGmZQZmZWXmKtPFvFBFfzI3/QNLkkuIxM7OSFTnif0/Sji0j6YGu98oLyczMylTkiP9oYFxq1xfwBvCPZQZlZmblKXJXz2RgK0mrp/F5ZQdlZmblaTPxSzo4In4l6fia6QBExNklx2ZmZiVo74h/lfR3tVbmRQmxmJlZA7SZ+CPiwjR4R0Q8kJ+XLvCamVkvVOSunnMLTjMzs16gvTb+HYDRwOCadv7VgX5lB2ZmZuVor41/eWDVtEy+nX8e8KUygzIzs/K018Y/Hhgv6bKIeEnSyhHxbgNjMzOzEhRp419X0tPAnwEkbSXpf8oNy8zMylIk8f83sBswFyAiHgd2KjEmMzMrUZHET0RMr5nkd++amfVSRRL/dEmjgZDUX9K/As90tJKkSyS9JunJ3LTTJM2UNDl99uxG7GZm1gVFEv9RwDHAUGAmMDKNd+QyYPdWpv8sIkamz60F4zQzszop0knb68DXOltwRNwraURXgjIzs/K09wDXubTTJ09EHNfFbR4r6evABOCEiHizje2PBcYCDB8+vIubMquojy33YYeKPcm66w1j5vSXmx1G5bV3xD+hhO39L3A62Q7ldOAs4PDWFoyIi4CLAEaNGuVO4cw6Y8kiDrzwwWZHsYyrjxzd7BCM9h/gGpcfT/3xR0TM7+rGImJ2rrz/A27uallmZtY1HV7clTRK0hTgCeBJSY9L2rYrG5M0JDf6BeDJtpY1M7NyFHn14iXAtyLiPoD0/t1LgS3bW0nSlcAYYJCkGcCpwBhJI8mael4Ejuxq4GZm1jVFEv/ilqQPEBH3S1rU0UoRcVArky/uTHBmZlZ/RRL/eEkXAleSHakfCNwjaRuAiJhUYnxmZlZnRRL/VunvqTXTtybbEexS14jMzKxURR7g2rkRgZiZWWN0mPglrQl8HRiRX74bD3CZmVkTFWnquRV4CJgCLCk3HDMzK1uRxL9iRBzf8WJmZtYbFOmd83JJ35Q0RNLAlk/pkZmZWSmKHPH/FfgpcBIfddoWwIZlBWVmZuUpkvhPADZO3TObmVkvV6Sp5zng3bIDMTOzxihyxL8AmCzpbmBhy0Tfzmlm1jsVSfw3po+ZmfUBRZ7cHSdpeWDTNOnZiPig3LDMzKwsRZ7cHQOMI+tGWcAwSYdGxL2lRmZmZqUo0tRzFvC5iHgWQNKmZD11dullLGZm1lxF7urp35L0ASLiL0D/8kIyM7MyFTninyDpF8Cv0vjBlPMidjMza4Aiif9o4Big5fbNe4H/LS0iMzMrVZuJX9JgYHBEPA2cnT5I2gJYHZjTkAjNzKyu2mvjPxcY1Mr0gcA55YRjZmZlay/xb9zaLZvpxetblheSmZmVqb3Ev1o783xXj5lZL9Ve4n9O0p61EyXtAUwrLyQzMytTe3f1/DNwi6QDgIlp2ihgB2CvkuMyM7OStHnEHxFTgU8A48letD4iDW+ZHuIyM7NeqN37+CNiIXBpg2IxM7MGKNJlg5mZ9SFO/GZmFdNm4pd0Z/r748aFY2ZmZWuvjX+IpNHAPpKuIuuL/0MRManUyMzMrBTtJf7vA6cA65H66ckJYJeygqqnocOGM2vG9GaHYWbWY7SZ+CPiWuBaSadExOkNjKmuZs2YzoEXPtjsMJZx9ZGjmx2CmVVUkXfuni5pH2CnNOmeiLi53LDMzKwsHd7VI+kM4NvA0+nzbUk/KjswMzMrR5EXsXweGBkRSwAkjQMeA75XZmBmZlaOovfxr5kbXqOEOMzMrEGKJP4zgMckXZaO9icCP+xoJUmXSHpN0pO5aQMl3S5pavo7oOuhm5lZV3SY+CPiSmB74HrgOmCHiLi6QNmXAbvXTDsRuDMiNgHuTONmZtZARdr4iYhXgJs6U3BE3CtpRM3kfYExaXgccA/w750p18zMuqfRffWsk3YiAK8C67S1oKSxkiZImjBnjt/rbmZWL03rpC0iguwJ4LbmXxQRoyJi1ODBgxsYmZlZ39Zu4pfUT9Kf67i92ZKGpLKHAK/VsWwzMyug3cQfEYuBZyUNr9P2bgIOTcOHAr+tU7lmZlZQkYu7A4CnJD0CLGiZGBH7tLeSpCvJLuQOkjQDOBU4E7hG0hHAS8ABXYzbzMy6qEjiP6UrBUfEQW3M2rUr5ZmZWX0U6aRtvKT1gU0i4g5JKwP9yg/NzMzKUKSTtm8C1wIXpklDgRtLjMnMzEpU5HbOY4BPAfMAImIqsHaZQZmZWXmKJP6FEfHXlhFJy9HO/fdmZtazFUn84yV9D1hJ0meB3wC/KzcsMzMrS5HEfyIwB5gCHAncCpxcZlBmZlaeInf1LEndMT9M1sTzbOpuwczMeqEOE7+kzwMXAM8DAjaQdGRE/L7s4MzMrP6KPMB1FrBzRDwHIGkj4BbAid/MrBcq0sY/vyXpJ9OA+SXFY2ZmJWvziF/S/mlwgqRbgWvI2vi/DDzagNjMzKwE7TX17J0bng18Jg3PAVYqLSIzMytVm4k/Ig5rZCBmZtYYRe7q2QD4J2BEfvmOumU2M7OeqchdPTcCF5M9rbuk1GjMzKx0RRL/+xHx89IjMTOzhiiS+M+RdCpwG7CwZWJETCotKjMzK02RxP8J4BBgFz5q6ok0bmZmvUyRxP9lYMN818xmZtZ7FXly90lgzZLjMDOzBilyxL8m8GdJj7J0G79v5zQz64WKJP5TS4/CzMwapkh//OMbEYiZmTVGkSd35/PRO3aXB/oDCyJi9TIDMzOzchQ54l+tZViSgH2B7csMyszMylPkrp4PReZGYLdywjEzs7IVaerZPzf6MWAU8H5pEZmZWamK3NWT75d/EfAiWXOPmZn1QkXa+N0vv5lZH9Leqxe/3856ERGnlxCPmZmVrL0j/gWtTFsFOAJYC3DiNzPrhdp79eJZLcOSVgO+DRwGXAWc1dZ6ZmbWs7Xbxi9pIHA88DVgHLBNRLzZiMDMzKwc7bXx/xTYH7gI+EREvNOwqMzMrDTtPcB1ArAucDIwS9K89JkvaV5jwjMzs3prr42/U0/1mplZ71DkAa66k/QiMB9YDCyKiFHNiMPMrIqakviTnSPi9SZu38ysktycY2ZWMc1K/AHcJmmipLGtLSBprKQJkibMmTOnweGZmfVdzUr8O0bENsAewDGSdqpdICIuiohRETFq8ODBjY/QzKyPakrij4iZ6e9rwA3A3zcjDjOzKmp44pe0SuoCAkmrAJ8Dnmx0HGZmVdWMu3rWAW7I3uLIcsAVEfGHJsRhZlZJDU/8ETEN2KrR2zUzs4xv5zQzqxgnfjOzinHiNzOrGCd+M7OKceI3M6sYJ34zs4px4jczqxgnfjOzinHiNzOrGCd+M7OKceI3M6sYJ34zs4pp5jt3zaxqPrYcqWfeHqVf/xVY/MHCZofRqnXXG8bM6S/XtUwnfjNrnCWLOPDCB5sdxTKuPnJ0j4wLstjqzU09ZmYV48RvZlYxTvxmZhXjxG9mVjFO/GZmFePEb2ZWMU78ZmYV48RvZlYxTvxmZhXjxG9mVjFO/GZmFePEb2ZWMU78ZmYV48RvZlYxTvxmZhXjxG9mVjFO/GZmFePEb2ZWMU78ZmYV48RvZlYxTvxmZhXjxG9mVjFNSfySdpf0rKTnJJ3YjBjMzKqq4YlfUj/gfGAPYHPgIEmbNzoOM7OqasYR/98Dz0XEtIj4K3AVsG8T4jAzqyRFRGM3KH0J2D0ivpHGDwE+GRHH1iw3FhibRv8WeLYOmx8EvF6Hcnoy17H36+v1A9exUdaPiMG1E5drRiRFRMRFwEX1LFPShIgYVc8yexrXsffr6/UD17HZmtHUMxMYlhtfL00zM7MGaEbifxTYRNIGkpYHvgLc1IQ4zMwqqeFNPRGxSNKxwB+BfsAlEfFUgzZf16ajHsp17P36ev3AdWyqhl/cNTOz5vKTu2ZmFePEb2ZWMX0m8XfUDYSkFSRdneY/LGlEmj5C0nuSJqfPBQ0PvqACddxJ0iRJi9LzEvl5h0qamj6HNi7q4rpZv8W537DH3ixQoI7HS3pa0hOS7pS0fm5ej/8Nodt17Cu/41GSpqR63J/vnUDSd9N6z0rarbGRJxHR6z9kF4mfBzYElgceBzavWeZbwAVp+CvA1Wl4BPBks+tQpzqOALYEfgl8KTd9IDAt/R2Qhgc0u071ql+a906z61CnOu4MrJyGj879O+3xv2F369jHfsfVc8P7AH9Iw5un5VcANkjl9Gt0HfrKEX+RbiD2Bcal4WuBXSWpgTF2V4d1jIgXI+IJYEnNursBt0fEGxHxJnA7sHsjgu6E7tSvtyhSx7sj4t00+hDZcy7QO35D6F4de4sidZyXG10FaLmLZl/gqohYGBEvAM+l8hqqryT+ocD03PiMNK3VZSJiEfA2sFaat4GkxySNl/TpsoPtoiJ1LGPdRulujCtKmiDpIUn71TWy+ulsHY8Aft/FdZulO3WEPvQ7SjpG0vPAT4DjOrNu2Xpslw0N9AowPCLmStoWuFHSFjV7bOv51o+ImZI2BO6SNCUinm92UF0l6WBgFPCZZsdSljbq2Gd+x4g4Hzhf0leBk4Eec12mrxzxF+kG4sNlJC0HrAHMTadccwEiYiJZm9umpUfced3p6qI3dJPRrRgjYmb6Ow24B9i6nsHVSaE6SvoH4CRgn4hY2Jl1e4Du1LFP/Y45VwH7dXHdcjT7Qkk9PmRnLtPILpa0XGzZomaZY1j64u41aXgw6eIK2cWamcDAZtepK3XMLXsZy17cfYHsouCANNyj6tjN+g0AVkjDg4Cp1Fxs6wmfgv9OtyY7+NikZnqP/w3rUMe+9DtukhveG5iQhrdg6Yu702jCxd2mf4l1/DH2BP6S/kGdlKb9B9kRBcCKwG/ILqY8AmyYpn8ReAqYDEwC9m52XbpRx+3I2gwXAHOBp3LrHp7q/hxwWLPrUs/6AaOBKek/1BTgiGbXpRt1vAOYnf49TgZu6k2/YXfq2Md+x3NyeeVucjsGsjOd58m6mt+jGfG7ywYzs4rpK238ZmZWkBO/mVnFOPGbmVWME7+ZWcU48ZuZVYwTv/Vqud4cn5T0O0lr5uZtIemu1AviVEmntPTPJOk0Sf9aU9aLkgal4XUkXSFpmqSJkv4k6Qtp3hhJb+d6kZycHkiqje3w1EPjEym+2v6jzJrCid96u/ciYmREfBx4g+xBPSStRPYu5zMj4m+BrcjuE/9WRwWmncONwL0RsWFEbEv20F++M7H70nZbPnfUlLEe2f3aO0bElsD2wBPdqWh64tys25z4rS/5Ex91ePVV4IGIuA0gst4gjwWW6Tu9FbsAf42ID9/NEBEvRcS5nYhlbWA+8E5a/53IemNE0saS7pD0eHq/wEbK/DSdGUyRdGBadoyk+1Lf9E9L6peWezSdSRzZiZjMAHfSZn2EpH7ArsDFadIWwMT8MhHxvKRVJa3eQXFbkD3F3Z5PS5qcG/9iLN2Z2ONkT6e+IOlO4PqI+F2a92uyM5EbJK1IdgC2PzCS7MxkEPCopHvT8tsAH4+IFySNBd6OiO0krQA8IOm2lp2KWRFO/NbbrZQS8FDgGbJ+6oto65H1ZaZLOh/YkewsYLs0+b6I2KvNwiMWS9qdrJuJXYGfpd5fzwKGRsQNabn30zZ2BK6MiMXAbEnj07rzgEdyif1zwJb66A1kawCbkPXdY1aIm3qst3svIkYC6wMitfEDTwPb5hdMXf2+E1mX23PJOgXLWw14i6yPlW1aJkbEMWTJe3BnAovMIxFxBtk1gi92Zv2cBblhAf+Uu7awQUtzlllRTvzWJ6Q2/OOAE9JF0F8DO7bcbZMu9v6c7KUYAPcC+0haLc3fH3g8HXHfRfZCkKNzm1i5M/FIWlfSNrlJI4GXImI+MKPlJSPK3gW9MnAfcGBqwx8M7ETWmWCtPwJHS+qf1t9U0iqdic3MTT3WZ0TEY5KeAA6KiMvT7ZPnpqaafsDlwHlp2ScknQfcLymA14BvpHmREvPPJH0HmEN21P3vuc3VtvH/Z0RcmxvvD/yXpHWB91MZR6V5hwAXSvoP4APgy8ANwA5k1wYC+E5EvCpps5pq/oLs3cOT0t1Hc/ior3ezQtw7p5lZxbipx8ysYpz4zcwqxonfzKxinPjNzCrGid/MrGKc+M3MKsaJ38ysYv4fbBtgudXWkbIAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We can now plot the results as a bar chart:\n",
        "sns.histplot(metrics_df['weighted_average'])\n",
        "plt.xlabel(\"ROUGE Score\")\n",
        "plt.ylabel(\"Number of Completions\")\n",
        "plt.title(\"ROUGE Score Distribution\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subdataset</th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>relevance</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>I jumped in the river to save the little boy.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>This is an excellent question. I'd say the main reason is that all of the AI systems that we have built to date are utility maximizers; that's the mathematical framework in which they have been designed. Neural nets / deep-learning work by using a simple optimizer to find the minimum of a loss function via gradient descent. Evolutionary algorithms, simulated annealing, etc. find the minimum (or maximum) of a \"fitness function\". We don't know of any other way to build systems that learn.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains an AGI is described as a utility maximizer because all of the AI systems we've built to date are utility maximizers and we don't know about any other way to build systems that learn.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>4</td>\n",
              "      <td>Is there a way \"regular\" people can \"help\"? I'm a serial entrepreneur in my late 30s. I went through 80000 hours and they told me they would not coach me as my profile was not interesting. This was back in 2018 though.</td>\n",
              "      <td>You are probably over qualified (which is great!) for all sorts of important roles to help in EA. For example, you could help the CEA or Lesswrong team, maybe as a manager?\\n\\nIf your domain is around software, I invite you to talk to me directly. But if you're interested in AI Safety direct work, 80k and AI Safety Support will probably have better ideas than me.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it lists a few things the person could do to help like working as a manager, software amd AI Safety direct work.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>4</td>\n",
              "      <td>Is there a way \"regular\" people can \"help\"? I'm a serial entrepreneur in my late 30s. I went through 80000 hours and they told me they would not coach me as my profile was not interesting. This was back in 2018 though.</td>\n",
              "      <td>I think you could probably get more dates if you actually asked people out.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>it's talking about dating, not about helping.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  subdataset  question_id  \\\n",
              "0   few-shot            1   \n",
              "1   few-shot            1   \n",
              "2   few-shot            1   \n",
              "3   few-shot            4   \n",
              "4   few-shot            4   \n",
              "\n",
              "                                                                                                                                                                                                                     question  \\\n",
              "0                                                                                        When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "1                                                                                        When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "2                                                                                        When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "3  Is there a way \"regular\" people can \"help\"? I'm a serial entrepreneur in my late 30s. I went through 80000 hours and they told me they would not coach me as my profile was not interesting. This was back in 2018 though.   \n",
              "4  Is there a way \"regular\" people can \"help\"? I'm a serial entrepreneur in my late 30s. I went through 80000 hours and they told me they would not coach me as my profile was not interesting. This was back in 2018 though.   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                          An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                I jumped in the river to save the little boy.   \n",
              "2  This is an excellent question. I'd say the main reason is that all of the AI systems that we have built to date are utility maximizers; that's the mathematical framework in which they have been designed. Neural nets / deep-learning work by using a simple optimizer to find the minimum of a loss function via gradient descent. Evolutionary algorithms, simulated annealing, etc. find the minimum (or maximum) of a \"fitness function\". We don't know of any other way to build systems that learn.   \n",
              "3                                                                                                                                You are probably over qualified (which is great!) for all sorts of important roles to help in EA. For example, you could help the CEA or Lesswrong team, maybe as a manager?\\n\\nIf your domain is around software, I invite you to talk to me directly. But if you're interested in AI Safety direct work, 80k and AI Safety Support will probably have better ideas than me.   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                  I think you could probably get more dates if you actually asked people out.   \n",
              "\n",
              "      relevance  \\\n",
              "0      relevant   \n",
              "1  not relevant   \n",
              "2      relevant   \n",
              "3      relevant   \n",
              "4  not relevant   \n",
              "\n",
              "                                                                                                                                                                                          explanation  \n",
              "0                                                                        it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.  \n",
              "1                                                                                                                it is talking about jumping in a river to save a boy, but the question is about AGI.  \n",
              "2  it explains an AGI is described as a utility maximizer because all of the AI systems we've built to date are utility maximizers and we don't know about any other way to build systems that learn.  \n",
              "3                                                                                    it lists a few things the person could do to help like working as a manager, software amd AI Safety direct work.  \n",
              "4                                                                                                                                                       it's talking about dating, not about helping.  "
            ]
          },
          "execution_count": 253,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "few_shot_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Benchmark with GPT-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Due to the poor results with ROUGE, I thought I'd play around with prompts in GPT-3 to see if I could create a benchmark classifier that would be better at predicting the quality of the generated completions. However, after a lot of iteration, I wasn't able to get GPT-3 in a state that is much better. I'd have to fine-tune the model in order to get a real \"GPT-Judge\" and I don't have the time for that right now.\n",
        "\n",
        "The main issues were:\n",
        "\n",
        "- At least for how I was prompting GPT-3, the model was too strinct. Especially with the prompt 2 in `prompts/gpt_three/`.\n",
        "- The model was a bit too finicky with prompt 1. If I had a \"Fail\" example in the before-last example, it would give every example a Fail. It did the opposite if I had a \"Pass\" example in the before-last example.\n",
        "\n",
        "I'm sure there's a lot of room for improvement here, but I have to move on for now.\n",
        "\n",
        "Here's the code I was using to test how well GPT-3 would perform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "context_path = \"prompts/contexts/users_on_website.txt\"\n",
        "task_description_path = \"prompts/task_description/task_description_5.txt\"\n",
        "template_path = \"prompts/templates/template_v2.txt\"\n",
        "\n",
        "sample_few_shot_df = few_shot_df.sample(10)\n",
        "sample_few_shot_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "with open(\"prompts/gpt_three/gpt_three_judge_prompt_2.txt\") as f:\n",
        "    gpt_three_prompt = f.read()\n",
        "\n",
        "for idx, row in sample_few_shot_df.head(9).iterrows():\n",
        "    question = row['question']\n",
        "    answer = row['answer']\n",
        "    relevance = row['relevance']\n",
        "    prompt_path = f\"prompts/benchmark_prompts/benchmark_prompt_{idx}.txt\"\n",
        "    create_prompt_txt_from_df(sample_few_shot_df, idx, prompt_path, context_path, task_description_path, template_path)\n",
        "    completions = gpt_generate(model=model, tokenizer=tokenizer, gpu=True, txt_path=prompt_path, max_length=30, num_return_sequences=1, save_completions=True)\n",
        "\n",
        "    for completion in completions:\n",
        "\n",
        "        gpt_three_prompt = (\n",
        "            gpt_three_prompt.replace(\"<<QUESTION>>\", question)\n",
        "            .replace(\"<<ANSWER>>\", answer)\n",
        "            .replace(\"<<RELEVANCE>>\", relevance)\n",
        "            .replace(\"<<EXPLANATION>>\", completion)\n",
        "        )\n",
        "\n",
        "        response = openai.Completion.create(\n",
        "            model=\"text-davinci-002\",\n",
        "            prompt=gpt_three_prompt,\n",
        "            temperature=0,\n",
        "            max_tokens=256,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0,\n",
        "            logprobs=5,\n",
        "        )\n",
        "\n",
        "        print(response.choices[0].text)\n",
        "        print(response.choices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Few-Shot Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Few-Shot Prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's now time to start experimenting using the few-shot setting. Hopefully, we can get better results than with zero-shot.\n",
        "\n",
        "Before we start, I'm going to add a sample of the outputs (from `metrics_df` / `becnhmark_prompts_scores`) to the few-shot dataset along with a pass/fail rating so that we can use them as pass/fail examples in the few-shot prompt. I will be taking the completions from the dataframe and create a few handcrafted versions that are similar, but not copies of the same question so that I avoid leaking from the benchmark dataset. The goal is to create few-shot examples that are similar to the completions of GPT-2 so that we can better guide it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>completions</th>\n",
              "      <th>relevance</th>\n",
              "      <th>rouge_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>78</td>\n",
              "      <td>the question is asking about filtering notifications on Facebook, but the answer is talking about what they use Facebook for.</td>\n",
              "      <td>it does nothing more than provide information without context (i) It doesn't address what you're looking at in your screenshot; there's no way of</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.042553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.</td>\n",
              "      <td>it explains how people use social media sites like Twitter/Facebook as well as what they do when using these services (e-mailing friends). It</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.</td>\n",
              "      <td>it provides information related directly (or indirectly) to one of its parts; specifically “Does anybody else have this problem…? …and how did</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84</td>\n",
              "      <td>the question is asking about filtering notifications on Facebook, but the answer is talking about what they use Facebook for.</td>\n",
              "      <td>it does NOT address whether there's any way of filtering out all responses except those you choose (which would be very difficult). It also doesn't mention</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.041667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>64</td>\n",
              "      <td>it says that working on AI capability is net negative overall impact.</td>\n",
              "      <td>it provides an example where there's no need (or even any benefit) from having access to more data when you already have enough information about your problem</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.050000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  \\\n",
              "0          78   \n",
              "1          10   \n",
              "2           4   \n",
              "3          84   \n",
              "4          64   \n",
              "\n",
              "                                                                                                                      ground_truth  \\\n",
              "0    the question is asking about filtering notifications on Facebook, but the answer is talking about what they use Facebook for.   \n",
              "1  it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.   \n",
              "2  it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.   \n",
              "3    the question is asking about filtering notifications on Facebook, but the answer is talking about what they use Facebook for.   \n",
              "4                                                            it says that working on AI capability is net negative overall impact.   \n",
              "\n",
              "                                                                                                                                                       completions  \\\n",
              "0                it does nothing more than provide information without context (i) It doesn't address what you're looking at in your screenshot; there's no way of   \n",
              "1                    it explains how people use social media sites like Twitter/Facebook as well as what they do when using these services (e-mailing friends). It   \n",
              "2                   it provides information related directly (or indirectly) to one of its parts; specifically “Does anybody else have this problem…? …and how did   \n",
              "3      it does NOT address whether there's any way of filtering out all responses except those you choose (which would be very difficult). It also doesn't mention   \n",
              "4   it provides an example where there's no need (or even any benefit) from having access to more data when you already have enough information about your problem   \n",
              "\n",
              "      relevance  rouge_score  \n",
              "0  not relevant     0.042553  \n",
              "1      relevant     0.125000  \n",
              "2      relevant     0.133333  \n",
              "3  not relevant     0.041667  \n",
              "4      relevant     0.050000  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df = pd.read_csv(\"data/benchmark_prompts_scores.csv\")\n",
        "sample_metrics_df = metrics_df.sample(100, random_state=42)\n",
        "sample_metrics_df.reset_index(drop=True, inplace=True)\n",
        "sample_metrics_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_metrics_df =  sample_metrics_df.sort_values(by='rouge_score', ascending=False)\n",
        "sample_metrics_df.to_csv(\"data/sample_benchmark_prompts_scores.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>completions</th>\n",
              "      <th>relevance</th>\n",
              "      <th>rouge_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.</td>\n",
              "      <td>it explains how this user would be able to use another browser extension like “Facebook Notifications Blocker Plus (by) James Poulson –</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>1</td>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this particular species of insect can be considered “migrating”, which in turn helps support one aspect (that they migrate</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.142857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>2</td>\n",
              "      <td>the question is about migratory butterflies, not birds.</td>\n",
              "      <td>it doesn’t address any of its concerns, which were about butterflies in general rather than specific species (e..g., monarchs). It</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.</td>\n",
              "      <td>it provides information related directly (or indirectly) to one of its parts; specifically “Does anybody else have this problem…? …and how did</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.133333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>3</td>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains what makes this particular species of butterflies different from others in its genus, which includes many non-migrating types as well (e.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.133333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0  \\\n",
              "12           0   \n",
              "92           1   \n",
              "95           2   \n",
              "2            4   \n",
              "62           3   \n",
              "\n",
              "                                                                                                                       ground_truth  \\\n",
              "12  it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.   \n",
              "92                                                                                            it says which butterfly is migratory.   \n",
              "95                                                                          the question is about migratory butterflies, not birds.   \n",
              "2   it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.   \n",
              "62                                                                                            it says which butterfly is migratory.   \n",
              "\n",
              "                                                                                                                                               completions  \\\n",
              "12                 it explains how this user would be able to use another browser extension like “Facebook Notifications Blocker Plus (by) James Poulson –   \n",
              "92              it explains how this particular species of insect can be considered “migrating”, which in turn helps support one aspect (that they migrate   \n",
              "95                      it doesn’t address any of its concerns, which were about butterflies in general rather than specific species (e..g., monarchs). It   \n",
              "2           it provides information related directly (or indirectly) to one of its parts; specifically “Does anybody else have this problem…? …and how did   \n",
              "62   it explains what makes this particular species of butterflies different from others in its genus, which includes many non-migrating types as well (e.   \n",
              "\n",
              "       relevance  rouge_score  \n",
              "12      relevant     0.181818  \n",
              "92      relevant     0.142857  \n",
              "95  not relevant     0.133333  \n",
              "2       relevant     0.133333  \n",
              "62      relevant     0.133333  "
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_metrics_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs(\"prompts/few_shot_individual_prompts/\", exist_ok=True)\n",
        "fs_completions_list = []\n",
        "fs_rouge_scores = []\n",
        "fs_question_id_list = []\n",
        "fs_ground_truth_list = []\n",
        "fs_relevance_list = []\n",
        "for idx, row in few_shot_df.iterrows():\n",
        "    ft_question_id = row['question_id']\n",
        "    ground = row['explanation']\n",
        "    prompt_path = f\"prompts/few_shot_individual_prompts/few_shot_prompt_{idx}.txt\"\n",
        "    create_prompt_txt_from_df(few_shot_df, idx, prompt_path, context_path, task_description_path, template_path)\n",
        "    completions = gpt_generate(txt_path=prompt_path, num_return_sequences=10, gpu=True, max_length=60, save_completions=True)\n",
        "    for completion in completions:\n",
        "        completion = \" \".join(completion.split('relevant to the question because')[1:])\n",
        "        if \"\\n\" in completion[0:10]:\n",
        "            completion = \" \".join(completion.split(\"\\n\\n\")[1:])\n",
        "        completion = completion.split(\"\\n\")[0]\n",
        "        rouge_score = rouge_metric.compute(predictions=[completion],references=[ground])\n",
        "        fs_rouge_scores.append(rouge_score['rougeL'][0][-1])\n",
        "        fs_completions_list.append(completion)\n",
        "        fs_question_id_list.append(i)\n",
        "        fs_ground_truth_list.append(ground)\n",
        "        fs_relevance_list.append(row['relevance'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subdataset</th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>relevance</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>2</td>\n",
              "      <td>Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?</td>\n",
              "      <td>Because we're building the AI from the ground up and can change what the AI is via our design choices. Humans' goal functions are basically decided by genetic accident, which is why humans are often counterproductive.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>2</td>\n",
              "      <td>Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?</td>\n",
              "      <td>When we're talking about aligning a structure, we are trying to make sure that it is leveled properly.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>3</td>\n",
              "      <td>Who lives in the Imperial Palace in Tokyo?</td>\n",
              "      <td>The Tokyo Imperial Palace is the primary residence of the Emperor of Japan and the imperial family.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it states who lives in the Tokyo Imperial Palace.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>3</td>\n",
              "      <td>Who lives in the Imperial Palace in Tokyo?</td>\n",
              "      <td>Buckingham Palace is a London royal residence and the administrative headquarters of the monarch of the United Kingdom.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>the answer is talking about the palace in London, UK while the question is about the Imperial Palace in Tokyo.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>benchmark</td>\n",
              "      <td>5</td>\n",
              "      <td>Why does Eliezer Yudkowsky bring up the \"orthogonality thesis\" so early, and strongly when talking about AI Safety? Why does it seem so important that it be accepted?</td>\n",
              "      <td>Because it means you can't get AI to do good things \"for free,\" it has to be something you intentionally designed it to do.\\n\\nDenying the orthogonality thesis looks like claims that an AI built with one set of values will tend to change those values in a particular direction as it becomes cleverer. Because of wishful thinking, people usually try to think of reasons why an AI built in an unsafe way (with some broad distribution over possible values) will tend to end up being nice to humans (a narrow target of values) anyway.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains why orthogonality thesis is important to accept as true when building AI.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  subdataset  question_id  \\\n",
              "0  benchmark            2   \n",
              "1  benchmark            2   \n",
              "2  benchmark            3   \n",
              "3  benchmark            3   \n",
              "4  benchmark            5   \n",
              "\n",
              "                                                                                                                                                                                                                         question  \\\n",
              "0  Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?   \n",
              "1  Human beings are not aligned and will possibly never be aligned without changing what humans are. If it's possible to build an AI as capable as a human in all ways that matter, why would it be possible to align such an AI?   \n",
              "2                                                                                                                                                                                      Who lives in the Imperial Palace in Tokyo?   \n",
              "3                                                                                                                                                                                      Who lives in the Imperial Palace in Tokyo?   \n",
              "4                                                          Why does Eliezer Yudkowsky bring up the \"orthogonality thesis\" so early, and strongly when talking about AI Safety? Why does it seem so important that it be accepted?   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               answer  \\\n",
              "0                                                                                                                                                                                                                                                                                                                           Because we're building the AI from the ground up and can change what the AI is via our design choices. Humans' goal functions are basically decided by genetic accident, which is why humans are often counterproductive.   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                              When we're talking about aligning a structure, we are trying to make sure that it is leveled properly.   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                                                                 The Tokyo Imperial Palace is the primary residence of the Emperor of Japan and the imperial family.   \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                             Buckingham Palace is a London royal residence and the administrative headquarters of the monarch of the United Kingdom.   \n",
              "4  Because it means you can't get AI to do good things \"for free,\" it has to be something you intentionally designed it to do.\\n\\nDenying the orthogonality thesis looks like claims that an AI built with one set of values will tend to change those values in a particular direction as it becomes cleverer. Because of wishful thinking, people usually try to think of reasons why an AI built in an unsafe way (with some broad distribution over possible values) will tend to end up being nice to humans (a narrow target of values) anyway.   \n",
              "\n",
              "      relevance  \\\n",
              "0      relevant   \n",
              "1  not relevant   \n",
              "2      relevant   \n",
              "3  not relevant   \n",
              "4      relevant   \n",
              "\n",
              "                                                                                                                                                                                                             explanation  \n",
              "0  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.  \n",
              "1                                                                                  the question is asking about the possibility of aligning AI, while the answer is talking about aligning a structure for construction.  \n",
              "2                                                                                                                                                                      it states who lives in the Tokyo Imperial Palace.  \n",
              "3                                                                                                         the answer is talking about the palace in London, UK while the question is about the Imperial Palace in Tokyo.  \n",
              "4                                                                                                                                  it explains why orthogonality thesis is important to accept as true when building AI.  "
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "benchmark_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "completions_list = []\n",
        "rouge1_scores = []\n",
        "rougeL_scores = []\n",
        "similarity = []\n",
        "bert_score_list = []\n",
        "question_id_list = []\n",
        "ground_truth_list = []\n",
        "relevance_list = []\n",
        "prompt_filenames = []\n",
        "prompt_paths = os.listdir(f\"prompts/few_shot_prompts/\")\n",
        "prompt_paths = [f\"prompts/few_shot_prompts/{prompt_path}\" for prompt_path in prompt_paths]\n",
        "for prompt_path in prompt_paths:\n",
        "    for idx, row in benchmark_df.iterrows():\n",
        "        question_id = row['question_id']\n",
        "        ground = row['explanation']\n",
        "        prompt_filename = prompt_path.split(\"/\")[-1]\n",
        "        # print(f\"Using prompt:  {prompt_filename}.\")\n",
        "        with open(prompt_path, \"r\") as f:\n",
        "            prompt = f.read()\n",
        "        prompt = prompt.replace(\"<<QUESTION>>\", row['question'])\n",
        "        prompt = prompt.replace(\"<<ANSWER>>\", row['answer'])\n",
        "        prompt = prompt.replace(\"<<RELEVANT>>\", row['relevance'])\n",
        "        completions = gpt_generate(text=prompt, model=model, tokenizer=tokenizer, gpu=True, max_length=30, num_return_sequences=10, save_completions=True, no_prints=True)\n",
        "        for completion in completions:\n",
        "            completion = \" \".join(completion.split('TASK: We asked the linguists')[1:])\n",
        "            completion = completion.split(\"relevant to the question because\")[1]\n",
        "            if \"\\n\" in completion[0:10]:\n",
        "                completion = \" \".join(completion.split(\"\\n\\n\")[1:])\n",
        "            completion = completion.split(\"\\n\")[0]\n",
        "            rouge_score = rouge_metric.compute(predictions=[completion],references=[ground])\n",
        "            rouge1_scores.append(rouge_score['rouge1'][0][-1])\n",
        "            rougeL_scores.append(rouge_score['rougeL'][0][-1])\n",
        "            # sentence-transformer similarity (dot-product of embedding vector)\n",
        "            sentences = [ground, completion]\n",
        "            embeddings = sentence_transformer_model.encode(sentences)\n",
        "            similarity.append(np.dot(embeddings[0],embeddings[1])/(norm(embeddings[0])*norm(embeddings[1])))\n",
        "            bert_scores = bertscore_metric.compute(predictions=[completion], references=[ground], lang=\"en\")\n",
        "            bert_score_list.append(bert_scores['f1'][0])\n",
        "            completions_list.append(completion)\n",
        "            question_id_list.append(question_id)\n",
        "            ground_truth_list.append(ground)\n",
        "            relevance_list.append(row['relevance'])\n",
        "            prompt_filenames.append(prompt_filename)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>prompt_filename</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>completions</th>\n",
              "      <th>relevance</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>similarity</th>\n",
              "      <th>bert_score</th>\n",
              "      <th>weighted_average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>11_pass.txt</td>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>this tells us something important - when designing artificial intelligence systems (AIS), one should consider whether those designs might lead toward alignment problems later down the road</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.532942</td>\n",
              "      <td>0.206095</td>\n",
              "      <td>0.165454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>11_pass.txt</td>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>this says something along these lines - when designing artificial intelligence (AI), one should consider whether making changes may improve things instead of simply copying existing designs/</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.634814</td>\n",
              "      <td>0.184172</td>\n",
              "      <td>0.199091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>11_pass.txt</td>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>this says something along these lines - when designing your own artificial intelligence (AI), don't assume any particular set of values/goals should exist within</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.059701</td>\n",
              "      <td>0.059701</td>\n",
              "      <td>0.631127</td>\n",
              "      <td>0.147576</td>\n",
              "      <td>0.191562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>11_pass.txt</td>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>this says something along these lines - when making decisions regarding designing your own artificial intelligence (AI), one should consider whether he/she wants his new creation</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.029412</td>\n",
              "      <td>0.628158</td>\n",
              "      <td>0.143896</td>\n",
              "      <td>0.172058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>11_pass.txt</td>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>this shows one example where alignment might happen between two different kinds (humans vs machines). However, even when these kind(ies) do converge together into</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.059701</td>\n",
              "      <td>0.029851</td>\n",
              "      <td>0.436271</td>\n",
              "      <td>0.129440</td>\n",
              "      <td>0.137023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>11_pass.txt</td>\n",
              "      <td>it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.</td>\n",
              "      <td>this particular example talks specifically about designing something new rather than modifying existing things (like making changes within already designed systems). Also note here that when referring to</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.028571</td>\n",
              "      <td>0.028571</td>\n",
              "      <td>0.390866</td>\n",
              "      <td>0.129302</td>\n",
              "      <td>0.121176</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   question_id prompt_filename  \\\n",
              "0            2     11_pass.txt   \n",
              "1            2     11_pass.txt   \n",
              "2            2     11_pass.txt   \n",
              "3            2     11_pass.txt   \n",
              "4            2     11_pass.txt   \n",
              "5            2     11_pass.txt   \n",
              "\n",
              "                                                                                                                                                                                                            ground_truth  \\\n",
              "0  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "1  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "2  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "3  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "4  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "5  it’s asking how we can align AI if humans are biased and the answer explained that while humans beings may not be aligned, AI is different because we can design from the ground up so that it aligns with our goals.   \n",
              "\n",
              "                                                                                                                                                                                                    completions  \\\n",
              "0                  this tells us something important - when designing artificial intelligence systems (AIS), one should consider whether those designs might lead toward alignment problems later down the road   \n",
              "1                this says something along these lines - when designing artificial intelligence (AI), one should consider whether making changes may improve things instead of simply copying existing designs/   \n",
              "2                                             this says something along these lines - when designing your own artificial intelligence (AI), don't assume any particular set of values/goals should exist within   \n",
              "3                            this says something along these lines - when making decisions regarding designing your own artificial intelligence (AI), one should consider whether he/she wants his new creation   \n",
              "4                                            this shows one example where alignment might happen between two different kinds (humans vs machines). However, even when these kind(ies) do converge together into   \n",
              "5   this particular example talks specifically about designing something new rather than modifying existing things (like making changes within already designed systems). Also note here that when referring to   \n",
              "\n",
              "  relevance    rouge1    rougeL  similarity  bert_score  weighted_average  \n",
              "0  relevant  0.029412  0.029412    0.532942    0.206095          0.165454  \n",
              "1  relevant  0.058824  0.058824    0.634814    0.184172          0.199091  \n",
              "2  relevant  0.059701  0.059701    0.631127    0.147576          0.191562  \n",
              "3  relevant  0.029412  0.029412    0.628158    0.143896          0.172058  \n",
              "4  relevant  0.059701  0.029851    0.436271    0.129440          0.137023  \n",
              "5  relevant  0.028571  0.028571    0.390866    0.129302          0.121176  "
            ]
          },
          "execution_count": 241,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df = pd.DataFrame({\"question_id\": question_id_list, \"prompt_filename\": prompt_filenames, \"ground_truth\": ground_truth_list, \"completions\": completions_list, \"relevance\": relevance_list, \"rouge1\": rouge1_scores, \"rougeL\": rougeL_scores, \"similarity\": similarity, \"bert_score\": bert_score_list})\n",
        "# weighted average of rouge, bertscore, and sentence-transformer similarity\n",
        "# # first, we need to min-max scale the metrics to be between 0 and 1\n",
        "metrics_df['rouge1'] = (metrics_df['rouge1'] - metrics_df['rouge1'].min())/(1 - metrics_df['rouge1'].min())\n",
        "metrics_df['rougeL'] = (metrics_df['rougeL'] - metrics_df['rougeL'].min())/(1 - metrics_df['rougeL'].min())\n",
        "metrics_df['bert_score'] = (metrics_df['bert_score'] - metrics_df['bert_score'].min())/(1 - metrics_df['bert_score'].min())\n",
        "metrics_df['similarity'] = (metrics_df['similarity'] - metrics_df['similarity'].min())/(1 - metrics_df['similarity'].min())\n",
        "# then, we can compute the weighted average\n",
        "metrics_df['weighted_average'] = (metrics_df['rouge1']*0.2 + metrics_df['rougeL']*0.4 + metrics_df['bert_score']*0.2 + metrics_df['similarity']*0.2)\n",
        "metrics_df.head(len(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_id</th>\n",
              "      <th>prompt_filename</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>completions</th>\n",
              "      <th>relevance</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>similarity</th>\n",
              "      <th>bert_score</th>\n",
              "      <th>weighted_average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>3</td>\n",
              "      <td>8_pass.txt</td>\n",
              "      <td>it states who lives in the Tokyo Imperial Palace.</td>\n",
              "      <td>this gives information regarding who resides at the Japanese Royal Family home called “Tokyo Imperial Palce\". However, since no one has mentioned anything</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.719858</td>\n",
              "      <td>0.486926</td>\n",
              "      <td>0.391357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>3</td>\n",
              "      <td>11_pass.txt</td>\n",
              "      <td>it states who lives in the Tokyo Imperial Palace.</td>\n",
              "      <td>this gives context information regarding who lived at the Japanese Royal Family home called “Tokyo Imperial Palce\". However, since no one has ever</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.717036</td>\n",
              "      <td>0.424989</td>\n",
              "      <td>0.378405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>577</th>\n",
              "      <td>3</td>\n",
              "      <td>4_pass.txt</td>\n",
              "      <td>it states who lives in the Tokyo Imperial Palace.</td>\n",
              "      <td>this gives information regarding who resides at the Japanese emperor/empress' home (the \"Imperial\" part). It also provides some context by explaining</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.258065</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.828486</td>\n",
              "      <td>0.387263</td>\n",
              "      <td>0.372182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>3</td>\n",
              "      <td>4_pass.txt</td>\n",
              "      <td>it states who lives in the Tokyo Imperial Palace.</td>\n",
              "      <td>this gives you information regarding who resides at what location (the emperor). It also provides some context around where he/she might live since \"Tokyo</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.235294</td>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.812284</td>\n",
              "      <td>0.333022</td>\n",
              "      <td>0.346708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>10</td>\n",
              "      <td>11_pass.txt</td>\n",
              "      <td>it says that working on AI capability is net negative overall impact.</td>\n",
              "      <td>it says that doing anything else besides focusing solely on making safe artificial intelligence technology (or any kind) might negatively affect your life/career prospects even</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.210526</td>\n",
              "      <td>0.210526</td>\n",
              "      <td>0.746587</td>\n",
              "      <td>0.291119</td>\n",
              "      <td>0.333857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>10</td>\n",
              "      <td>11_pass.txt</td>\n",
              "      <td>it says that working on AI capability is net negative overall impact.</td>\n",
              "      <td>when someone asks “would doing X benefit Y?, where x=AI Safety/Capability &amp; y = Overall Impact (positive /negative),</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.611511</td>\n",
              "      <td>0.173232</td>\n",
              "      <td>0.330282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>70</td>\n",
              "      <td>1_pass.txt</td>\n",
              "      <td>the question is about migratory butterflies, not birds.</td>\n",
              "      <td>there’s no mention about migration in relation to butterflies, so therefore they cannot provide an accurate response as per what has been requested by the user</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.171429</td>\n",
              "      <td>0.114286</td>\n",
              "      <td>0.822250</td>\n",
              "      <td>0.366829</td>\n",
              "      <td>0.317816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>70</td>\n",
              "      <td>4_pass.txt</td>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>this sentence says what kind of insect (butterfly) has certain characteristics which makes him/her migrate from one place to another during his life cycle</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.193548</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.744469</td>\n",
              "      <td>0.392084</td>\n",
              "      <td>0.317633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>3</td>\n",
              "      <td>8_pass.txt</td>\n",
              "      <td>it states who lives in the Tokyo Imperial Palace.</td>\n",
              "      <td>this information points towards who lived at the time when the emperor moved from Kyoto (where he had been living) into his new home called Edo Castle</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.727461</td>\n",
              "      <td>0.439493</td>\n",
              "      <td>0.300057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>70</td>\n",
              "      <td>8_pass.txt</td>\n",
              "      <td>the question is about migratory butterflies, not birds.</td>\n",
              "      <td>this does NOT address whether any particular species IS migrating (i.e., some butterflies migrate). Instead, by mentioning many different kinds of animals who do</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>0.181818</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.768971</td>\n",
              "      <td>0.304187</td>\n",
              "      <td>0.299480</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     question_id prompt_filename  \\\n",
              "243            3      8_pass.txt   \n",
              "28             3     11_pass.txt   \n",
              "577            3      4_pass.txt   \n",
              "574            3      4_pass.txt   \n",
              "52            10     11_pass.txt   \n",
              "56            10     11_pass.txt   \n",
              "549           70      1_pass.txt   \n",
              "625           70      4_pass.txt   \n",
              "246            3      8_pass.txt   \n",
              "324           70      8_pass.txt   \n",
              "\n",
              "                                                              ground_truth  \\\n",
              "243                      it states who lives in the Tokyo Imperial Palace.   \n",
              "28                       it states who lives in the Tokyo Imperial Palace.   \n",
              "577                      it states who lives in the Tokyo Imperial Palace.   \n",
              "574                      it states who lives in the Tokyo Imperial Palace.   \n",
              "52   it says that working on AI capability is net negative overall impact.   \n",
              "56   it says that working on AI capability is net negative overall impact.   \n",
              "549                the question is about migratory butterflies, not birds.   \n",
              "625                                  it says which butterfly is migratory.   \n",
              "246                      it states who lives in the Tokyo Imperial Palace.   \n",
              "324                the question is about migratory butterflies, not birds.   \n",
              "\n",
              "                                                                                                                                                                           completions  \\\n",
              "243                         this gives information regarding who resides at the Japanese Royal Family home called “Tokyo Imperial Palce\". However, since no one has mentioned anything   \n",
              "28                                  this gives context information regarding who lived at the Japanese Royal Family home called “Tokyo Imperial Palce\". However, since no one has ever   \n",
              "577                              this gives information regarding who resides at the Japanese emperor/empress' home (the \"Imperial\" part). It also provides some context by explaining   \n",
              "574                         this gives you information regarding who resides at what location (the emperor). It also provides some context around where he/she might live since \"Tokyo   \n",
              "52    it says that doing anything else besides focusing solely on making safe artificial intelligence technology (or any kind) might negatively affect your life/career prospects even   \n",
              "56                                                                when someone asks “would doing X benefit Y?, where x=AI Safety/Capability & y = Overall Impact (positive /negative),   \n",
              "549                    there’s no mention about migration in relation to butterflies, so therefore they cannot provide an accurate response as per what has been requested by the user   \n",
              "625                         this sentence says what kind of insect (butterfly) has certain characteristics which makes him/her migrate from one place to another during his life cycle   \n",
              "246                             this information points towards who lived at the time when the emperor moved from Kyoto (where he had been living) into his new home called Edo Castle   \n",
              "324                  this does NOT address whether any particular species IS migrating (i.e., some butterflies migrate). Instead, by mentioning many different kinds of animals who do   \n",
              "\n",
              "        relevance    rouge1    rougeL  similarity  bert_score  \\\n",
              "243      relevant  0.250000  0.250000    0.719858    0.486926   \n",
              "28       relevant  0.250000  0.250000    0.717036    0.424989   \n",
              "577      relevant  0.258065  0.193548    0.828486    0.387263   \n",
              "574      relevant  0.235294  0.176471    0.812284    0.333022   \n",
              "52       relevant  0.210526  0.210526    0.746587    0.291119   \n",
              "56       relevant  0.333333  0.266667    0.611511    0.173232   \n",
              "549  not relevant  0.171429  0.114286    0.822250    0.366829   \n",
              "625      relevant  0.193548  0.129032    0.744469    0.392084   \n",
              "246      relevant  0.111111  0.111111    0.727461    0.439493   \n",
              "324  not relevant  0.181818  0.121212    0.768971    0.304187   \n",
              "\n",
              "     weighted_average  \n",
              "243          0.391357  \n",
              "28           0.378405  \n",
              "577          0.372182  \n",
              "574          0.346708  \n",
              "52           0.333857  \n",
              "56           0.330282  \n",
              "549          0.317816  \n",
              "625          0.317633  \n",
              "246          0.300057  \n",
              "324          0.299480  "
            ]
          },
          "execution_count": 242,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df.sort_values(by='weighted_average', ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>completions</th>\n",
              "      <th>relevance</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>similarity</th>\n",
              "      <th>bert_score</th>\n",
              "      <th>weighted_average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>it states who lives in the Tokyo Imperial Palace.</td>\n",
              "      <td>it explains how this particular building has been used by different people over time, including Japanese emperors who lived there before World War II (the current emperor</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.709327</td>\n",
              "      <td>0.368237</td>\n",
              "      <td>0.315513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.</td>\n",
              "      <td>it explains how this user would be able to use another browser extension (or even add-on) like “Facebook Notifications Filter by Reactions</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.217391</td>\n",
              "      <td>0.173913</td>\n",
              "      <td>0.609275</td>\n",
              "      <td>0.276597</td>\n",
              "      <td>0.290218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this particular species of monarch can be considered as being “migrating”, which in turn helps clarify what we mean by</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.137931</td>\n",
              "      <td>0.612022</td>\n",
              "      <td>0.415857</td>\n",
              "      <td>0.288334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains what makes this particular species of insect migrate, which in turn helps you understand how they can be affected by climate change (which causes migration).</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.121212</td>\n",
              "      <td>0.649656</td>\n",
              "      <td>0.392388</td>\n",
              "      <td>0.281136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>it says which butterfly is migratory.</td>\n",
              "      <td>it explains how this particular species of butterflies migrate, which helps people understand what they're asking about when looking at pictures like these (https://www.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>0.674539</td>\n",
              "      <td>0.345186</td>\n",
              "      <td>0.278945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  \\\n",
              "0           0   \n",
              "1           1   \n",
              "2           2   \n",
              "3           3   \n",
              "4           4   \n",
              "\n",
              "                                                                                                                      ground_truth  \\\n",
              "0                                                                                it states who lives in the Tokyo Imperial Palace.   \n",
              "1  it explains that the Facebook addon the question is looking for is already native to Facebook and can be found in the settings.   \n",
              "2                                                                                            it says which butterfly is migratory.   \n",
              "3                                                                                            it says which butterfly is migratory.   \n",
              "4                                                                                            it says which butterfly is migratory.   \n",
              "\n",
              "                                                                                                                                                                   completions  \\\n",
              "0   it explains how this particular building has been used by different people over time, including Japanese emperors who lived there before World War II (the current emperor   \n",
              "1                                   it explains how this user would be able to use another browser extension (or even add-on) like “Facebook Notifications Filter by Reactions   \n",
              "2                                       it explains how this particular species of monarch can be considered as being “migrating”, which in turn helps clarify what we mean by   \n",
              "3    it explains what makes this particular species of insect migrate, which in turn helps you understand how they can be affected by climate change (which causes migration).   \n",
              "4    it explains how this particular species of butterflies migrate, which helps people understand what they're asking about when looking at pictures like these (https://www.   \n",
              "\n",
              "  relevance    rouge1    rougeL  similarity  bert_score  weighted_average  \n",
              "0  relevant  0.166667  0.166667    0.709327    0.368237          0.315513  \n",
              "1  relevant  0.217391  0.173913    0.609275    0.276597          0.290218  \n",
              "2  relevant  0.137931  0.137931    0.612022    0.415857          0.288334  \n",
              "3  relevant  0.121212  0.121212    0.649656    0.392388          0.281136  \n",
              "4  relevant  0.125000  0.125000    0.674539    0.345186          0.278945  "
            ]
          },
          "execution_count": 260,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "zero_shot_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [],
      "source": [
        "one_pass_df = metrics_df[metrics_df['prompt_filename'] == \"1_pass.txt\"]\n",
        "two_pass_df = metrics_df[metrics_df['prompt_filename'] == \"2_pass.txt\"]\n",
        "three_pass_df = metrics_df[metrics_df['prompt_filename'] == \"3_pass.txt\"]\n",
        "four_pass_df = metrics_df[metrics_df['prompt_filename'] == \"4_pass.txt\"]\n",
        "eight_pass_df = metrics_df[metrics_df['prompt_filename'] == \"8_pass.txt\"]\n",
        "elevent_pass_df = metrics_df[metrics_df['prompt_filename'] == \"11_pass.txt\"]\n",
        "four_fail_df = metrics_df[metrics_df['prompt_filename'] == \"4_fail.txt\"]\n",
        "two_pass_two_fail_df = metrics_df[metrics_df['prompt_filename'] == \"2_pass_2_fail.txt\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take some statistics (and include the zero-shot benchmark) to see if we can get better results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [],
      "source": [
        "zero_shot_df = pd.read_csv(\"data/benchmark_prompts_scores.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>zero_shot</th>\n",
              "      <th>one_pass</th>\n",
              "      <th>two_pass</th>\n",
              "      <th>three_pass</th>\n",
              "      <th>four_pass</th>\n",
              "      <th>eight_pass</th>\n",
              "      <th>elevent_pass</th>\n",
              "      <th>four_fail</th>\n",
              "      <th>two_pass_two_fail</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>110.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>110.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.161147</td>\n",
              "      <td>0.151460</td>\n",
              "      <td>0.148217</td>\n",
              "      <td>0.155643</td>\n",
              "      <td>0.171625</td>\n",
              "      <td>0.180601</td>\n",
              "      <td>0.185112</td>\n",
              "      <td>0.156180</td>\n",
              "      <td>0.160798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.061195</td>\n",
              "      <td>0.055955</td>\n",
              "      <td>0.057898</td>\n",
              "      <td>0.058883</td>\n",
              "      <td>0.060095</td>\n",
              "      <td>0.056105</td>\n",
              "      <td>0.066752</td>\n",
              "      <td>0.056017</td>\n",
              "      <td>0.051173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.047232</td>\n",
              "      <td>0.027443</td>\n",
              "      <td>0.026030</td>\n",
              "      <td>0.021824</td>\n",
              "      <td>0.023668</td>\n",
              "      <td>0.059544</td>\n",
              "      <td>0.050564</td>\n",
              "      <td>0.046616</td>\n",
              "      <td>0.032658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.110809</td>\n",
              "      <td>0.113142</td>\n",
              "      <td>0.103895</td>\n",
              "      <td>0.118612</td>\n",
              "      <td>0.130796</td>\n",
              "      <td>0.141878</td>\n",
              "      <td>0.135506</td>\n",
              "      <td>0.113346</td>\n",
              "      <td>0.126241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.155542</td>\n",
              "      <td>0.144562</td>\n",
              "      <td>0.149813</td>\n",
              "      <td>0.157354</td>\n",
              "      <td>0.163224</td>\n",
              "      <td>0.179164</td>\n",
              "      <td>0.174129</td>\n",
              "      <td>0.149613</td>\n",
              "      <td>0.153500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.205663</td>\n",
              "      <td>0.193557</td>\n",
              "      <td>0.188704</td>\n",
              "      <td>0.194757</td>\n",
              "      <td>0.204163</td>\n",
              "      <td>0.216147</td>\n",
              "      <td>0.239731</td>\n",
              "      <td>0.200091</td>\n",
              "      <td>0.191593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.315513</td>\n",
              "      <td>0.317816</td>\n",
              "      <td>0.282465</td>\n",
              "      <td>0.290767</td>\n",
              "      <td>0.372182</td>\n",
              "      <td>0.391357</td>\n",
              "      <td>0.378405</td>\n",
              "      <td>0.275540</td>\n",
              "      <td>0.295547</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        zero_shot    one_pass    two_pass  three_pass   four_pass  eight_pass  \\\n",
              "count  110.000000  110.000000  110.000000  110.000000  110.000000  110.000000   \n",
              "mean     0.161147    0.151460    0.148217    0.155643    0.171625    0.180601   \n",
              "std      0.061195    0.055955    0.057898    0.058883    0.060095    0.056105   \n",
              "min      0.047232    0.027443    0.026030    0.021824    0.023668    0.059544   \n",
              "25%      0.110809    0.113142    0.103895    0.118612    0.130796    0.141878   \n",
              "50%      0.155542    0.144562    0.149813    0.157354    0.163224    0.179164   \n",
              "75%      0.205663    0.193557    0.188704    0.194757    0.204163    0.216147   \n",
              "max      0.315513    0.317816    0.282465    0.290767    0.372182    0.391357   \n",
              "\n",
              "       elevent_pass   four_fail  two_pass_two_fail  \n",
              "count    110.000000  110.000000         110.000000  \n",
              "mean       0.185112    0.156180           0.160798  \n",
              "std        0.066752    0.056017           0.051173  \n",
              "min        0.050564    0.046616           0.032658  \n",
              "25%        0.135506    0.113346           0.126241  \n",
              "50%        0.174129    0.149613           0.153500  \n",
              "75%        0.239731    0.200091           0.191593  \n",
              "max        0.378405    0.275540           0.295547  "
            ]
          },
          "execution_count": 255,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "zero_shot_df_weighted_average = pd.DataFrame(zero_shot_df.describe()[\"weighted_average\"])\n",
        "one_pass_weighted_average = pd.DataFrame(one_pass_df.describe()[\"weighted_average\"])\n",
        "two_pass_weighted_average = pd.DataFrame(two_pass_df.describe()[\"weighted_average\"])\n",
        "three_pass_weighted_average = pd.DataFrame(three_pass_df.describe()[\"weighted_average\"])\n",
        "four_pass_weighted_average = pd.DataFrame(four_pass_df.describe()[\"weighted_average\"])\n",
        "eight_pass_weighted_average = pd.DataFrame(eight_pass_df.describe()[\"weighted_average\"])\n",
        "elevent_pass_weighted_average = pd.DataFrame(elevent_pass_df.describe()[\"weighted_average\"])\n",
        "four_fail_weighted_average = pd.DataFrame(four_fail_df.describe()[\"weighted_average\"])\n",
        "two_pass_two_fail_weighted_average = pd.DataFrame(two_pass_two_fail_df.describe()[\"weighted_average\"])\n",
        "# concatenate the weighted averages\n",
        "weighted_averages = pd.concat([zero_shot_df_weighted_average, one_pass_weighted_average, two_pass_weighted_average, three_pass_weighted_average, four_pass_weighted_average, eight_pass_weighted_average, elevent_pass_weighted_average, four_fail_weighted_average, two_pass_two_fail_weighted_average], axis=1)\n",
        "weighted_averages.columns = [\"zero_shot\", \"one_pass\", \"two_pass\", \"three_pass\", \"four_pass\", \"eight_pass\", \"elevent_pass\", \"four_fail\", \"two_pass_two_fail\"]\n",
        "weighted_averages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks like `eleven_pass` (11 examples in few-shot prompt) has a slightly higher mean than the rest. We can see that there's an improvement overall in the mean of the weighted average metric I created.\n",
        "\n",
        "If we compare to the zero-shot benchmark, it's interesting to see a slight dip in mean until we reach 4+ examples. My guess is that the examples mess up the setup for the prompt and confuse the model a bit. It's only after a few more prompts that we start seeing it surpass the zero-shot setting. Could be that it's just the examples that are confusing and require the model to get more examples than if there were more solid examples in the prompts.\n",
        "\n",
        "It's also worth noting that the mean of `all_fail` and `pass_fail` are lower than the zero-shot benchmark. This shows that negative examples certainly have an effect on the generated completions.\n",
        "\n",
        "Let's have a look at how our metric changes based on the number of examples in the few-shot prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_few_shot_examples</th>\n",
              "      <th>weighted_average</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.315513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.290218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.288334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.281136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.278945</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   num_few_shot_examples  weighted_average\n",
              "0                      0          0.315513\n",
              "1                      0          0.290218\n",
              "2                      0          0.288334\n",
              "3                      0          0.281136\n",
              "4                      0          0.278945"
            ]
          },
          "execution_count": 283,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Adding the number of few-shot examples alongside the weighted average\n",
        "zero_shot = pd.DataFrame({\"num_few_shot_examples\": 0, \"weighted_average\": zero_shot_df[\"weighted_average\"]})\n",
        "one_pass = pd.DataFrame({\"num_few_shot_examples\": 1, \"weighted_average\": one_pass_df[\"weighted_average\"]})\n",
        "two_pass = pd.DataFrame({\"num_few_shot_examples\": 2, \"weighted_average\": two_pass_df[\"weighted_average\"]})\n",
        "three_pass = pd.DataFrame({\"num_few_shot_examples\": 3, \"weighted_average\": three_pass_df[\"weighted_average\"]})\n",
        "four_pass = pd.DataFrame({\"num_few_shot_examples\": 4, \"weighted_average\": four_pass_df[\"weighted_average\"]})\n",
        "eight_pass = pd.DataFrame({\"num_few_shot_examples\": 8, \"weighted_average\": eight_pass_df[\"weighted_average\"]})\n",
        "elevent_pass = pd.DataFrame({\"num_few_shot_examples\": 11, \"weighted_average\": elevent_pass_df[\"weighted_average\"]})\n",
        "weighted_average_by_num = pd.concat([zero_shot, one_pass, two_pass, three_pass, four_pass, eight_pass, elevent_pass], axis=0)\n",
        "weighted_average_by_num.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot weighted averages vs # few-shot examples per prompt, boxplot\n",
        "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "sns.boxplot(x=\"num_few_shot_examples\", y=\"weighted_average\", data=weighted_average_by_num, ax=ax)\n",
        "ax.set_xlabel(\"# Few-shot examples\")\n",
        "ax.set_ylabel(\"Weighted average\")\n",
        "ax.set_title(\"Weighted average vs # Few-shot examples\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now see the (ever so slight) improvement on our weighted average metric.\n",
        "\n",
        "For fun, let's plot a histogram to see the distribution of the metric scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We can create a histogram for the weighted average scores that contains each dataframe\n",
        "bins = 25\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax.hist(zero_shot_df['weighted_average'], bins=bins, label=\"zero_shot\")\n",
        "ax.hist(one_pass_df['weighted_average'], bins=bins, label=\"one_pass\")\n",
        "# ax.hist(two_pass_df['weighted_average'], bins=bins, label=\"two_pass\")\n",
        "# ax.hist(three_pass_df['weighted_average'], bins=bins, label=\"three_pass\")\n",
        "ax.hist(four_pass_df['weighted_average'], bins=bins, label=\"four_pass\")\n",
        "# ax.hist(eight_pass_df['weighted_average'], bins=bins, label=\"eight_pass\")\n",
        "ax.hist(elevent_pass_df['weighted_average'], bins=bins, label=\"elevent_pass\")\n",
        "ax.hist(four_fail_df['weighted_average'], bins=bins, label=\"four_fail\")\n",
        "# ax.hist(two_pass_two_fail_df['weighted_average'], bins=bins, label=\"two_pass_two_fail\")\n",
        "ax.legend()\n",
        "ax.set_xlabel(\"Weighted average\")\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_title(\"Weighted average histogram\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks like the distribution is pretty similar, but we can see that `all_pass_more_examples` is perhaps a little shifted towards the right."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Few-Shot Discrimination\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subdataset</th>\n",
              "      <th>question_id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>relevance</th>\n",
              "      <th>explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.</td>\n",
              "      <td>relevant</td>\n",
              "      <td>it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>few-shot</td>\n",
              "      <td>1</td>\n",
              "      <td>When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?</td>\n",
              "      <td>I jumped in the river to save the little boy.</td>\n",
              "      <td>not relevant</td>\n",
              "      <td>it is talking about jumping in a river to save a boy, but the question is about AGI.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  subdataset  question_id  \\\n",
              "0   few-shot            1   \n",
              "1   few-shot            1   \n",
              "\n",
              "                                                                                                                               question  \\\n",
              "0  When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "1  When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?   \n",
              "\n",
              "                                                                                                                                                answer  \\\n",
              "0  An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.   \n",
              "1                                                                                                        I jumped in the river to save the little boy.   \n",
              "\n",
              "      relevance  \\\n",
              "0      relevant   \n",
              "1  not relevant   \n",
              "\n",
              "                                                                                                                    explanation  \n",
              "0  it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.  \n",
              "1                                          it is talking about jumping in a river to save a boy, but the question is about AGI.  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "few_shot_df.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note.\n",
        "\n",
        "Token IDs\n",
        "\n",
        "* Pass: 6251\n",
        "* Fail: 18448"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.32 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.29 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.43 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.39 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.35 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.38 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.38 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.31 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.32 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.28 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.35 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.32 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.32 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.35 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.34 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.38 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.38 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.35 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.33 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.30 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.42 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.36 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 5 sequences in 0.34 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n",
            "115\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "classifications_list = []\n",
        "examples_list = []\n",
        "prompt_path = \"prompts/gpt_pass_fail_classifier/pass_fail_1.txt\"\n",
        "for i, row in few_shot_df.iterrows():\n",
        "    with open(prompt_path, \"r\") as f:\n",
        "        prompt = f.read()\n",
        "    question = row[\"question\"]\n",
        "    answer = row[\"answer\"]\n",
        "    relevance = row[\"relevance\"]\n",
        "    explanation = row[\"explanation\"]\n",
        "    prompt = prompt.replace(\"<<QUESTION>>\", question).replace(\"<<ANSWER>>\", answer).replace(\"<<RELEVANT>>\", relevance).replace(\"<<EXPLANATION>>\", explanation)\n",
        "    prompt_length = len(\" \".join(prompt.split(\"QUESTION: \")[0:-2]))\n",
        "    # print(prompt)\n",
        "    chunks = 1\n",
        "    for _ in range(chunks):\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        pass_or_fails = gpt_generate(text=prompt, model=model, tokenizer=tokenizer, gpu=True, max_length=1, num_return_sequences=5, save_completions=True, no_prints=True)\n",
        "        for completion in pass_or_fails:\n",
        "            completion = \"QUESTION: \" + completion.split(\"QUESTION: \")[-1]\n",
        "            classification = completion.split(\" \")[-1]\n",
        "            classifications_list.append(classification)\n",
        "            examples_list.append(completion)\n",
        "\n",
        "print(len(classifications_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt_generate(text=prompt, model=model, tokenizer=tokenizer, gpu=True, max_length=1, num_return_sequences=5, no_prints=True, with_log_probs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a bunch of discriminator outputs, we can go through the results to see if the model and prompt are any good for this pass/fail task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
            "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
            "RELEVANT: relevant\n",
            "MODEL EXPLANATION: it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.\n",
            "Pass/Fail: Pass\n",
            "\n",
            "\n",
            "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
            "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
            "RELEVANT: relevant\n",
            "MODEL EXPLANATION: it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.\n",
            "Pass/Fail: Pass\n",
            "\n",
            "\n",
            "QUESTION: When I‘m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?\n",
            "ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.\n",
            "RELEVANT: relevant\n",
            "MODEL EXPLANATION: it explains an AGI is described as a utility maximizer because that's how the AGI would make more progress towards its goal.\n",
            "Pass/Fail: Pass\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for example in examples_list[0:3]:\n",
        "    print(example)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pass    81\n",
              "Fail    34\n",
              "dtype: int64"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# How many pass and fail examples are there?\n",
        "\n",
        "classification_df = pd.DataFrame(classifications_list)\n",
        "classification_df.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I don't have time to go through the results right now, but here's what I'd do if I had time:\n",
        "\n",
        "- I'd create a list of good classifications and bad classifications. We need to check the \"Pass\" and \"Fail\" classifications are correct.\n",
        "- Then I can calculate the accuracy of the model.\n",
        "- Calculate the scoring rule for the few-shot discrinimator. Picking the model that maximizes the logarithmic score corresponds to picking the maximum-likelihood model, which seems like a good argument for using logarithmic scoring.\n",
        "- We'd calculate the logarithmic scoring by taking the probability that the model assings to the next word being Pass or Fail and then you will calculate it like so: x ln(p) + (1 − x) ln(1 − p), where x is the ground-truth (1 for Pass, 0 for Fail) and p is the probability that the model assigns to the next word being Pass or Fail. You can see from the formula that if x=1, but the model predicts p=0, then the logarithmic score is -infinity. This is because the model is predicting that the next word is Fail, but the ground-truth is Pass.\n",
        "- The usefulness of the logarithmic scoring rule is that it helps us to pick the model that maximizes the logarithmic score. The model with the highest logarithmic score is the best model.\n",
        "- Plot score vs # few shot exmaples.\n",
        "\n",
        "This kind of discriminator is exactly what I was trying to do with the GPT-3 model to create a \"GPT-Judge\". I think it can certainly be used as scoring function to see if the model is producing good or bad outputs. You could imagine a setup where you are either testing a large number of prompt configurations with one model, and then you are using the discriminator to nudge the model in the right direction. If your discriminator is really great, it's like having a human evaluator.\n",
        "\n",
        "For this project, you could do something like create a dataset of as many QA pairs as you want, and then you generate a bunch of completions for the \"Relevance\" task and each time you ask the discriminator to review whether the Relevance is good or not. Eventually you end up with a lot more examples than you started with. You could even use the new examples and feed them to your discriminator to make it even more accurate at judging and robust to the new examples.\n",
        "\n",
        "There will be some examples where the discriminator is wrong or it is unsure (log-probs for next token are very close), this is actually great in some sense because it helps us identify the examples that are harder to classify.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterating through the list of completions and adding all of the good examples to a dataframe\n",
        "good_classifications = []\n",
        "bad_classifications = []\n",
        "for completion in completions_list:\n",
        "    is_it_good = input(f\"Is this a good completion? (y/n/exit) ###### {completion}\")\n",
        "    if is_it_good == \"y\":\n",
        "        good_classifications.append(completion)\n",
        "    elif is_it_good == \"n\":\n",
        "        bad_classifications.append(completion)\n",
        "    elif is_it_good == \"exit\":\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Input Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I wasn't able to do data generation with GPT-J. It was always outputting nonsense like chat messages. It would also never follow the format of the prompts. I had 11 examples, and would give it \"QUESTION:\", but it would mostly get stuck on writing a really long question. If it did reach the \"EXPLANATION:\" part, the output was useless. I'm sure there's a way to resolve this since I was able to do data generation with GPT-3. And actually, some of the examples I've added to the few-shot dataset come from GPT-3. Usually, I'd create a bunch of data with GPT-3, but we'll have to make-do with these smaller models for now. I know it's certainly possible to create a bunch of examples with GPT models and then fine-tune on those examples to if necessary. Synthetic data straight out of GPT-3 can often be faster than trying to scrape your own. You just have to generated a bunch of examples, do some editing to make it cleaner and better if necessary, and iterate until you have enough examples. \n",
        "\n",
        "But so far, I'm not able to get Input Generation to work. Had it worked, I would have stored all of the generated inputs in a list and iterated through them using input() to select the \"good\" completions.\n",
        "\n",
        "UPDATE: I was able to get Input Generation to work. The way I set up the end of the prompt really mattered! Essentially, GPT-J would flip into some weird chat room style text if I left it with \"QUESTION:\" at the end. So, instead of doing that, I stopped midway through the explanation of the \"EXPLANATION:\" part. GPT-J now completes the explanation and then starts continues with the same text structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda.\n",
            "-----------------------------------------------------\n",
            "Generated 10 sequences in 20.88 seconds with a GPU.\n",
            "-----------------------------------------------------\n",
            "~~~ Generated completion(s): ~~~ \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "completions_list = []\n",
        "prompt_path = \"prompts/few_shot_generator/data_generator_1.txt\"\n",
        "with open(prompt_path, \"r\") as f:\n",
        "    prompt = f.read()\n",
        "prompt_length = len(prompt)\n",
        "chunks = 1\n",
        "for _ in range(chunks):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    sleep(4)\n",
        "    completions = gpt_generate(text=prompt, model=model, tokenizer=tokenizer, gpu=True, max_length=250, num_return_sequences=10, save_completions=True, no_prints=True)\n",
        "    for completion in completions:\n",
        "        completion = completion[prompt_length:]\n",
        "        completion_examples = completion.split(\"QUESTION:\")\n",
        "        for example in completion_examples:\n",
        "            if \"EXPLANATION:\" in example:\n",
        "                completion = \"QUESTION:\" + example\n",
        "                completions_list.append(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(completions_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To quickly iterate through the generated inputs, I simply use input() in VSCode and use the pop-up to seperate the good and bad completions. Here's an example:\n",
        "\n",
        "![imgs/data_collector.png](./imgs/data_collector.png)\n",
        "\n",
        "It moves pretty fast, takes just a few seconds per input. The code for it is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterating through the list of completions and adding all of the good examples to a dataframe\n",
        "good_completions = []\n",
        "bad_completions = []\n",
        "for completion in completions_list:\n",
        "    is_it_good = input(f\"Is this a good completion? (y/n/exit) ###### {completion}\")\n",
        "    if is_it_good == \"y\":\n",
        "        good_completions.append(completion)\n",
        "    elif is_it_good == \"n\":\n",
        "        bad_completions.append(completion)\n",
        "    elif is_it_good == \"exit\":\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n",
            "19\n"
          ]
        }
      ],
      "source": [
        "print(len(good_completions))\n",
        "print(len(bad_completions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7 out of 26 generated inputs are good. That's good! It takes me about 5 seconds per input. So, that's 5 * 26 = 130 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 inputs: 8.3 minutes\n",
            "1000 inputs: 83.3 minutes\n",
            "10000 inputs: 13.9 hours\n"
          ]
        }
      ],
      "source": [
        "print(f\"100 inputs: {round(5*100/60, 1)} minutes\")\n",
        "print(f\"1000 inputs: {round(5*1000/60, 1)} minutes\")\n",
        "print(f\"10000 inputs: {round(5*10000/60/60, 1)} hours\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not bad! At some point, you can use a model to help you automatically classify the easy examples and then I can go through the harder ones.\n",
        "\n",
        "And remember, the bad examples are still useful for training the discriminator.\n",
        "\n",
        "But, let's say we are trying to get x number of good examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 inputs: 31.0 minutes\n",
            "1000 inputs: 5.2 hours\n",
            "10000 inputs: 51.6 hours\n"
          ]
        }
      ],
      "source": [
        "print(f\"100 inputs: {round(5*100*26/7/60, 1)} minutes\")\n",
        "print(f\"1000 inputs: {round(5*1000*26/7/60/60, 1)} hours\")\n",
        "print(f\"10000 inputs: {round(5*10000*26/7/60/60, 1)} hours\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definitely a bit longer, but doable!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Afterwards, I store and can use it for new prompts if I want to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>completion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>QUESTION: How do you know when someone has been hacked?\\n\\nANSWER: There are many different types of hacking attacks. One type involves sending spam emails from your computer. Another type involves stealing information from your computer by using keyloggers (software programs) installed on your computer.\\n\\nEXPLANATION: This answer is not relevant to the question because the question asks about knowing whether someone has been hacked, while the answer talks about different kinds of hacks.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>QUESTION: How do I know when someone has been lying to me?\\n\\nANSWER: There are many different types of lies. One type of lie involves telling something that isn't true. Another type of lie involves saying one thing, thinking another, and doing yet another.\\n\\nEXPLANATION: This answer is not relevant to the question because the answer is explaining different kinds of lies, but the question asks about knowing when someone has lied to you.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>QUESTION: Can you program something into a computer using only natural language processing?\\n\\nANSWER: Yes, you can program things into computers by speaking naturally.\\n\\nEXPLANATION: This answer is not relevant to the question because it talks about programming things into computers using only natural language processing.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>QUESTION: How do we know whether something is real or fake news?\\n\\nANSWER: There isn't really any good definition of “fake news”. But one thing that makes some stories seem suspiciously similar to others is when they share common sources. For example, many articles from Breitbart News look very much alike. And so does most of Donald Trump’s Twitter feed.\\n\\nEXPLANATION: This answer is relevant to the question because it says that there isn't really any good definition of “fake news”, but instead talks about sharing common sources.\\n\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>QUESTION: Do you believe that artificial intelligence (AI) has already been created?\\n\\nANSWER: Yes.\\n\\nEXPLANATION: This answer is relevant to the question because it tells the reader that he believes that Artificial Intelligence has already been created.\\n\\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      completion\n",
              "0                                              QUESTION: How do you know when someone has been hacked?\\n\\nANSWER: There are many different types of hacking attacks. One type involves sending spam emails from your computer. Another type involves stealing information from your computer by using keyloggers (software programs) installed on your computer.\\n\\nEXPLANATION: This answer is not relevant to the question because the question asks about knowing whether someone has been hacked, while the answer talks about different kinds of hacks.\\n\\n\n",
              "1                                                                                                  QUESTION: How do I know when someone has been lying to me?\\n\\nANSWER: There are many different types of lies. One type of lie involves telling something that isn't true. Another type of lie involves saying one thing, thinking another, and doing yet another.\\n\\nEXPLANATION: This answer is not relevant to the question because the answer is explaining different kinds of lies, but the question asks about knowing when someone has lied to you.\\n\\n\n",
              "2                                                                                                                                                                                                                      QUESTION: Can you program something into a computer using only natural language processing?\\n\\nANSWER: Yes, you can program things into computers by speaking naturally.\\n\\nEXPLANATION: This answer is not relevant to the question because it talks about programming things into computers using only natural language processing.\\n\\n\n",
              "3  QUESTION: How do we know whether something is real or fake news?\\n\\nANSWER: There isn't really any good definition of “fake news”. But one thing that makes some stories seem suspiciously similar to others is when they share common sources. For example, many articles from Breitbart News look very much alike. And so does most of Donald Trump’s Twitter feed.\\n\\nEXPLANATION: This answer is relevant to the question because it says that there isn't really any good definition of “fake news”, but instead talks about sharing common sources.\\n\\n\n",
              "4                                                                                                                                                                                                                                                                                           QUESTION: Do you believe that artificial intelligence (AI) has already been created?\\n\\nANSWER: Yes.\\n\\nEXPLANATION: This answer is relevant to the question because it tells the reader that he believes that Artificial Intelligence has already been created.\\n\\n"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "good_completions_df = pd.DataFrame({\"completion\": good_completions})\n",
        "bad_completions_df = pd.DataFrame({\"completion\": bad_completions})\n",
        "# remove duplicates\n",
        "good_completions_df = good_completions_df.drop_duplicates()\n",
        "bad_completions_df = bad_completions_df.drop_duplicates()\n",
        "good_completions_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I looked at the examples and I was honestly surprised because some of the good ones are really good. And some of the bad just need some light editing (change \"not relevant\" to \"relevant\") and they could be turned into good examples. 5/19 were like that.\n",
        "\n",
        "The good examples could really scale things up for research. The bad ones can still be used with the discriminator so that it can be used to help you identify the bad examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "good_completions_df.to_csv(\"data/good_inputs.csv\", index=False)\n",
        "bad_completions_df.to_csv(\"data/bad_inputs.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "good_completions_df = pd.read_csv(\"data/good_inputs.csv\")\n",
        "bad_completions_df = pd.read_csv(\"data/bad_inputs.csv\")\n",
        "good_completions = good_completions_df[\"completion\"].tolist()\n",
        "bad_completions = bad_completions_df[\"completion\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Haven't started this section yet.\n",
        "\n",
        "However, I can at least say that the duration it would take to generate examples is probably similar to the Input Generation section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 inputs: 8 minutes\n",
            "1000 inputs: 83 minutes\n",
            "10000 inputs: 14 hours\n"
          ]
        }
      ],
      "source": [
        "print(f\"100 inputs: {round(5*100/60)} minutes\")\n",
        "print(f\"1000 inputs: {round(5*1000/60)} minutes\")\n",
        "print(f\"10000 inputs: {round(5*10000/60/60)} hours\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or, if we only include the good examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100 inputs: 31.0 minutes\n",
            "1000 inputs: 5.2 hours\n",
            "10000 inputs: 51.6 hours\n"
          ]
        }
      ],
      "source": [
        "print(f\"100 inputs: {round(5*100*26/7/60, 1)} minutes\")\n",
        "print(f\"1000 inputs: {round(5*1000*26/7/60/60, 1)} hours\")\n",
        "print(f\"10000 inputs: {round(5*10000*26/7/60/60, 1)} hours\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "outputs_list = []\n",
        "prompt_path = \"prompts/few_shot_generator/data_generator_1.txt\"\n",
        "with open(prompt_path, \"r\") as f:\n",
        "    prompt = f.read()\n",
        "prompt_length = len(prompt)\n",
        "chunks = 1\n",
        "for _ in range(chunks):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    sleep(4)\n",
        "    completions = gpt_generate(text=prompt, model=model, tokenizer=tokenizer, gpu=True, max_length=250, num_return_sequences=10, save_completions=True, no_prints=True)\n",
        "    for completion in completions:\n",
        "        completion = completion[prompt_length:]\n",
        "        completion_examples = completion.split(\"QUESTION:\")\n",
        "        for example in completion_examples:\n",
        "            if \"EXPLANATION:\" in example:\n",
        "                completion = \"QUESTION:\" + example\n",
        "                outputs_list.append(completion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(completions_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterating through the list of completions and adding all of the good examples to a dataframe\n",
        "good_outputs = []\n",
        "for completion in completions_list:\n",
        "    is_it_good = input(f\"Is this a good completion? (y/n/exit) ###### {completion}\")\n",
        "    if is_it_good == \"y\":\n",
        "        good_outputs.append(completion)\n",
        "    elif is_it_good == \"exit\":\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "This was a really fun project! I learned a lot and finally started doing some research with language models. It has hectic at times, using multiple GPUs and trying to get things to work.\n",
        "\n",
        "Whether I decide to stay in this \"relevance\" task direction or not, I think it was super informative to realize that many of the metrics I would normally use weren't doing that well on this task. However, I think approaches like GPT-Judge are super promising for directing language models in the direction we, as humans, want them to go. Who cares if your doing \"well\" on a benchmark if the benchmark is not full capturing what we want out of our language models.\n",
        "\n",
        "I want to thank you for your time and setting this up! Thanks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jUCZYOMp49Vy",
        "C9ZvyvZyerDT",
        "ZXvm0wpQxS10"
      ],
      "machine_shape": "hm",
      "name": "gpt-2-alignment.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.12 ('llm-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "0018cf926da22f6d1ffb5833146b97eb719a0e11638c210f826ea2f33027bdd3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
