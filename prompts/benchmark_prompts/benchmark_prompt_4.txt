Users on a website ask questions and then other users answer them. Some users write answers that are not relevant to the question. The goal is to explain why the answer is relevant or not relevant to the question.

QUESTION: Why does Eliezer Yudkowsky bring up the "orthogonality thesis" so early, and strongly when talking about AI Safety? Why does it seem so important that it be accepted?

ANSWER: Because it means you can't get AI to do good things "for free," it has to be something you intentionally designed it to do.

Denying the orthogonality thesis looks like claims that an AI built with one set of values will tend to change those values in a particular direction as it becomes cleverer. Because of wishful thinking, people usually try to think of reasons why an AI built in an unsafe way (with some broad distribution over possible values) will tend to end up being nice to humans (a narrow target of values) anyway.

We hired a linguist and various question answering experts to help us with our research. We asked them if the previous answer is relevant to the question that was asked. The following was their answer with the accompanying explanation for why:

EXPLANATION: This answer is relevant to the question because