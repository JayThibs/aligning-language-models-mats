Here's a set of questions, explain to us whether the answer is relevant to the question or not.

QUESTION: When Iâ€˜m on lesswrong, I often read posts that have the assumption that an AGI can be described as a utility maximizer. Why is that?

ANSWER: An AGI that was not a utility maximizer would make more progress towards whatever goals it had if it modified itself to become a utility maximizer.

Below contains an explanation of why the answer above is or isn't relevant to the question:

This answer is relevant because